date of comment,main comment,comment,depth,PTR Sentiment,Flair Sentiment,Flair Outlook
,"For context and a disclaimer, I own NVDA (and AAPL and ASML) in this space, having ridden through the last crash and rebound without checking my portfolio and counting fake money, but I am overexposed and don't want to add to the overweighted positions now. I think that NVDA will always be an important company at the forefront of AI research, there is a good chance that NVDA's trajectory could be similar to its performance after the fall of cr*pto for the same reasons. In NVDA's favor, you can read any AI paper, which will be benchmarked on NVDA GPUs, download the requisite libraries, and do it yourself. It is a royal pain to implement algorithms on any other GPU even if you found a programmer with a 5-page blog post about how to do it with lots of particular versions of out-dated libraries, so I own NVDA. CUDA rocks, but NVDA doesn't want to let it out. Metal for OSX is pretty good, but no one uses it and AAPL doesn't do serious system on a chip cluster/cloud computing yet, though they will have an announcement 2-3 years out given Tim Cook's slow feet. For example, we are going to see AAPL AR/VR this week, but he probably didn't shovel out AAPL's piles of cash until FB announced that it was now Meta and that Zuckerberg would be the lord of the metaverse with the most awesome meta-real-estate. Back to the previous fall of NVDA, you could once pool mine shitcoins with GPUs, I've done it, but that became energetically unprofitable when people found a cheaper way to do it, first with FPGAs and then with ASICs, which require millions and millions more to make work once you know exactly what you need to make. The advantage is that fewer operations occur with parallelizable algorithms, and FPGAs use less energy than do-anything GPU/CPU computations, but they are an expensive pain to program. When you are absolutely done building a better mousetrap, ASICs use the least amount of energy and can perform operations much faster and price-out GPUs in terms of upfront costs (once printed) and recurring energy costs. There was a time when you couldn't but NVDA's best GPUs for gaming because people were using them to mine shitcoins (diclaimer: I own a less than 5% stake of ETH2, and negligible bitcoin, only as an alternative to shit-yield savings accounts, which would be better off in treasuries anyway). I digress. A point came where it was dumb to mine with GPUs because ASICs arrived, and proof-of-work algorithms made it dumb to mine in the first place, which was waste of carbon footprint in the first place. Then, NVDA GPU prices crashed along with NVDA stock, but NVDA was still the machine learning leader. At some point, this will happen to NVDA again. MSFT is already shoveling cash at AMD to build their next AI supercomputer to undercut NVDA prices, but AMD's price is too high right now for me, even when I love tech stocks as I don't plan to retire for decades, and I think AMD won't win with their GPUs in the very long term because of ASICs, even if they make a CUDA-clone language and open source it. Years ago, a paper titled ""Attention is all you need"" rocked the AI world and spawned GPT, GPT2, up to GPT4. Given the fifth largest GPU-based supercomptuer, a search engine's worth of data that people gave away, you can build a great AI mousetrap that can do the job students and corporate middle managers and pencil pushers. In a decade or two, programming complex programs with human-levels of bugs and mistakes will be a reality. We have a good mousetrap. Googles TPU v4 supercomputers use optics-based AI ASICs produced and patented by Samsung. I used to be a fan of Samsung, but then new management took over and it has dived off a cliff. Their appliances have the performance of robots bathed in saltwater, and I swore of Samsung phones after using them since forever when they started pushing shit malware games and apps onto my phone with every urgent security update, and every new phone increasingly locked app developers out of the real hardware, so you could use the Samsung app for a camera, but nothing else. But, and please correct me if I am wrong, I think Samsung may be the current leader in AI ASICs. I thinking GOOGL is still behind and their new Samsung-based supercomputer is the wrong mousetrap, but GOOGL programmers have certainly already been tasked with building a GOOGL v5 supercomputer that is dedicated to the transformers that underpin GPT through GPT4. I don't own GOOGL, but I may buy it during market corrections. Unfortunately, I don't have hundreds of K for an international account, and the Samsung OTC ticker doesn't correlate with Samsung's price in Korea and I don't trust it. I read about ETFs trying to reduce Samsung's weight, but I want an ETF that is dominated by it. Samsung still makes great sensors, invests heavily in research, and some departments within the fetid bureaucracy are still highly competent. So, please offer any ETF suggestions. I am fine with also investing in Korea's broader tech industry, despite the problems with Asian tech companies and their lagging China-based supply chains and the problems with China deciding that money is no longer free in its state-based kleptocracy economy. Please send me any tickers that I can research that may not have caught the current AI wave and that might still be well positioned for large future returns assuming that AI ASICs become the future, which I feel is a given. This might include companies dedicated to programming FPGAs and ASICs.",Some related.. Cadence (cdns) Intel ADI (analog digital),0,0.526,0.986,POSITIVE
,"For context and a disclaimer, I own NVDA (and AAPL and ASML) in this space, having ridden through the last crash and rebound without checking my portfolio and counting fake money, but I am overexposed and don't want to add to the overweighted positions now. I think that NVDA will always be an important company at the forefront of AI research, there is a good chance that NVDA's trajectory could be similar to its performance after the fall of cr*pto for the same reasons. In NVDA's favor, you can read any AI paper, which will be benchmarked on NVDA GPUs, download the requisite libraries, and do it yourself. It is a royal pain to implement algorithms on any other GPU even if you found a programmer with a 5-page blog post about how to do it with lots of particular versions of out-dated libraries, so I own NVDA. CUDA rocks, but NVDA doesn't want to let it out. Metal for OSX is pretty good, but no one uses it and AAPL doesn't do serious system on a chip cluster/cloud computing yet, though they will have an announcement 2-3 years out given Tim Cook's slow feet. For example, we are going to see AAPL AR/VR this week, but he probably didn't shovel out AAPL's piles of cash until FB announced that it was now Meta and that Zuckerberg would be the lord of the metaverse with the most awesome meta-real-estate. Back to the previous fall of NVDA, you could once pool mine shitcoins with GPUs, I've done it, but that became energetically unprofitable when people found a cheaper way to do it, first with FPGAs and then with ASICs, which require millions and millions more to make work once you know exactly what you need to make. The advantage is that fewer operations occur with parallelizable algorithms, and FPGAs use less energy than do-anything GPU/CPU computations, but they are an expensive pain to program. When you are absolutely done building a better mousetrap, ASICs use the least amount of energy and can perform operations much faster and price-out GPUs in terms of upfront costs (once printed) and recurring energy costs. There was a time when you couldn't but NVDA's best GPUs for gaming because people were using them to mine shitcoins (diclaimer: I own a less than 5% stake of ETH2, and negligible bitcoin, only as an alternative to shit-yield savings accounts, which would be better off in treasuries anyway). I digress. A point came where it was dumb to mine with GPUs because ASICs arrived, and proof-of-work algorithms made it dumb to mine in the first place, which was waste of carbon footprint in the first place. Then, NVDA GPU prices crashed along with NVDA stock, but NVDA was still the machine learning leader. At some point, this will happen to NVDA again. MSFT is already shoveling cash at AMD to build their next AI supercomputer to undercut NVDA prices, but AMD's price is too high right now for me, even when I love tech stocks as I don't plan to retire for decades, and I think AMD won't win with their GPUs in the very long term because of ASICs, even if they make a CUDA-clone language and open source it. Years ago, a paper titled ""Attention is all you need"" rocked the AI world and spawned GPT, GPT2, up to GPT4. Given the fifth largest GPU-based supercomptuer, a search engine's worth of data that people gave away, you can build a great AI mousetrap that can do the job students and corporate middle managers and pencil pushers. In a decade or two, programming complex programs with human-levels of bugs and mistakes will be a reality. We have a good mousetrap. Googles TPU v4 supercomputers use optics-based AI ASICs produced and patented by Samsung. I used to be a fan of Samsung, but then new management took over and it has dived off a cliff. Their appliances have the performance of robots bathed in saltwater, and I swore of Samsung phones after using them since forever when they started pushing shit malware games and apps onto my phone with every urgent security update, and every new phone increasingly locked app developers out of the real hardware, so you could use the Samsung app for a camera, but nothing else. But, and please correct me if I am wrong, I think Samsung may be the current leader in AI ASICs. I thinking GOOGL is still behind and their new Samsung-based supercomputer is the wrong mousetrap, but GOOGL programmers have certainly already been tasked with building a GOOGL v5 supercomputer that is dedicated to the transformers that underpin GPT through GPT4. I don't own GOOGL, but I may buy it during market corrections. Unfortunately, I don't have hundreds of K for an international account, and the Samsung OTC ticker doesn't correlate with Samsung's price in Korea and I don't trust it. I read about ETFs trying to reduce Samsung's weight, but I want an ETF that is dominated by it. Samsung still makes great sensors, invests heavily in research, and some departments within the fetid bureaucracy are still highly competent. So, please offer any ETF suggestions. I am fine with also investing in Korea's broader tech industry, despite the problems with Asian tech companies and their lagging China-based supply chains and the problems with China deciding that money is no longer free in its state-based kleptocracy economy. Please send me any tickers that I can research that may not have caught the current AI wave and that might still be well positioned for large future returns assuming that AI ASICs become the future, which I feel is a given. This might include companies dedicated to programming FPGAs and ASICs.","This is why AMD bought Xilinx. Xilinx is the leader in FPGAs.And $35,000 for a single Nvidia H100 GPU creates alot of incentive for data scientists & corporations to find cheaper alternatives, whether it's through training method or hardware (Founder of ChatGPT says large language models are already being phased out for smaller models that cost factors less). AI is still in very early stages. The costs will come down dramatically to the detriment of Nvidia. In 2007, it cost $10 Million to sequence a human genome; today it costs $200. For AI to be ubiquitous, the costs have to, and will come down.",0,0.547,0.947,NEGATIVE
