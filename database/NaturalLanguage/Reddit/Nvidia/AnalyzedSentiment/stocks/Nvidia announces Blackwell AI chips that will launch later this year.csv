date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Blackwell sounds like a code name for an evil ai from a scary video game,0,0.53,NEGATIVE,0.995
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Like the Blackwall from Cyberpunk,1,0.498,NEGATIVE,0.759
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","It’s not the Blackwall you should be afraid of, it’s what lies beyond it.",2,0.502,NEGATIVE,0.81
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Have no mouth,3,0.5,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","You know what, that's probably why I thought of it.",2,0.52,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",What makes you think it's not....,1,0.525,POSITIVE,0.996
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Its named after David Blackwell,1,0.503,POSITIVE,0.826
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Yeah, it's named after the mathematician, game theorist David Blackwell.",2,0.502,NEGATIVE,0.807
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",I think it’s named after David Blackwell. I know this because I used to live in a dorm called Blackwell.,1,0.512,POSITIVE,0.973
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",You lived in a dorm named after a future AI chip? I guess skynet really does send a terminator back.,2,0.535,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It’s in UC Berkeley. So I guess we should nuke Berkeley before things get out of hand.,3,0.504,NEGATIVE,0.826
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",I’m not opposed to that.,4,0.5,NEGATIVE,0.855
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","David H Blackwell is a luminary in the field of mathematical statistics, and a pioneer as an African American in the field of higher math. https://en.wikipedia.org/wiki/Rao%E2%80%93Blackwell_theorem",1,0.507,POSITIVE,0.992
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","this is why NVDA is the new apple in AI -- there is only one at the moment but competition is red hot, including those coming from startups",0,0.571,POSITIVE,0.993
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","this is why NVDA is the new apple in AI -- there is only one at the moment but competition is red hot, including those coming from startups",1,0.571,POSITIVE,0.993
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","this is why NVDA is the new apple in AI -- there is only one at the moment but competition is red hot, including those coming from startups",2,0.571,POSITIVE,0.993
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","there is only onelol when has apple ever been the ""only one"". You can argue they offered a more consumer friendly experience or a more premium experience but they have never been the the ""only one"".",3,0.526,NEGATIVE,0.774
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",When the iPhone was released and Android didn’t exist yet I guess ,4,0.496,NEGATIVE,0.989
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Ever heard of BlackBerry or Symbian OS based smartphones?I guess Apple was the one to remove the 3.5mm audio jack and the industry soon followed LOL. I’m being a bit harsh on Apple but let’s be honest, they are rarely the pioneers.",5,0.513,NEGATIVE,0.991
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Neither of those companies had phones with a touch screen like Apples which is what made the iPhone revolutionary.,6,0.515,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","They are Microsoft of the 90's, when MSFT went from 2.50 to 45 and went 18x. Microsoft dominated the home PC market by building a software infrastructure that was easy to use. NVDIA is dominating this new enterprise ""AI"" market with software infrastructure that is easy to adapt to.",3,0.573,POSITIVE,0.991
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",This,4,0.5,POSITIVE,0.954
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",his is why NVDA is the new apple in AIWhat does that mean?,3,0.587,NEGATIVE,0.94
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","I wonder when NVDA decides it wants to also lead in AI development, and give itself the benefit of the latest and greatest hardware at cost.",3,0.556,POSITIVE,0.936
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Including Samsung. Don't sleep on Samsung. They are an industrial powerhouse.,3,0.512,NEGATIVE,0.858
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","sure there are: https://arstechnica.com/information-technology/2022/11/hungry-for-ai-new-supercomputer-contains-16-dinner-plate-size-chips/https://www.zdnet.com/article/ai-startup-cerebras-unveils-the-largest-chip-yet-for-generative-ai/https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-codeit's a lot more expensive though, several millions, where with NV you can purchase one card and scale from there if you want. they use the whole wafer as one big chip.",2,0.545,POSITIVE,0.589
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Highly doubt they are profitable though. They are basically building a one huge chip. Building a big chip to gain performance is not new. But it is not free: (1) You are relying on the foundry to give you good yield (2) If you were to add redundancy to keep certain yield, then effectively your yield is still lower (3) Not everything scales with more advanced node. So if you are stuck with one advanced node, you pay more and get nothing in return.Most of the industry is moving towards chiplets to avoid building everything in one giant chip. They are moving complete opposite. ",3,0.529,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",afaik the graphics cards are all about parallelism (this is probably similar to them) and you can just turn off the paths that are bad.,4,0.505,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",There are no competitors currently NVDA has a monopoly on this space,3,0.511,POSITIVE,0.989
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",NVDA's problem isn't the competition. Its how much capex can these hyperscalers will spend buying their chips. So at the end how will companies make money from this and how often will they upgrade/buy new chips. Remains to be seen.,1,0.548,NEGATIVE,0.997
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Hmm I wonder, do you think they can charge a yearly fee per GPU to run the AI software?",2,0.55,NEGATIVE,0.995
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",There are no competitors lol,1,0.51,POSITIVE,0.927
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",,1,,,
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Jesus Christ…You’re not wrong but I was not expecting to see an American History X curb stomp reference today. Especially not such a graphic one.,2,0.518,NEGATIVE,0.97
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",,3,,,
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",We don’t care to understand. Buy,1,0.553,NEGATIVE,0.97
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",The second best database chip is the H200. NVDA is competing against itself (and I say this as an AMD shareholder).,1,0.57,POSITIVE,0.962
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","There is NONE.And with the introduction of NIM, it's going to get even harder, not impossible, to steal customers from Nvidia.",1,0.536,NEGATIVE,0.989
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","AMD's MI300 processors are actually quite competitive in many applications, offering higher performance than H200 in certain cases. There are limitations due to APIs and whatnot.This blackwell chip, after digging into the details, does not actually have any architectural improvements in throughput per die area (cost) or per watt (efficiency). They just scaled up hopper in a massive way, making it bigger (so that manufacturing is more costly) and throwing two on the same gpu. The biggest improvement was just adding an alternate mode for 4-bit and 6-bit computation, which is less precise and more efficient and becoming more popular in AI work, also very easy to implement. If AMD has any real improvements in the pipeline for their next generation it should outpace this.",1,0.542,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It would be like Nvidia being a top tier MLB team and their competitors being minor league teams. I’ve never seen a company dominate a field of tech like they are. ,1,0.561,NEGATIVE,0.989
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",5x the power for 2x the price (analyst estimate) - the inevitable semi cycle price compression begins,0,0.513,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","It's not really 5x the power though. It's like 2.30x the power for 2x the price in some cases, and in full precision workloads (which don't matter in AI) the new architecture is actually slower than previous.",1,0.54,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Really impressive stuff. Nvidia isn’t letting off the gas, if anything they’re pushing the accelerator down harder ",0,0.516,POSITIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Can't imagine what it would have been like to work there over the last 5 years. They have a hand in each of the worlds most powerful industries.,1,0.526,POSITIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",I can imagine that you’d suddenly be very wealthy haha ,2,0.5,NEGATIVE,0.631
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","True, but also very tired I imagine.",3,0.562,NEGATIVE,0.985
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Imagine working there over the last 5 years with stock options,2,0.613,POSITIVE,0.99
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It’s made the surrounding real estate in Santa Clara insanely priced (always has been high af),3,0.511,NEGATIVE,0.996
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Nvidians are a few of the people who can actually afford it… but just barely haha,4,0.496,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Companies like NV do RSU’s not options so it’s even better.,3,0.527,POSITIVE,0.998
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It’s actually not too different from being at the other big tech ai research labs. The breadth of the research is always awesome though. Got to meet a lot of scientists from places I’d never expect.,2,0.523,POSITIVE,0.958
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","thats how its done in this super comeptitive market; what is impressive is that they already anticipated this years ago, not just anticipating that ai will do well, but that there will massive competitions, and supposdely according to Jensen has already planned ahead.Competitions is scray tho, just take a look at groq ( not musk's grog) this is a hardware startup company that has made lots of head line as to how fast they can do generative models - test it out with LLama for yourself on their website - its not just hype bc that shit is fast...anyway my pt is that there are sharks circling evrywhere and very smart capable people are working day and night no this.",1,0.537,POSITIVE,0.965
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","It's much easier to make hardware for running an already trained model though, it's the training hardware/software that's hard to beat. You can only really train a model on nvidia at the moment but that model can easily (usually) be converted and deployed to run on intel/amd/arm. Lots of companies are going to be making big promises now for funding, but I've seen many make big claims before and disappear",2,0.535,POSITIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",This doesn't appear to be caused by competition. No one in the AI shovel market has meaningful marketshare. It's basically 99% NVDA. This appears more like an argument to just make a better generation so they can sell more billion dollar orders to the big tech companies,2,0.547,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",They're def going for as much market share as they can get!,1,0.578,POSITIVE,0.925
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",NVDA is currently down,0,0.508,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",You're going to make some kids very angry here,1,0.513,NEGATIVE,0.998
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",The volatility can be whiplash inducing.,1,0.502,NEGATIVE,0.976
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",AH low volume,1,0.515,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","The jump of performance isn’t bigger than investors had expected. Flops is 2.5x but each Blackwell chip is actually two dies that have been interfused, so that’s what 25% faster. That said, energy efficiency is a bit better, mostly due to the copper necking.The bigger takeaway is that Nvidia is going to try to eat share of its partners by selling systems as opposed to selling chips.Traditionally Nvidia has been selling chips and partners sold the GPUs (packaging the chips, the memory, PCBs, fans, etc). Nvidia started making its own cards with 3090 and company. It built on this foundation for A100 with the Redstone chips selling heatsinks and GPU boards. That said in 2016/2017 they did an 8x V100 system it hand delivered by Huang to OpenAI.Blackwell takes this to the next extreme. Selling the entire rack. This means all the cost associated with it they get to sell. So instead of selling a chip for $20-25k they can sell a system rack for $5-10M.",1,0.549,NEGATIVE,0.863
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Each die has 2.5X FLOPs,2,0.5,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Oh shit you’re right! Bad job on the presentation, it wasn’t obvious.That said, H100 is 6x times the FLOPS compared to A100 yet the transformerxllarge benchmark is only 73% faster. Comparing the pricing of H100 to A100 isn’t 6x, it’s closer to 50-100%.That said, there is another performance boost due to 192GB HBM. So that might add another 20%.But the real value driver here is probably the less need for sharding large models given you can fit more on 192GB and faster interconnect. Another part is the power efficiency but that’s a minor part.All in probably roughly the same jump as Ampere to Hopper. So if price is $50k, most of the performance gain is captured by Nvidia in the price as opposed to given to the customer. If they go for $40k it will be a nice split of gains to Nvidia and to the customer. There is also increased cogs to make them for sure, so I can see nvidia probably going to land around $45k for Blackwell. If they go with $40k they will have to come down on Hopper to like $25k. But I doubt they will come down that far.",3,0.535,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Sell the news.,1,0.56,NEGATIVE,0.991
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",They pretty much got a monopoly atm on selling pickaxes and shovels in the gold rush of the 21st century,0,0.505,NEGATIVE,0.981
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","With NIM, a bit of the software ecosystem got added to their portfolio.",1,0.524,POSITIVE,0.995
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",This feels like the real story that will develop over the year or two. They are building an ecosystem that’s going to generate a ton of recurring revenue.,2,0.562,POSITIVE,0.994
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It seems NVIDIA just secured trillions of recurring revenue by being not only the go to hardware shop for AI but the go to software platform to build custom AI,0,0.565,NEGATIVE,0.676
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Trillions of recurring revenue?!Right now consensus is $100B by 2025.That’s a looong way to go,1,0.512,POSITIVE,0.579
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Yup!That's what NIM makes possible. Going to make even older GPUs useful for devs.,1,0.509,POSITIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","On top of that, they are able to sell GPU to Microsoft Azure and resell the NIM and Omniverse software APIs from Azure to global customers powered by their chip. This is like Apple’s iPhone and iOS App Store but way bigger.",2,0.534,NEGATIVE,0.979
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Sounds very similiar to the Blackwall firewall used to keep out rogue AIs in the Cyberpunk dystopian future setting. https://cyberpunk.fandom.com/wiki/Blackwall,0,0.507,POSITIVE,0.609
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",The Blackwall is an AIMight be time to pick that game up for another run,1,0.515,NEGATIVE,0.997
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Be careful when googling Blackwell, especially image or video search. If you see the name Ruth Blackwell, you're not in the right spot. And I mean that, btw.",0,0.505,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Spooky.,1,0.5,POSITIVE,0.984
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Cyberdine systems has entered the chat,0,0.522,NEGATIVE,0.839
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Some impressive stuff. I remember when nvda was dumping following china ban news, the sentiment was all negative. The popular opinion was that this was still overvalued under $125 and only worth buying under $50. Amd, intc are catching up, big companies designing their own chips, nvda doesn’t have moat, etc. Its just crazy how outlook shifts",0,0.532,NEGATIVE,0.691
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Same, had my finger on the buy button at the $120 range but let the FUD get to me",1,0.529,NEGATIVE,0.997
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Got me as well. I had no position and wanted to get in. Wanted 100 shares but end up getting 25 at avg of $124 and then waited for under $100 to get full position which never happened.,2,0.527,NEGATIVE,0.746
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",At least you got some! I sold a $110 put and it never got assigned lol. I just kept hearing everyone saying it was going to $50 etc and bought into it. Ah welll,3,0.507,POSITIVE,0.996
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Nvidias moat is quite deep, I’d encourage one to do more research.",1,0.514,NEGATIVE,0.582
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",It is and it’s widely recognized now. Im just amazed how most were discounting their moat when stock price was falling.,2,0.551,POSITIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Who is producing all of these? And why TSM is not going up smh…,0,0.515,NEGATIVE,0.888
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Their margin is very low so no new competetors shows up, this is intentional so they stay at the top",1,0.557,POSITIVE,0.624
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",I called it. Everyone kept saying competitors will come up with something groundbreaking.Seems Nvdia is on the top of their game.,0,0.513,POSITIVE,0.981
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Nvidia have got the whole ecosystem sewn up at the moment. When I was at uni, Cuda had just come out and opened up GPUs for general processing. The rival technology used by Intel and AMD is OpenCL, it does the same as Cuda only not as well and no one wants to implement the same thing twice in two different languages when one of them is worse, that's why all the training software is written for cuda and Nvidia. That's the main thing that's kept AMD out of the space, AMD GPUs were just as good as nvidia and still nearly as good at the moment. Intel have been failing for years to get into the gpu market even before the ai stuff started. They have their hands full not losing the cpu market at the moment. It baffled me why AMD didn't invest in paying software engineers to add support for their hardware to the training software. I nearly put £1000 in nvidia around 2010 when it was $15ish, but tossed a coin and went intel instead, not bitter about it at all of course :D",1,0.558,POSITIVE,0.533
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",H100 obsolete cancel my order,0,0.554,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Anyone know release date for the T-800?,0,0.54,POSITIVE,0.845
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","I increasingly feel like one day NVDA at $850 is going to seem like a total bargain. In the same way many regarded Apple as overvalued throughout their peaking growth in the 2010's only to go 2x, 3x, 4x yet again. $2 trillion market cap already feels insane, but it wasn't long after Apple shattered the $1 trillion mark that it hit $3 trillion.If NVDA can maintain their dominance in this space (of which I am admittedly skeptical), it's going to be a situation where the biggest tech companies on earth are routinely placing massive hardware orders for each new generation. In addition to the many other companies that are likely to spawn out of AI tech if it lives up to the hype in application.",0,0.534,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","1000x faster in 8 years, but for normal consumers 24GB of Vram is enough :(",0,0.514,NEGATIVE,0.908
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",you really have to say Nvidia is a great company but for the price its for me a no i recently trackt down the future cash flows and the multiple projects that nvidias cash flows are going tho gro 40% Anually for the next 10 years i dont think this is going to happen,0,0.554,NEGATIVE,0.971
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Forward P/E is 28. That is insanely cheap.,1,0.523,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","This is good to see, but I wonder how adoption will look especially if they’re offering NIM as a way to convert exisiting customers into more heavy users. TSMC continuing to be the manufacturer is good for that stock too.",0,0.544,NEGATIVE,0.836
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",The numbers are so staggering that it feels made up…,0,0.504,NEGATIVE,0.544
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Down roughly 6.5% after hours, anything to do with the keynote?",0,0.505,NEGATIVE,1.0
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Do you mean 1.5%?,1,0.505,NEGATIVE,0.897
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html",Probably people selling the news,1,0.516,NEGATIVE,0.998
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Buy the rumor, sell the news.",1,0.579,NEGATIVE,0.999
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Their announcement was roughly in line with expectations, so no strong reason to sky rocket.",1,0.535,NEGATIVE,0.975
,"Nvidia on Monday announced a new generation of artificial intelligence chips and software for running AI models. The announcement, made during Nvidia’s developer’s conference in San Jose, comes as the chipmaker seeks to solidify its position as the go-to supplier for AI companies. Nvidia’s share price is up five-fold and total sales have more than tripled since OpenAI’s ChatGPT kicked off the AI boom in late 2022. Nvidia’s high-end server GPUs are essential for training and deploying large AI models. Companies like Microsoft and Meta have spent billions of dollars buying the chips. The new generation of AI graphics processors is named Blackwell. The first Blackwell chip is called the GB200 and will ship later this year. Nvidia is enticing its customers with more powerful chips to spur new orders. Companies and software makers, for example, are still scrambling to get their hands on the current generation of H100s and similar chips. “Hopper is fantastic, but we need bigger GPUs,” Nvidia CEO Jensen Huang said on Monday at the company’s developer conference in San Jose, California. “Let me introduce you to a very big GPU.” The company also introduced revenue-generating software called NIM that will make it easier to deploy AI, giving customers another reason to stick with Nvidia chips over a rising field of competitors. Nvidia executives say that the company is becoming less of a mercenary chip provider and more of a platform provider, like Microsoft or Apple, on which other companies can build software. “The sellable commercial product was the GPU and the software was all to help people use the GPU in different ways,” said Nvidia enterprise VP Manuvir Das in an interview. “Of course, we still do that. But what’s really changed is, we really have a commercial software business now.” Das said Nvidia’s new software will make it easier to run programs on any of Nvidia’s GPUs, even older ones that might be better suited for deploying but not building AI. “If you’re a developer, you’ve got an interesting model you want people to adopt, if you put it in a NIM, we’ll make sure that it’s runnable on all our GPUs, so you reach a lot of people,” Das said. Every two years Nvidia updates its GPU architecture, unlocking a big jump in performance. Many of the AI models released over the past year were trained on the company’s Hopper architecture — used by chips such as the H100 — which was announced in 2022. Nvidia says Blackwell-based processors, like the GB200, offer a huge performance upgrade for AI companies, with 20 petaflops in AI performance versus 4 petaflops for the H100. The additional processing power will enable AI companies to train bigger and more intricate models, Nvidia said. The chip includes what Nvidia calls a “transformer engine specifically built to run transformers-based AI, one of the core technologies underpinning ChatGPT. The Blackwell GPU is large and combines two separately manufactured dies into one chip manufactured by TSMC. It will also be available as an entire server called the GB200 NVLink 2, combining 72 Blackwell GPUs and other Nvidia parts designed to train AI models. Amazon, Google, Microsoft, and Oracle will sell access to the GB200 through cloud services. The GB200 pairs two B200 Blackwell GPUs with one Arm-based Grace CPU. Nvidia said Amazon Web Services would build a server cluster with 20,000 GB200 chips. Nvidia said that the system can deploy a 27-trillion-parameter model. That’s much larger than even the biggest models, such as GPT-4, which reportedly has 1.7 trillion parameters. Many artificial intelligence researchers believe bigger models with more parameters and data could unlock new capabilities. Nvidia didn’t provide a cost for the new GB200 or the systems it’s used in. Nvidia’s Hopper-based H100 costs between $25,000 and $40,000 per chip, with whole systems that cost as much as $200,000, according to analyst estimates. Nvidia also announced it’s adding a new product named NIM to its Nvidia enterprise software subscription. NIM makes it easier to use older Nvidia GPUs for inference, or the process of running AI software, and will allow companies to continue to use the hundreds of millions of Nvidia GPUs they already own. Inference requires less computational power than the initial training of a new AI model. NIM enables companies that want to run their own AI models, instead of buying access to AI results as a service from companies like OpenAI. The strategy is to get customers who buy Nvidia-based servers to sign up for Nvidia enterprise, which costs $4,500 per GPU per year for a license. Nvidia will work with AI companies like Microsoft or Hugging Face to ensure their AI models are tuned to run on all compatible Nvidia chips. Then, using a NIM, developers can efficiently run the model on their own servers or cloud-based Nvidia servers without a lengthy configuration process. “In my code, where I was calling into OpenAI, I will replace one line of code to point it to this NIM that I got from Nvidia instead,” Das said. Nvidia says the software will also help AI run on GPU-equipped laptops, instead of on servers in the cloud. Source: https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html","Actually, lust over 1% so far.People who were expecting a big pop because of GTC are not going to be happy, if it continues like this.But this is only AH, so no guarantee if won't shoot up tomorrow.",1,0.516,NEGATIVE,0.97
