date of comment,main comment,comment,depth,sentiment,PTR Sentiment,Flair Outlook,Flair Sentiment
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",GPT-REEEE,0,,0.5,NEGATIVE,0.999
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","I didn’t read everything as I fell over at the bit about not being truthful about tokens. You clearly misunderstood the difference between an api limit and inference speed. The 450 token per second comparison to 20 is pretty much correct . Just go try it out on each service and you can see for yourself.There are many negative aspects to this ipo , I’d say the main one is virtually all the income coming from 1 customer but I think they have a unique take on speed and even tsmc is trying to develop full wafer solutions themselves so it probably has a future.I would have preferred they waited a couple more years for revenue to develop before an ipo.Disclaimer. I am invested 20k pre ipo and will be buying more if there is a dip.",0,,0.517,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Lmao this guy really did a whole DD based on⁠The inference side when all the revenue is training at the moment and⁠Used rate limits as “speed”He really belongs here 😭,1,,0.511,POSITIVE,0.671
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Yeah . I doubt he’s even bothered to read the sec filing which of course is going to be v biased but at least the info is there. There a good discussion herehttps://news.ycombinator.com/item?id=41702789with possibly less biased opinions both ways which is worth a read for anyone interested.,2,,0.511,POSITIVE,0.53
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Thanks for this link - YC definitely tends to have better discussions than here. The NVDA stock sub is even worse. I own mid 6 figures of NVDA and the people in that sub make me wanna exit completely lmfao,3,,0.522,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",What? You don’t like the “Buy the Dip!” Convo?,4,,0.5,NEGATIVE,0.989
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","nah, not really.",4,,0.502,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","I mean nothing in that yc talk is really saying anything useful. Here's one quote. A legion of lawyers huh.I don’t think their value add is simple 'single wafer' with all other variables the same. In fact, I think the block and system that gets the most out of that form factor is the secret sauce and not as easily replicated—especially since the innovations are almost certainly protected by an enormous moat of patents and guarded by a legion of lawyers.This comment is poignant and well... Why haven't that submitted the MLPerf results?At the end of the day, Cerebras has not submitted any MLPerf results... That means they are hiding something. Something not very competitive.",3,,0.512,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","I think on Reddit and other forums you will always be able to selectively find an opinion which doesn’t add much to the discussion. As you yourself have shown there is useful information there too and I prefer to look at everything both negative and positive and come to my own conclusions .For example the following link to a deleted video I hadn’t come across in the couple of hundred hours of research I have donehttps://web.archive.org/web/20230812020202/https://www.youtube.com/watch?v=pzyZpauU3IgI don’t know why the video was deleted , perhaps the female boffin wasn’t photogenic enough…What cerebras have created is not trivial and not really that simple to recreate quickly . For sure it’s going to be expensive but it’s more in the league of supercomputing. Whether there is a market for it remains to be seen.Don’t get me wrong , I’m not a cerebras fanboyThe chances of them taking significant market share from nvidia is slim but I don’t believe it’s non existent.The main concern most people have is the g42 involvement and bulk of revenue. I think it’s a legitimate concern and will probably be the main reason for a fall in share price after ipo but I myself am not particularly worried by it. I think g42 have probably got a great deal , better than anyone else will get and they will stay on board while cerebras gets more customers.All just my own opinion, like everyone else on Reddit etc I could just as easily be wrong as right !!",4,,0.518,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Thanks for sharing that video link. I hadn't seen that before. I am a pre-IPO investor in a similar situation as yourself and I try to get as much info as possible. My guess on that video is that it was deleted because someone thought it revealed too much (too late, I guess). But I could see myself deleting it too, why give everyone a complete roadmap of your proprietary tech. Sure, they could figure it out anyways but let them spend the time and resources.You nailed the main risk on the company, the customer concentration is enormous, that is something they will need to work to rapidly alleviate, but part of the problem here is there aren't that many entities that can write these checks and need the product bad and Nvidia has a lock on the 4 or 5 regular joes stateside, and those guys aren't gonna turn to a startup for many good reasons.So it's either you take on the rare big elephant when you see it, or you remain in little league. Nvidia, which is orders of magnitude the size of Cerebras also has high customer concentration. Perhaps as inferencing becomes a bigger part of the AI process there will be some changes on that in the sector as smaller players buy a bigger share of the sector output.But for Cerebras the combo of just 1 customer at the scale that is relevant for the IPO + the geopolitical risk of that customer is too high to waive off. I think as investors one just takes known facts into account, pick spots not get too greedy, you know.The OP didn't understand many things. The idea that if US reduces export controls this is good for Nvidia and bad for Cerebras is incorrect. Actually, for G42 in particular, which is not just a customer but an investor in Cerebras, it likely accelerates Cerebras sales. People need to do their DD before spending a lot of time writing stuff.",5,,0.529,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",I don't get what you mean by 1. For microsoft? I assure you that is not true in the least. It is inference as that is a constantly running thing in perpetuity as if they are literally internet rest calls; which they are. For 2 I explain my response to that above. You're right but I have to relate them because of the nature of the queries (token size) and responses.The inference side when all the revenue is training at the moment and,2,,0.509,NEGATIVE,0.996
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",For Cerebras - the company you’re doing DD on.Their G42 contract and all that revenue is building data centers focused on training large models. They didn’t even have an inference offering when that G42 deal happened.,3,,0.564,NEGATIVE,0.989
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","I applaud your comment. very transparent. I have not used cerebras so I was just going by the marketing on their webpage. What I am saying is that the information and their comparisons are very outdated as Microsoft has aggressively opened the throughput for the OpenAI models.One thing I will quibble with is that api limit is related to inference speed because it is throughput. In a way it is expected throughput as well. Here's what I mean. Previously when you used LLM's in Azure you would get a warning for tokens you could generate per minute (if you went over the limit) and then outright rate limiting would 404 your requests going forward. Now, to your point latency is what we are really talking about here. What I can tell you is that each model has its own latency expectations that Microsoft never publishes because if you're on a pay as you go service you are effectively on a shared service.What I can tell you is that 4 was terrible, 4-turbo was a little better, 4o is a lot better and 4o-mini is perfect. In terms of latency. 4o-mini is a smaller param model.Tokens are complex especially in an enterprise environment. I am passing thousands of tokens through expecting thousands of tokens back on complex queries.With that said, if you really are comparing apples to apples (small model versus small moel I think their claims fall apart and that's my point.",1,,0.52,NEGATIVE,0.997
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Those 30M tokens/m are API limits, where multiple instances can be used, not how fast a single GPU can vomit out tokens, lmao.",0,,0.534,NEGATIVE,0.917
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Nobody uses Azure on a shared or dedicated service with a single GPU. You're missing the point of how this works. GPUs are typically connected through NVLINK and networking, which increases throughput and reduces latency. With more powerful and interconnected GPUs, latency decreases, and throughput improves. In this way, throughput and latency are related, especially when processing complex models. Since the new GPT-o1, many factors influence speed and throughput behind the scenes, and they do indeed relate.",1,,0.518,NEGATIVE,0.955
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","What? Ok, sure, but you are comparing apple to oranges.Why dont you try to call the 4o api with a question that require a long answer, measure the time it took to generate+stream, and then count how many tokens it was thus obtaining the tokens/s… you will se it is nowhere near the ridiculous figure of 500 fucking thousands tokens per second!",2,,0.514,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Yes but ultimately even if a service can meet the same performance with a different means doesnt mean it is being done as efficiently... ,2,,0.524,NEGATIVE,0.661
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",As of a year ago Nvidia had to re-allocate $5 Billion worth of chips because of export rules. Who knows how much it would be worth now.https://www.tomshardware.com/news/nvidia-to-re-allocate-dollar5-billion-worth-of-gpus-thanks-to-us-export-rules-report,0,,0.556,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Have you studied why their Chips are faster than Nvidia per watt? Nope,0,,0.546,NEGATIVE,0.996
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",They are not. Also their doing purely dumb shit with non yielding non scaling production which is why the chips ate so expensive. The inference trick and I know you won't know what this means is all a memory hack for a small param model.They're all smoke and mirrors.,1,,0.522,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Actually openai says that training and inference are now on par and there is a paradigm shift to more inference for newer models, something you missed.Another point you are missing is that a company is making faster chips with a valuation 200x smaller than the leader who cannot manufacture enough chips for the demand.Disclosure: I'm invested in Cerebras already.",2,,0.547,NEGATIVE,0.502
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Training will never be on par with inference until models can be trained instantly, which is just a fact. But you're right that workloads are shifting more towards inference. That said, the models doing the inference still need to be trained. Training is time-based, while inference is an ongoing, aggregate usage over time. As chips get faster, training times will drop, and inference will grow. But with future data and models being larger and more complex, who knows how training will pan out. At some point, training will be 'good enough' and modular training will come into play, allowing real-time updates, while inference handles the queries and keeps adding to that long-term memory.",3,,0.529,NEGATIVE,0.971
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","If you are an institution or anyone that is just using the models (most of the world) you need inference, not training. All humanity will use inference for some task or another, not training. Where do you think the money is?",4,,0.517,NEGATIVE,0.998
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","where/ how did you invest into Cerebras, if you don't mind me asking? Do you perhaps know the that time valuation?",3,,0.525,NEGATIVE,0.853
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Equityzen, 2.5bln",4,,0.5,POSITIVE,0.525
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","when was that if you don't mind me asking? It's rumored to be at ca. 4bn now, although i presume that it will grow fast given the value of their competitors.",5,,0.511,NEGATIVE,0.974
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",The IPO is happening at 7-8 I think,6,,0.513,NEGATIVE,0.836
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Lame company will go bankrupt after IPO, NVDA is killing AMD and Intel so what can a tiny newbie offer?",0,,0.617,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",It's literally nonsense funding from UAE and cnbc is all over it,1,,0.5,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","everything I had read of Cerebras throughout this year that even was focused on any positives they may have is that even if they could be a premium provider of bespoke solutions, there’s a huge leap from having the design for an incredible (and incredibly expensive) chip architecture, there’s a huge leap from that to actually making that a scalable hardware business — let alone adaptable beyond bespoke clients with a software solution to match.that said, competition is good so i think anyone that is bullish on Nvidia should hope it has rivals (well, aspiring rivals) that succeed and push them.",0,,0.555,POSITIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","The more you look, the more you find hype and pimples.",1,,0.508,NEGATIVE,0.999
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Think i can read all that? Calls or puts? And extra ketchup,0,,0.504,POSITIVE,0.917
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Ultimately it comes down to the $600 Billion question: https://www.sequoiacap.com/article/ais-600b-question/,0,,0.511,NEGATIVE,0.613
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",This article has a point but remember all the top AI players are racing to get to AGI. The first one to get there will dominate EVERYTHING. Not just one sector. It's all the sectors. It is a winner takes all game. That's why they are spending billions of dollars for infrastructure build out. 600 billion will look like a drop in the bucket at that point.,1,,0.526,POSITIVE,0.945
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","that won't happen with the current LLMs for sure.Also, which nootropics do/did you use? :)",2,,0.566,NEGATIVE,0.99
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",100% agree LLM on its own won't get to AGI but the hardware compute will play a part in getting there.Lions mane 1gram + Vit D3 15000 iu daily changed my life. ;)Tried a lot of things but those two have been consistent for me and backed by research.,3,,0.516,POSITIVE,0.963
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Lions mane 1gram + Vit D3 15000 iu daily changed my life. ;)changed how, please tell?also, the human brain uses cca. 20W of power, is rather slow frequency and supposedly uses some quantum effects to do its thing (conscience maybe?) so i don't know... IMO we'll need quantum computers for AGI or some other breakthrough.",4,,0.51,NEGATIVE,1.0
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.","Better memory, better mood, better sleep which sounds basic but the gains compounded. The better memory part was surprising for me. I’ve had friends and family that experienced the same on Lions Mane.Agree that we’ll need a number of breakthroughs to get to AGI. In the meantime, im just gonna keep buying NVDA calls 😂",5,,0.511,NEGATIVE,0.704
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Oh god. That article is stupid for real.,1,,0.508,NEGATIVE,0.999
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",You probably didn’t even read the whole thing lol ,2,,0.495,NEGATIVE,0.99
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",Is it possible that OpenAI could buy Cerebras? Sam Altman said that they were looking around for an AI hardware company to acquire.,0,,0.651,NEGATIVE,0.99
,"The Middle East wants chips badly but US restrictions prevent the US most powerful chips entering into certain geographical areas such as China and the Middle East; this entails the UAE. Nevertheless, the UAE wants in on the GPU chips craze and is apparently willing to pay anything to enter into the chip bonanza. The UAE can't get Nvidia's, AMD's, or Intel's best GPU chips so instead they are investing heavily into alternatives such as Cerebras. There is nothing inherently wrong with this but it's also the biggest red flag as an investment opportunity. If the agreements for Nvidia chips open up in the Middle East and specifically with the UAE that could be a crushing blow to this aspiring startup. Specifically, Cerebras Systems reported a net loss of $66.6 million for the first six months of 2024, on $136.4 million in revenue. For the same period in 2023 it has a net loss of $77.8 million on just $8.7 million in sales. 87% of this revenue for the first half of 2024 was directly from the UAE G42. The other red flag from this startup is the way in which they promote their business. It's all seemingly smoke and mirrors and conveniently based on outdated GPU pricing and throughput information; which is very publically available. For some reason, Groq and Cerebras love to keep using memory to unlock speeds on small/tiny models which is impractical and inefficient for a scaled system; or a system that is a large foundational LLM. As well, they have no clue what models will do next so it's a major after the fact architecture that uses llama because they have access to it. https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed. A prime example of this is OpenAI's GPT-o1 model that uses reasoning in coordination with its model capabilities. Because they are not being used on the forefront of this technology they don't know how or when a model's size, function, or needs will evolve into the future. All of this plus the pricing from OpenAI and Microsoft is coming down exponentially. For Example: They are referring to a 70b param model that shoves an entire model onto memory. They are going up against h100's which is a very old technology at this point. They make no reference to h200's let alone blackwell Because they are referencing such a small model the pricing model they suggest would be radically different for a 400b param model and forget about trillion param models which are coming next. They're not being truthful about tokens per s. As of today this is Azure GPT 4o and GPT 4 mini tokens per minute gpt-4o & GPT-4 Turbo global standard Model Tier Quota Limit in tokens per minute (TPM) Requests per minute gpt-4o Enterprise agreement 30 M 180 K gpt-4o-mini Enterprise agreement 50 M 300 As you can clearly see 30 million tokens per minute is 500k tokens per second and mini is 833,333 tokens per second. So i don't know why they are referring to 20 tokens per second or their 450 tokens per second seems way off. maybe they mean million. Even if that is the case and 70 b would be more like mini it is way higher than their limit. On pricing which they lay out a 3:1 input versus output is fine the price would be for mini which is a comparable model is roughly .10 cents (input) + .20 cents = .30 cents. Per million. for regular 4o it would be higher and let's face it GPT 4o is a far superior model than llama 3.1 3.33 dollar + 5 = $8.33 Source: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ From this information what I can tell you is that what we just went over is pricing. it is not some guarantee for what a model produces per second. That shit is very random. What I can tell you is from GPT 4 to GPT 4 turbo to Gpt 4o the speed is dramatically better. GPT 4o mini is damn near real time. Take that for what it's worth. I am not saying they're being dishonest here but I am saying they are being very cheeky with how they advertise things. The company is currently being primarily supported by the UAE, with investments worth roughly $900 million for new AI supercomputers known as the Condor Galaxy series. Any growth here is singular to this source of investment and not driven by organic growth or usage. This is not a competitor to Nvidia but rather a temporary solution in the Middle East until chip embargoes are alleviated. The media here loves to use sexy headlines on non-technical verifications of what it is they are actually comparing. I.e. H100's are now old, or The fact an entity may need to serve millions of clients... No they are instead reporting self-prompting headlines from Cerebras that say things like our chips are 20x faster than Nvidia. Live Update: As I am writing and researching this topic it has just been reported by Reuters that the U.S. is setting a new rule to allow chip shipments to the Middle East including the UAE which is a boon to Nvidia and Microsoft. US sets new rule that could spur AI chip shipments to the Middle East Here are couple excerpts: WASHINGTON, Sept 30 (Reuters) - The U.S. Commerce Department on Monday unveiled a rule that could ease shipments of artificial intelligence chips like those from Nvidia Corp (NVDA.O), to data centers in the Middle East. G42, a UAE-based AI company with historic ties to China, has been a focus of those concerns. In April, Microsoft Corp. (MSFT.O), opens new tab announced that it would invest $1.5 billion in the company, with plans to provide G42 with chips and model weights, sophisticated data that improves an AI model's ability to emulate human reasoning.The deal drew scrutiny from China hardliners in Congress, even though G42 said in February that it had divested from China and was accepting constraints imposed on it by the United States to work with American companies. LOL you can't make this up. Literally this just got reported by Reuters today 9/30/2024. This completely aligns with my argument above regarding the UAE-G42. With chips now entering into the Middle East from Nvidia and potentially others I don't know how this startup IPO makes it off the ground. I don't mean to be bearish but I don't think this is the time for them to raise an IPO without showing more progress. I could be wrong. As of now, I don't plan on buying any shares. Instead, I will be adding more shares into Nvidia because now this is bullish news for Nvidia.",No because it is trash technology,1,,0.618,NEGATIVE,1.0
