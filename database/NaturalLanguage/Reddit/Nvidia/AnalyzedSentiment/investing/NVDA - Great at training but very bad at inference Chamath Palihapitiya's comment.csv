date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","It‚Äôs not ‚Äúbad‚Äù at inference, but rather GPU is just not as optimized for inference than training. Inference doesn‚Äôt need high precision, maybe can even get pass with int4, while training typically need fp32, maybe in some rare cases tf16.There are going to be a lot more inference chips demand than training, but I think those will be lower margin business. You have ASIC, FPGA, and GPUs all competing in this sector, whereas for training, you‚Äôre more likely to see ASIC and GPUs.",0,0.509,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",Do you think a small scale SoC like Tegra can address the coming demand for inference?,1,0.509,NEGATIVE,0.583
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","I think it‚Äôs likely that nVidia has hundreds of people knowledgeable about the details and working on their own chips as well as many startups (Groq, Cerebras)addressing the same problem. The high end training side is harder with less competition and nVidia will stay dominant there.Another approach, Apple with their ARM instruction set chips has been adding additional ‚Äúneural engine‚Äù hardware to their CPU which supports low latency (no memory movement to coprocessor) inference at moderate performance. Likely cloud providers will do the same.Intel as well. Downside is compiling for many chip variants with varying hardware capabilities.",2,0.521,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",No. For smaller models they‚Äôll run on your phone/computer. Larger models need more processing power than what the Tegra is built for,2,0.509,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","I don't think Tegra is a great platform for inference, given the lack of bandwidth, performance, and VRAM. Something like AGX Xavier would've been better.It all depends on the actual workload, but I don't think there's a large scale workload lower than what AGX Xavier can handle.",2,0.505,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Thanks. So - one difference bw training and inference is also that inference is latency-sensitive (e.g., self-driving car) and should be energy-efficient (e.g., smartphone offline LLM prompting). Training phase doesn't care too much about latency and energy use as long as costs are reduced.But it seems to me that NVidia can easily design inference chips given their huge skills in the (more challenging) training chip design. Also based on your comments, inference seems to be the easier problem.",1,0.529,POSITIVE,0.931
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Correction: training is extremely latency and bandwidth sensitive, because you want the all the chips to talk to each other, and maintain coherency through the computational phase. This is why companies are willing to drop hundreds of thousands of dollars on massive systems like DGX or HGX with NVLink.While it is true that NVDA can design a competitive inference chip (and they‚Äôve already done that, with Jetson, AGX, A2, L4, etc), and some customers‚Äô workloads absolutely require significantly more computational power than a typical ASIC, GPUs in general isn‚Äôt really the best for inference, especially considering performance / wh or performance / $. But again, since the barrier to entry for inference is so much lower, it‚Äôs far easier for companies to jump in, thus pushing margin low.",2,0.51,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Great points. Thanks.I guess that's why NVidia attempts to provide end-to-end software solutions such as the GR00T foundational model and Omniverse. So they keep customers locked in their ecosystem.If we're talking about reducing the barrier to entry in, say, humanoid robotics, you're essentially forced to go for NVidia's Training+Inference solution assuming you don't want to start a billion-dollar training project yourself as a small startup.In other words, even if you can provide a low-cost inference solution - most companies will already be locked into NVidia's Training+Inference+Data end-to-end software solution so you won't be able to get these customers.",3,0.542,POSITIVE,0.992
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","It's less the memory but instead the parallelism that the GPUs provide. Don't get me wrong, the more memory the more you model can handle, but you can also solve that with some techniques. The biggest thing is blasting through multiple layers of a model, for that you need parallel processing that can compute all the node values at once.It's still a problem for inference, but you're not doing O(n) number of calculation, you're just doing O(1) because you're only going through your model once. I haven't seen anyone batch together inference calls, which would make the GPU more efficient than the CPU, so their GPUs are over kill for inference, as keeping them running isn't worth the demands on them.",1,0.513,NEGATIVE,0.999
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Not at all. VRAM is extremely crucial for inference, especially if you‚Äôre talking about modern AI models like LLM, image classification, stable diffusion, or predictive analytics. Some models are so large that the minimum memory needed is 16G. For instance, even for a relatively small LLM model like Mistral-7b, with the smallest quantization of 4bit, a 4070Ti with its 16G would struggle.And inference isn‚Äôt just running the model once. It could mean running the model multiple times until a usable result is achieved, like in the case of LLM and RAG.",2,0.508,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Chamath is known for bullshitting. Even if he tells you the sky is blue, he's probably lying. Don't listen to anything he has to say on any subject.",0,0.498,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Well, I wouldn't like to buy his VC crypto shitcoin bags for sure.",1,0.534,NEGATIVE,0.992
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","I dont think he's in crypto, its all of his spac companies that are bs",2,0.519,NEGATIVE,0.993
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",I wouldn't trust Chamath to speak honestly on anything and would instead assume this means he's invested in a company on the inference side of things.,0,0.556,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Haha, yeah that's exactly what's happening here I guess: https://groq.com/",1,0.506,NEGATIVE,0.999
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",Chamath is a grifter clown,0,0.5,NEGATIVE,0.988
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Came in here to say pretty much the same thing, Chamath is a witless moron who got lucky once and has continually failed in everything he's done since.",1,0.504,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","It's bullshit. I work in AI for machine vision, and there is nothing faster for inference than an Nvidia chip. For time-critical applications where you need the result ASAP you choose Nvidia. Otherwise if you have more time you can go with inference on CPUs, integrated GPU, etc - you'll be slower but of course cheaper.The reason it is so much more obvious with training is because training a network - depending on complexity - can take hours or even days, whereas the inference is a matter of (milli)seconds. If you save me 3 hours I will be prepared to pay more for your chip compared to if you save me 300ms. But again this depends on the demands of the actual situation.",0,0.52,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","That's a great comment, thanks! I think many people are still mid-curving it with Nvidia - and miss out on the great investment opportunities (e.g., https://youtu.be/EU3CfiQp3AE?si=izFfv6kV5EncZ0oC)Thanks for the outstanding perspective by an industry insider - that's why I love Reddit!",1,0.516,POSITIVE,0.87
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","At some point moving data to GPU will become expensive compared to CPU inference in the same memory space, right?",1,0.512,NEGATIVE,0.87
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","To infer anything using a statistical technique you first need to train the model, the training is the hard part and the inference is a direct product of the training. He's chatting shit as far as I can tell.",0,0.511,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","This is not bullshit, just look at groq, they produce hardware that is 10x faster than anything Nvidia has for inference, however the amount of accelerators you would need for training would be unfeasible",1,0.518,NEGATIVE,0.59
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Fair enough, I still struggle to see how inference is going to be ""100 times bigger"" than training. Like you mentioned, the compute intensive and rate limiting part will always be the training no?",2,0.51,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Right now everybody is rushing to get to a product that is obvious, a product that is rote memorization and actually just a fancy chat relay bot from the 90s with a better database rule set (instead of creating the whole system, a library is created which has internal rules for choice in said system, same thing though). Inference is the first true step to AI, though still nowhere close. And no, we don‚Äôt have inference yet, so yeah, it‚Äôs a much bigger mountain that has a valley we don‚Äôt even know on the other side - that will be practical use, the actual shovel being used in actual AI, that is what he‚Äôs saying.",3,0.532,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",So the prediction is that inference will become a more active and computationally intensive step in the process and less of a lightweight application of the parameters produced by training to some test/validation/wild data?,4,0.511,NEGATIVE,0.998
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","he‚Äôs discussing the market, right? He‚Äôs stating their training methods are not designed to capture the market properly or will require far more work to fine tune to specific needs and that making that part better is the right target.Why do you think that means intensive as a singluar concept - let‚Äôs say you need to train all Y to do X, but then Y is used in many concepts by Z companies. X is trained right now, Y is not yet but has more Zs and more market cap than Xs, and more are needed because there are more end users (think a POS management system, needs a lot of things that do sales, so one training, but the interfacing with novel concerns depends on company, so numerous interface models needed).He is saying that the actual commercialized step is interface, and that the commercialization will be far more, as a whole, intensive and market shared cap setting than just the first phase of the whole concept. He‚Äôs not downplaying training one bit, he‚Äôs saying 15 pebbles can weigh more than 1 rock, with the right pebbles and rock.",5,0.531,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","I see his point a bit better now, it'll be interesting to see where all that market cap accrues over the coming years. I guess my mistake was seeing inference as the more trivial and ""easy to do"" part of the process.",6,0.526,NEGATIVE,0.976
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",,7,,,
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",,7,,,
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Training is training model. Inference is asking your model question and getting an answer back. I think it is true that NVIDIA is not great at inference.One thing to note is that Chamath has invested in a company that has created a chip for inference that is supposed to be fast, oh they claim it to be.",0,0.542,NEGATIVE,0.996
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Just like Chamath. Spews sh*t from his mouth, nothing reaches the brain.",0,0.501,NEGATIVE,0.978
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",Chamath jealous that his SPAC bagholders recovered something with NVDA. ü§£ü§£,0,0.501,NEGATIVE,0.981
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",Let‚Äôs just say he might be right in a different world. But this guy is a clown and full of it.,0,0.514,POSITIVE,0.977
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","Nvidia GPUs are absolutely critical for training large models currently but same cannot be said about inference. So, given the prices, companies do choose to run inference on lower-powered, low total memory cpu/tpu setup.",0,0.54,NEGATIVE,0.552
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!","So tldr so far seems to be that inference is the small (easier) problem that needs to be done thousands of times whereas training is the big (harder) problem that needs to be done once (periodically).So, we train Llama-2 model once but use it a gazillion times. The gazillion uses (=inference) collectively consume more compute than the training phase - even when retrained periodically.Still - I don't see why NVidia should have a persistent weakness in inference. It's not like this is a novel challenge for them - and it's not like they haven't proven they're not capable of developing suitable chip designs for various problem scenarios.",0,0.512,NEGATIVE,1.0
,"""AI is really two markets, training and inference. Inference is going to be 100 times bigger than training. Nvidia is really good at training but very miscast at inference."" - Chamath Palihapitiya üîóhttps://youtu.be/1ZQ33OnGFWE Can anybody elaborate on what Chamath meant by this? Why is Nvidia bad at inference? Is this about the idea that most of the value accrual will be on the AI application layer as opposed to the AI infrastructure layer? But then again - why does he believe NVidia won't succeed with its application layer (e.g., robotics platform, ...)? Hope somebody can shed some light on these questions!",i had listened to the podcast but i cannot understand how Nvidia is not great at inference. i mean the what AI is for‚Ä¶.? just keep learning stuff without making any educated guess?,0,0.53,NEGATIVE,0.998
