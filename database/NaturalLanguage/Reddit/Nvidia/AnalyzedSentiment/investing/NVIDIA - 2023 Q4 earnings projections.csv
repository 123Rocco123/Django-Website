date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","This is the worst analysis I've ever seen. ""M3 has an edge."" It's laughable.Nvidia is sold out, every chip they can make for the entire year. They will report more than $20B, it's guaranteed, not ""a low probability of exceeding."" Why? Just look at what the CFO said during the last call:""Revenue is expected to be $20.00 billion, plus or minus 2%.GAAP and non-GAAP gross margins are expected to be 74.5% and 75.5%, respectively, plus or minus 50 basis points.""$20B is in the bag and they will be on a $100B run rate ($25B/qtr) before the end of the year.",0,0.561,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",Should realize Apple‚Äôs chip strategy is very different to NVIDIA‚Äôs. Apple focuses entirely on their products and not data centers.This interview with Johny Srouji offers a lot of insight into that.https://m.youtube.com/watch?v=GdYa6VpZ27E,1,0.536,POSITIVE,0.95
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",Should realize Apple‚Äôs chip strategy is very different to NVIDIA‚ÄôsYou should direct your comments to the OP. I was responding to his ridiculous notion comparing an AI infrastructure company to a consumer technology company.,2,0.617,NEGATIVE,0.999
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","This interview with Johny Srouji offers a lot of insight into that. https://m.youtube.com/watch?v=GdYa6VpZ27ENot really sure what you think the nuggets to mine here were? Srouji's comments could basically a cut and paste of aspirations from any senior level semiconductor engineer in silicon valley. And yes, a delta is that their devices are 100% consumed internally, but that's not really insight.",2,0.519,NEGATIVE,0.922
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Sorry I guess it‚Äôs not clear that I was agreeing with you ;) I‚Äôm supporting your statement that it‚Äôs laughable OP said M3 has an edge. To Srouji‚Äôs interview, it‚Äôs clear that Apple is not direct a competitor to NVIDIA like AMD, Intel are. No company who‚Äôs buying NVIDIA chips can and will consider using Apple chips for their data centers.The only hypothetical threat is if Apple dominates the entire AI market over Meta, Microsoft, OpenAI and all the million other companies gunning for servers. I think we all know what the likelihood is of that.",3,0.574,NEGATIVE,0.988
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",So are you saying that we need to buy itü§î,1,0.549,NEGATIVE,0.89
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Timing of a buy is up to the individual investor, I continue to hold. ;)This company is finally being recognized as the technology giant they are. They've got many many years of growth ahead.",2,0.594,POSITIVE,0.999
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",People are also starting to recognize that their ceo is elite ,3,0.553,POSITIVE,0.993
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",starting?Some of us have known. :D,4,0.528,POSITIVE,0.95
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Sure, in the same way Intel and Microsoft were bubbles during the rise of the personal computer.Sounds like you should be short since you've got it so sussed out.",1,0.52,NEGATIVE,0.999
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Sure, in the same way Intel and Microsoft were bubbles during the rise of the personal computer.Sounds like you should be short since you've got it so sussed out.",2,0.52,NEGATIVE,0.999
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","not sure what world you're living in, but my peers both use it in business and their companies pay through Microsoft Office365 licenses.My neighbor runs a distribution company that just entered a multi year agreement to use an AI based ERP system.Sound like denial or out of touch, good luck",3,0.598,NEGATIVE,0.969
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","not sure what world you're living in, but my peers both use it in business and their companies pay through Microsoft Office365 licenses.My neighbor runs a distribution company that just entered a multi year agreement to use an AI based ERP system.Sound like denial or out of touch, good luck",4,0.598,NEGATIVE,0.969
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","I simply do not understand the ai minimalists. I‚Äôve had people angrily tell me how this is overhyped despite never seriously dipping their toes. Perhaps it depends on the circles one runs in, but I‚Äôd guess close to 1/2 the people I know are paying for ai, either through subscriptions or hardware.I have modest hobbyist systems that ran me like $5k, and what I have is nothing compared to the workstations I‚Äôve seen. This field is a lot of fun, and I think it‚Äôs going to get increasingly exciting(and expensive) over the coming years",5,0.516,POSITIVE,0.602
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Now, open eyes.From a business perspective, many companies are not willing to use it due to confidential company data. Which is a huge risk.Not sure where the notion that you can only get AI from public access came from but it's completely wrong. SuperMicro or Asus or MSI would be happy to sell anyone servers that will bring it all in house, or hosted by a major provider like Coreweave, and a company like Nvidia or SalesForce or Accenture will help you set it up and run it. I just sighted an example of my neighbors distribution company who is investing to use AI in house.Perhaps AI providers like OpenAI, Google and Meta can cover the massive costs that come with the purchase of the expensive and fast depreciating AI cards. But it doesn't seem to be a profitable business currently.A full 8GPU AI server can be purchased for under $20K.Your lack of privacy and huge cost and risk ideas are just HUGE MISTAKEN ASSUMPTIONS. You're out of touch with reality. The technology world moved on from these notions years ago. Businesses wouldn't be investing if they didn't see a path to improving their bottom lines.",5,0.555,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Now, open eyes.From a business perspective, many companies are not willing to use it due to confidential company data. Which is a huge risk.Not sure where the notion that you can only get AI from public access came from but it's completely wrong. SuperMicro or Asus or MSI would be happy to sell anyone servers that will bring it all in house, or hosted by a major provider like Coreweave, and a company like Nvidia or SalesForce or Accenture will help you set it up and run it. I just sighted an example of my neighbors distribution company who is investing to use AI in house.Perhaps AI providers like OpenAI, Google and Meta can cover the massive costs that come with the purchase of the expensive and fast depreciating AI cards. But it doesn't seem to be a profitable business currently.A full 8GPU AI server can be purchased for under $20K.Your lack of privacy and huge cost and risk ideas are just HUGE MISTAKEN ASSUMPTIONS. You're out of touch with reality. The technology world moved on from these notions years ago. Businesses wouldn't be investing if they didn't see a path to improving their bottom lines.",6,0.555,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","From a business perspective, many companies are not willing to use it due to confidential company data. Which is a huge risk.This point might have been a good one 12+ months ago but since the inception there have been a ton of private data offeringsLocal Models - Mostly Pioneered by MetaOpen Ai business licensesAzure OpenAi business licensesAWS BedrockPerhaps AI providers like OpenAI, Google and Meta can cover the massive costs that come with the purchase of the expensive and fast depreciating AI cards. But it doesn't seem to be a profitable business currently.This is almost always true about new tech though... so I take it you were not an early Amazon, Google, or Netflix investor to name a few...?",6,0.56,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",I am very much into Ai ATM and I am having trouble figuring out who will displace Nvidia if anyone... Not to say it can't happen but its not as simple as a lot of Bears are saying...,3,0.532,NEGATIVE,0.997
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",Exactly right. Nvidia is so far out in front with their platform no one will be able to get close for some time. Its the combination of hardware plus software that forms the platform (and moat) and every day they are adding to it.AMD is arguably the closest in the merchant chip space. They have a chip they're shipping soon but it will end up being a tiny sliver of the market this year.,4,0.565,POSITIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",Exactly right. Nvidia is so far out in front with their platform no one will be able to get close for some timeCorrect. And this will compound over time... much like it did with operating systems... one app gets written then it becomes a whole ecosystem.,5,0.529,NEGATIVE,0.95
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","Replacing them on the training side looks to be extremely difficult. But there‚Äôs going to be a huge business in inference hardware and I can see multiple companies taking that piece of the cake. Shit, I could even see Apple winning market share there ",4,0.541,NEGATIVE,0.534
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",How do you figure?,2,0.515,NEGATIVE,0.795
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",how can i delete someone else's post?,0,0.501,NEGATIVE,0.998
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.","""Nvidia Triples Quarterly Revenue, but Sales in China Are a Concern""https://www.nytimes.com/2023/11/21/business/nvidia-revenue-earnings.html""What percentage of Nvidia sales are to China?Approximately 20% to 25%. But Nvidia now expects a drop in sales to China, which have consistently accounted for approximately 20% to 25% of data center revenue, in the November-January quarter.""Almost a quarter of NVIDIA data center sales are in China! Basically you bet a company heavily relies on China's economy growth!",0,0.613,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",how heavy is your bag op?,0,0.503,NEGATIVE,0.997
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",,0,,,
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",So AMD MI300X is worse than Nvidia H100?,1,0.628,NEGATIVE,1.0
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",Yes,2,0.5,POSITIVE,0.992
,"The new 2023 Q3 figures come from a market analysis firm named Omdia (via Tom's Hardware): Meta and Microsoft, each purchasing 150,000 GPUs. Those two companies were responsible for 300,000 units, with the other 200,000 going to the rest: Oracle, Tencent, Google, and Amazon, Tesla, TickTok, Baidu, Alibaba. Q3 reported revenue was 18 bills. The reported 500k number seems to be true. Now the question is, who will be ordering another 500k batch in Q4? so that at least the Q4 result won't pale in comparison and won't be a disappointment for investors? META to the rescue (Hopefully)! As Zuckerberg stated: ‚ÄúBy the end of this year, we‚Äôre going to have around 350,000 Nvidia H100s. Or around 600,000 H100 equivalents of compute if you include other GPUs.‚Äù , which means they might order another 200k from Nvidia, or he may ""find 2*x H100 equivalent to compute"" to avoid paying a premium over generally specialized Nvidia GPUs and buy more specialized and power-to-output optimized ASICs chips like Google's TPUs. I don't think other big tech (Microsoft, OpenAI, and AWS) giants will order more GPUs because running a general-purpose GPU is very expensive: Running ChatGPT is very expensive for the company. Each query costs roughly 4 cents, according to an analysis from Bernstein analyst Stacy Rasgon. Google already has its own ASICs chips called TPU. Apple also has an edge with its M3 chips family. If their name is not on the Nvidia order line list yet, then they are certainly cooking some big updates to their M chip family. Everyone has shown with their dollar that they aren't willing to pay a high NVIDIA premium in the long run. And want to diversify to other chip makers, like AMD, Intel, and specialized ARM makers. Small businesses experiment and run their AI models using rented Amazon, Microsoft, and Google cloud capacities (mostly Google because of their TPU's low power-to-cost ratio). The rest of the market is peanuts to make any dent. [Conclusions]: Their a moderate-low probability Q4 revenues will reach a plateau of around 15-20 billion. A moderate probability that they will be below 15 billion. A low probability of exceeding 20 billion. Let's sidestep to AI marker TAM topic, Meanwhile, looking forward to 2027, the server market's value is estimated at a staggering $195.6 billion. However, this estimated revenue will be brought by those who rent AI processing capacities like AWS, GCP, and Azure cloud providers. And not ChatGPT clones. The market has not figured yet the TAM (total addressable market) of services like ChatGPT. However, I try to speculate on open numbers, which have stagnated for the last 6 months. 100 million monthly active users * 10$ per subscription produces 1 billion in revenue. The average cost of 1 query from a used is 4 cents. A single user runs 2,000 queries per month on average. The query is a single text message you send to the chatbot. 2000 * 0.04 = 80 $ to service 1 user per month. Until the OpenAI level of efficiency improves they will be losing 70$ to service 1 user per month. No wonder OpenAI has closed its ChatGPT Plus subscription. It doesn't seem sustainable. Last note, the majority of data centers upgrade their equipment every 4-5 years. So, if AI expectations won't burst, and chip efficiency improves 10-fold, and other chip makers won't catch up, don't expect big tech to re-stock their chips earlier than the end of 2027.",hey bozo you were wrong L O L,0,0.501,POSITIVE,0.671
