date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"Custom TPUs have driven past growth, and will accelerate the future as well Interesting and smart on Google. It gives them a pretty big competitive advantage not having to pay the Nvidia tax. https://www.techspot.com/news/103109-google-now-world-third-largest-data-center-processor.html","Google has been making noise about their TPU for literally years now, and they arguably relied on them too much in the past.Every year they’re also announcing they’re expanding their partnership with Nvidia and buying more H100s or whatever.Why? Because it’s not about chip throughout at this scale, it’s about network throughput. They still haven’t cracked that code.Meanwhile this week Microsoft announced they claim they have the worlds fastest AI accelerator which allows AI workloads to effectively load balance across nvidia, amd, and their own chips. They claim copilot and such is already leveraging this.But they’ve also made huge advancements with small language models like Phi3 that can run on Qualcomm ARM processors. All new Surface devices will come with this model running on a dedicated NPU unit on your PC, and it’s basically about as capable as GPT3 and includes multi-modal. Think about that for a moment.",0,0.544,POSITIVE,0.532
,"Custom TPUs have driven past growth, and will accelerate the future as well Interesting and smart on Google. It gives them a pretty big competitive advantage not having to pay the Nvidia tax. https://www.techspot.com/news/103109-google-now-world-third-largest-data-center-processor.html","My understanding is that TPUs are more of a boost to training than inference (or whatever that step is called in the Gen AI world). So I think it's difficult to make a direct comparison between different applications.buying more H100sIt's quite possible that this is for renting to GCP customers. You can't just pick up a piece of code designed for GPU and run it on a TPU. They're a different kind of architecture.network throughput. They still haven’t cracked that code.I'm curious if you have more info about this. I always thought Google was a leader in this area, and that it was one of the core strengths of their datacentre design.",1,0.529,NEGATIVE,0.987
,"Custom TPUs have driven past growth, and will accelerate the future as well Interesting and smart on Google. It gives them a pretty big competitive advantage not having to pay the Nvidia tax. https://www.techspot.com/news/103109-google-now-world-third-largest-data-center-processor.html",TPUs originally were ONLY for inference. That was V1. In V2 Google added support for training.Every version since does both inference and training.But also does create versions that are inference only. The 5E for example was for only inference.So I think it's difficult to make a direct comparison between different applications.Why?network throughput. They still haven’t cracked that code.Google TPUs thrive at moving data and doing it with a lot less power. Not sure what that is referring to.https://arxiv.org/abs/2304.01433,2,0.513,NEGATIVE,0.993
,"Custom TPUs have driven past growth, and will accelerate the future as well Interesting and smart on Google. It gives them a pretty big competitive advantage not having to pay the Nvidia tax. https://www.techspot.com/news/103109-google-now-world-third-largest-data-center-processor.html","Microsoft is working on (or has) custom processors for Azure. Meta and Amazon are also said to have custom processors in the works. Google's TPU is the most mature, but other players are working on similar chips.",0,0.566,NEGATIVE,0.792
,"Custom TPUs have driven past growth, and will accelerate the future as well Interesting and smart on Google. It gives them a pretty big competitive advantage not having to pay the Nvidia tax. https://www.techspot.com/news/103109-google-now-world-third-largest-data-center-processor.html",The head scratcher is why did it take Microsoft so long to get it and finally decide on trying to copy Google?Google started the TPUs over a decade ago. Now has the sixth generation done and working on the seventh.They did not do them in secret and even shared papers on them as they kept improving signficantly with each iteration.,1,0.517,POSITIVE,0.999
