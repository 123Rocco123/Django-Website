date of comment,main comment,comment,depth,PTR Sentiment,Flair Sentiment,Flair Outlook
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","If Tesla has hardware “years ahead” of Nvidia, why has Elon spent $4 Billion on their GPU’s this year? I think that rumor is wrong.",0,0.568,0.94,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Yes it’s doesn’t make any sense, given how iterative any sort of chip making is. I could believe that Tesla has contracted with a chip maker to make a function specific chip that works better for their needs than a general purpose Nvidia GPU, but that’s about the case I can see here.",1,0.524,0.974,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I remember Elon saying he was planning to make a tesla phone as well.,2,0.529,0.99,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Explicitly said the opposite of that the last time I listened to him talking…,3,0.638,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",He said he could but felt it was a waste of his limited time. He is focusing on neurslink as that is what will replace the phone.,3,0.598,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","No, that was never true. Lots of speculation though.",3,0.501,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Musk brand hardware is just an overclocked rtx 4090 with a “xvidia” sticker slapped on top.,1,0.511,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Not saying it cant happen but an engineer leaving Nvidia for Tesla is crazy LOL. Elon is known for overworking and underpaying his employees. Who would sign up for that.,1,0.511,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Some people love that kind of work atmosphere. If you love your work and are crazy driven, you want to be surrounded by others with the same drive. Elon has always searched out these people for his companies, and yeah they often burn out eventually but leave having accomplished and learned a lot from their work environment and coworkers.",2,0.506,0.887,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Considering that Tesla/spacex always rank very high on desired places to work, I’d say a lot of people want to sign up for that.",2,0.502,0.872,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Yes coz they ended up loaded $$$ for their hard work,3,0.503,0.989,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Promotion?,2,0.505,0.98,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",A promotion to lower money is no promotion at all.,3,0.474,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Tesla's effort on their ""dojo"" chip started in 2016/17. There have been numerous public updates on it and Elon claimed at one point it was going to be the biggest, baddest AI system in the world, then pivoted to, well it's just for training self driving, then to well, xAI needs it's own LLM and dojo wasn't designed for that. As far as I know it's basically scuttled/back-burnered.Clepto nailed it.",1,0.529,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I recall a conference call a couple quarters ago where Elon specifically said current Dojo was not close to Nvidia's capabilities yet and they were still buying Nvidia processors.,1,0.566,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Yes yes and we will have self driving cars tomorrow.,2,0.492,0.974,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",sounds like the guy's dad just heard rumors from Elon fan boys worshiping their idol spreading nonsense.,1,0.5,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Years ahead of other auto makers.,1,0.539,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Tesla isn’t “years ahead” of anybody, in any hardware. Let alone GPUs. Anyone who thinks Tesla is anything other than a falling star is delusional. Nvidia will be around for a very long time, quite likely 50 years or more by my estimation. Tesla will be a forgotten relic by that time. That being said, I completely agree that Nvidia stumbled into their success.",1,0.546,0.877,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","If NVIDIA is the main topic of the Thanksgiving diner, then I guess it is time to sell.",0,0.542,0.987,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I hear ya, if it's any consolation, stocks are the main thing we talk about year round. Mostly me asking him to look at a stock and him telling me it's too crap.",1,0.583,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",What did your dad have to say about NVDA?,2,0.493,0.93,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",He said it's crap and then he stormed out of the room. Everyone was crying. I was there,3,0.629,0.869,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I don't want to talk about it.,4,0.502,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",There was 2 years of AI hype going on so AI stocks went into overdrive and did not go up organically (slowly up overtime). Nvidia was one of those stocks. Still a good long term company though. They will survive and grow under the shadows (not noticeable by the casual investor; wouldn't know what hit them missing the boat).,4,0.573,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Casual investor is into index funds, and index funds own a ton of NVIDIA, so casuals won’t have missed much.",5,0.52,0.968,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I sucked his dad in the other room.,3,0.5,0.558,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",for NVDA shares ?,4,0.508,0.996,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Man that sounds like the epitome of a generational wealth family if I've ever heard of one.,2,0.508,0.649,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","My parents are an accountant and a lawyer. Dinner table conversation was either was tax law/company shit my dad was looking at or what case my mom was looking at, mixed with being a bunch of clever dicks (who do like each other). It was also all the normal what you did that day stuff, but being exposed to economic and legal concepts at a young age definitely helped me understand wealth. I’m an ML engineer, so a completely different path but an equally prestigious one.TL;DR you’re right, and those type of convos directly lead to better soft skills and being more hirable, it’s unfair.",3,0.523,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","TL;DR you’re right, and those type of convos directly lead to better soft skills and being more hirable, it’s unfair.Economic and legal concepts are separately useful information about how the world works and how to get ahead.Once you're talking stock picks at family dinner table, you're likely in a position where you have enough money to make you more money that is meaningful, which means you no longer need to be a participant in the economy/innovation/understanding except observing and lending your money so that it can grow. Hence my point is that this sounds like a generational wealth conversation.As someone who got lucky in life as a kid of an immigrant, I'm usually the one who has to teach American ""Wealth Accumulation Economics"" to my parents. But we don't talk about it over dinner table.",4,0.521,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I'm also an immigrant, from eastern Europe, we moved to the US in 1994. No generational wealth, my dad just got really into stock picking in the late 90s, it's been a hobby of his ever since so it's what he likes to talk about, that and cars, he worked as a mechanic for a short while and car salesman for a lot of his life.But for whatever reason money was always a topic of conversation around our table, what we could/couldn't afford, what we needed, what we could do without. When I was a kid it was about food and rent, then later about home improvement, renovation, college. So they definitely got wealthier.I consider myself rich because there's little that I want that I can't afford but I also don't want much, but people in my peer group don't think of themselves as rich. Rich is something more than whatever I have, for most people. I remember some interview with Rickie Gervais and somebody asked him about the millions he's made off of the office and his immediate reaction is something to the effect of ""sure I have a lot of cash, but that's I'm not rich, look at these guys with $100M yachts, that's rich"".",5,0.528,0.912,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Any growth stocks that interest your dad?,6,0.59,0.995,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Are you beating the SP500 with your stock picking?  I’d tell my son to buy an index if I had one.Most of the gains in the SP500 come from a few companies that no one can pick.  Stocks that overperform (Nvidia is the poster child for this) usually underperform in subsequent years.,2,0.552,0.996,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",IBM was a big topic of conversation in the very early 60s. By 1980 you WISH you had bought IBM in the early 60s.,1,0.512,0.986,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",When I graduated high school in the 1980s my parents worked at IBM and we talked about it. They also gave me 10 shares when I graduated high school. Brother and sister bough a car.its now over 500 shares and pays dividends 10x the original purchase price.Nothing beats generational compounding.,2,0.518,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I think it’s an amazing company but at $3.3T market cap, I wonder if it’s worth the risk.For example, let’s assume amazing growth and a $10T market cap by 2030. That’s 3.3x return vs S&P which will likely get 2-2.5x return over the same time period. Yes that’s a better return but it’s also a lot of concentration risk (what happens if Jensen steps down, earthquake happens at manufacturing plant, etc). Personally I see other AI plays and individual stock opportunities that are more worth my risk and I still get decent NVDA exposure in S&P.Personally I worry also that the extreme growth that wall streets expect is now well baked into the stock price divided",1,0.561,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","This is where I’m at: it’s a great company, excellent chance to still outperform popular indexes, but: its ship for true ‘moonshot’ gains has almost certainly sailed.",2,0.549,0.961,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Unfortunately, that's what I told myself three years ago when I decided to throw a little money into a handful of individual stocks.I mean, it's probably correct, and I'm ahead overall. But still. Le sigh.",3,0.54,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Nvidia has years of growth ahead with little of significance in the way of competition. For example, AMD, their largest traditional competitor, may end the year with 4% data center accelerator market share.Selling now would be idiotic.",1,0.604,0.993,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",What if there’s an AI bubble crash since AI products cost a ton and don’t actually seem to do anything?Not saying Nvidia is going away but growth will likely slow down or reverse when the AI bubble pops.,2,0.563,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","If NVIDIA is the main topic of the Thanksgiving diner, then I guess it is time to sell.They were all shining each other's shoes during the Thanksgiving discussion.",1,0.529,0.995,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Man thats how i felt about bitcoin when it was 3k then hit 20k and everyone talking about it. thought for sure that was the top. Well… looks like the tulip story doesn’t really apply the same these days,1,0.506,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",the tulip story doesn’t really apply the same these daysmaybe just a longer timeline,2,0.502,0.971,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","in summary sure, NVDA has super specialized chips that were first to market when users (including competitors) needed to train AI models nowadays, and we have a gold rush while they sell the equipment to mine goldbut, in fact, this IS exactly what they've been developing the computers foryou said stumbled into, but that's far from fact, and i'd say fictionwhat's closer to truth is how Jensen was saying a long, long time ago that their way will change computing forever""it won't be overnight, but give me a couple of decades and I'll change the world""here's how they described it; gaming was just the easiest use-case to make money and show how their way of computing was better for 99% of purposes, or whatever the reasoning.the power consumption required, goes down as the chips get better, so that's also part of ithe's talked about reducing the cost of computing, and THAT is what is allowing breakthroughs, and it's not CPU but as you did describe it, the complex GPU style 'algorithmic/networks of data/all at once' style of thing that fits perfectly with AIso again they have been developing the shovels for the gold rush they've been describing for decadesI wish I was listening and believed it if I heard it back then",0,0.532,0.568,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","It needed that killer app. If AI didn’t pan out as it is current day, there would be no shovel for the gold rush.Right around covid, there was a mad rush for self driving cars…every car manufacturer had to say they have self driving or plans lots of self driving development…that could have been the GPU story, but it flamed out. Luckily we had ChatGPT to showcase the way.",1,0.515,0.948,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","In my opinion, AI is already the second gold rush for NVIDIA. Crypto mining was the first. It also happens mostly on NVIDIA GPUs.I didn't realize it at that time, only when it happened the second time I understood that they were excellently positioned.",2,0.554,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I don't think that's right, or at least wasn't always right. The best hash rate to cost back in the day were AMD gpus, because although they were slower or less performant, the upfront cost was so much lower that older cards like rx5 and 4 series doubled or tripled in price in a very short timespan.Although NVDA cards went up due to mining, the larger gap up was for AMD cards actually because hashing doesn't care about raw power in a card, it was more about hash/$ rate as well as hash/unit electricity cost",3,0.53,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Crypto mining was its sudden rise and fall. GPU demand was through the roof, but dropped off when China banned it and crypto prices fell. Graphics cards went from low stock high prices to glut. Then OpenAi came and saved the day.",3,0.517,0.7,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",That first sentence is why I didn't buy years ago.  OpenAI wasn't known to me.  I remember researchers showcasing 4x Titan GPUs for ML and thinking that was kind of cool.,2,0.521,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Yes, they were always interested in general purpose high-performance computing and cared about supercomputer applications and high-bandwidth system to system communication (the really hard part) that goes well beyond gaming use cases.Neural machine learning was one example of HPC back then and has now driven most of the revenue. There's one special purpose aspect---the desire for very fast but low precision computing (fp16, fp8, int8) in large scale is peculiar to the neural machine learning case, and not so much traditional scientific simulation type HPC computing.There was no accident or stumbled into, they had the foresight to develop the software and hardware infrastructure to do both graphics (which paid the bills) and HPC (harder case which was the future).",1,0.517,0.957,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",good write up; this is why the goverment suing NVDA for being a monopology simply because other companies rely on their ecosystem is so frustrating -- they been at this for decades investing into the development even at the risk of bankrupt and now they are just reaping the benefit.,1,0.517,0.787,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","agreedit's an insane story of success, not ""killing competition"" or anythingyou have Google and others building their own chips WHILE STILL buying BILLIONS of dollars of NVDA chipscrazy how many of the top tech companies all have billions in orders for H100 and now Blackwell",2,0.521,0.968,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Well you're righr and wrong.Nvidia did invest in building the most powerful part of the computer. Which was a smart move.But it is also a coincidence these were used for crypto and now most importantly AI. CPUs do outperform GPUs for servers, and computing centers were pretty much just for servers until crypto and AI.We could've also found a more modern and efficient computing technology for AI by then as well.Nvidia is a bit like tube radio manufacturer that boomed when computers emerged (before silicon)",1,0.552,0.679,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I have been working in the ML space for close to a decade now.NVIDIA did position themselves well with CUDA but they also lucked out. CUDA essentially made it easy for tools like TensorFlow and PyTorch (the tools most commonly used for building and training models) to build on top of it. AMD just didn’t have the same level of flexibility and the ML libraries didn’t support them.Back when v100s (enterprise grade NVIDIA GPUs) were new (2017) I helped buy a dozen or so. There wasn’t really an AMD enterprise equivalent that we could have bought if we wanted to.AMD is just starting to catch up in terms of enterprise offerings. They finally have some enterprise grade GPUs. I now work for a compute platform company and we officially started supporting AMD GPUs this year. So far I am not aware of a single customer actually using them though but people are talking about them.The other thing that helped things to blow up was LLMs, or more specifically transformers. Prior to transformers, we generally saw diminishing returns in making models larger once we reached a certain point. Before transformers a 10 million parameter model would have been considered quite large. The biggest baddest Llama model is 405 billion. It takes 32 top of the line h100 GPUs to run a single instance of the model in full precision. The number of training hours completely dwarfs that number though. It requires thousands of GPUs to train.Right now NVIDIA is the only game in town. It may change soon but people choosing non NVIDIA GPUs are actively choosing something less powerful mostly for cost or availability reasons.",0,0.543,0.861,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I agree with you. I think CUDA is the main reason why Nvidia is the dominant player in the AI hardware space. The entire AI ecosystem is built around CUDA. Every library related to AI (TensorFlow, PyTorch, etc) has support for, will run faster, and more efficiently on an Nvidia card because of CUDA.",1,0.545,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","AMD is deliberately shooting themselves in the foot. They refuse to adopt a cuda compatible framework, they neglected HIP and shit on ZLUDA. And keep trying to push their own ROCM that nobody wants. And even then they only support rocm for a scant few of their top end consumer GPUs, meanwhile cuda works on basically every Nvidia GPU ever without issues.If they just swallowed the bitter pill, adopted nvidia cuda APIs and then made cuda work on all their cards, they could add 25% more VRAM and sell them 25% cheaper than nvidia and they are guaranteed to print. Customers have no brand loyalty, so considering how horribly nvidia is fleecing the market right now with its crap VRAM offerings I'm sure there's plenty of headroom for amd to make a killing",1,0.54,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","If they just swallowed the bitter pill, adopted nvidia cuda APIs and then made cuda work on all their cardsYou're forgetting the part where NVDA is going after CUDA compatibility layers so AMD is trying to make an open standard. ROCm is open source.",2,0.533,0.869,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","> NVIDIA did position themselves well with CUDA but they also lucked out. CUDA essentially made it easy for tools like TensorFlow and PyTorch (the tools most commonly used for building and training models) to build on top of it.NVidia also paid numerous top-quality engineers to integrate this well. That was not luck, it was foresight and intentional investment that Nvidia earning from now. They designed CUDA as it developed over time to the needs of ML and Pytorch and Pytorch designed around the capabilities of CUDA. They had previously positioned CUDA for traditional scientific HPC computing (intentionally) and neural ML was a natural fit.Other than that I agree 100% with you.",1,0.533,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","They did a lot of things right to get here, but there is a lot of things that had to happen right that they weren’t really involved in. On top of that it is pretty incredibly lucky that no one else was right there with them to take advantage at the same time.If someone at Google didn’t come up with transformers, and other people start to figure it that just keeps getting better the bigger we make it, and open AI didn’t get a huge amount if attention with ChatGPT 2/3, and every CEO on the planet didn’t buy into the idea that their business needs to be doing ai, and everyone else in the same market didn’t totally bungle things, then Meta doesn’t commit to buying 200k GPUs.The way I look at it, they put the best receiver on the planet on the field and he caught the ball being thrown by a blind quarterback with no one ready to take him down. They did a hell if a lot right, but they could have just as easily been just another gaming hardware company if things didn’t go just right.",2,0.534,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Huh so maybe advanced money destroyer can make a comeback,1,0.521,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Talking my book here but...$NVDA has been leveraging its GPUs for general purpose computation for around 15 years or so. They possess many tools for effectively deploying, maintaining and fully utilizing their hardware. Some parts of what NVDA provides can be copied rather easily(inference or running a bunch of matrix operations) and some can't(the investments in high-level libraries sitting on CUDA, debugging tools, tools to share GPUs between different workflows, tools to tune performance, etc).It's possible that if deep learning settles on a particular architecture that someone could make an effective competitor to NVDA and steal some business away.But don't forget, NVDA is also evolving and they're moving just as fast, if not faster, than their competition. NVDA isn't stuck if CUDA proves to be a limitation or is superseded by newer programming models.NVDA also has the ability to pivot its business if needed. They could flip a switch and become a massive cloud provider if they really needed to.Is AMD HW better on some levels? Hell yes. It's more efficient and has more raw power. But NVDA is clever and they've erased AMDs advantages several times now by being smart(i.e. adding dedicated tensor hw). They also have a broad view across the stack and quickly address issues faced at many levels whereas AMD and others just don't have the SW resources.If I was going to pick one serious competitor to NVDA it might actually be INTC. As a SWE I know that INTC actually has some decent SW resources and has a history of delivering effective solutions for leveraging their unique HW.",0,0.527,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Heard it here first folks AI kills 100,000 turkeys this Thanksgiving.. it truely is evil and must be stopped",0,0.53,0.889,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","It may have saved 100,000 turkeys diverting that energy to GPU’s instead of fryers 😂😂",1,0.512,0.867,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",If pops isn't aware the whole recent AI boom came from Sutskever & Hinton's AlexNet winning the ImageNet Large Scale Visual Recognition Challenge with its GPU powered deep learning network in 2012.https://en.wikipedia.org/wiki/AlexNetLots of podcasts about it. This one is good:https://www.economist.com/science-and-technology/2024/04/03/the-science-that-built-the-ai-revolution,0,0.53,0.884,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","That, and Natural Language Processing (Almost) From Scratch by Collobert et al. Both similar time periods, and both paved the way for modern AI.",1,0.531,0.998,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Saying Nvidia ""stumbled"" into an AI use case is extremely demeaning of Nvidia's foresight on this IMHO. They've been utilizing their GPUs for AI stuff since 2018 in a consumer product.It's also not the ""exact"" same product. They have specialized cores on the GPUs and their AI cards are just 100% of those cores instead of a limited set like their GPUs.But yes, the reason Nvidia is so dominant is because they've been working on AI inference hardware for a decade+, and most people aren't nearly that far along.",0,0.546,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Back then CUDA was absolute garbage. Trying tonwork with that was insane...,1,0.502,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",2018 is very recentpeople have been generating theories and algorithms since the birth of digital computingjust so happens that the mathematics of processing those algorithms happen to run much faster on the same kind of hardware that also happen to specialize in graphics processingat least for now,1,0.518,0.835,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Sure, but it's not the exact same hardware anymore. Tensor cores are their inference cores that don't do graphics processing.Someone will compete with Nvidia, but right now they are very far ahead of everyone. So someone needs to majorly innovate or Nvidia needs to take their foot off the gas for there to be real competition.EDIT: 2018 was also the release of their mass available consumer product. They were developing their AI inference shit since probably 2014 or earlier.",2,0.534,0.526,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Literally could’ve just said CUDA,0,0.69,0.919,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Jensen Huang has been developing gpus with ai in mind for years. They didn’t just stumble on to it. AI will be used to increase productivity. Much of computer programming is now done with AI. We’ll need fewer lawyers, accountants, engineers, burger flippers, truck drivers, pilots, etc. in the future. There is tremendous demand and Nvidia is first mover status on replacing an estimated $1 trillion of data center infrastructure. Their chips drastically reduces the total cost of compute in addition to being a full stack provider for many industries.",0,0.54,0.997,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I would argue that they did stumble into it, about 12 years ago, they realized it around 8-10 years ago and started actively developing towards it 7-8 years ago with Volta.",1,0.533,0.969,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",What about the companies Nvda has purchased and added to their portfolio?,0,0.535,0.805,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I have worked with Nvidia since 2022, and you are exactly correct on why they are generationally ahead of AMD, Intel, Tesla and everyone else. For a while I thought Intel’s Gaudi was going to catch them but it just doesn’t have the power and I’m not sure it will. Maybe for smaller applications but no where near Blackwell.",0,0.536,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Why put a tl;dr if it does not relate to the post at all? I read the post.,0,0.503,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","AlexNet changed the world and I encourage you to read about it. To my knowledge it was one of, if not the first time a deep neural network was trained with a GPU",0,0.533,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I don't think you're correct that Nvidia's ML GPUs are the best just because they ""stumbled into the AI use case"". This was the case in 2012 when AlexNet, the first famous deep neural network, crushed the Imagenet competition. But that was the start of the deep learning revolution and from that point on ML GPUs were developed as a separate product.The reason Nvidia stock has skyrocketed recently is because LLMs like ChatGPT, Claude, LLama, etc need far more advanced GPUs than older models like CNNs and transformers. A BERT model might have 100m parameters while an LLM can easily have 100B (some even have 500b or 1t for trained models which are then distilled post-training). In fact, Nvidia was losing ground on the older model space because special-purpose devices like Google's TPUs and AWS Inferentia do the same thing faster and cheaper. But LLMs require the most powerful and most complicated GPU archiectures (and the software stack to power them) and Nvidia is miles ahead of its competitors in that area. Nvidia's fate is now tied to LLMs. If some breakthrough comes along that lets you run an LLM in <1b parameters, they are cooked, but if 1t sized models show even more capability, their hardware becomes even more critical.",0,0.522,0.951,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",A million Tesla owners who paid for full self driving 10 years ago at 7500 a pop would disagree,0,0.552,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","For anyone following and interested in this thread, I can recommend the acquired podcast from October of last year (2023) with Nvidia CEO Jensen Huang. He specifically mentions their cuda development as the next phase.I bought my 50 NVDA shares in 2019 for ~$143 or so. I also sold all 50 in 2019 for a small profit. It split 4-1 in 2021 (and then 10-1 recently) and that’s why you should never, EVER sell stock that you own in a good company.I don’t want to do that math. Please no one do that math for me.",0,0.576,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","This is a good post however, I feel statement requires clarification:""they've been developing the product for decades for another purpose and stumbled into the AI use case.""I offer the following from Nvidia's 10Ks - PART I, ITEM 1. BUSINESS, Our Company10K - For the fiscal year ended January 31, 2016""NVIDIA is the world leader in visual computing. It enables us to open up new avenues of exploration, facilitate creativity and discovery, and power breakthroughs in new areas like artificial intelligence, virtual reality and autonomous cars.""10K - For the fiscal year ended January 29, 2017""Our Company Starting with a focus on PC graphics, NVIDIA invented the GPU to solve some of the most complex problems in computer science. We have extended our emphasis in recent years to the revolutionary field of artificial intelligence, or AI. The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films. Today, it also simulates human intelligence, enabling a deeper understanding of the physical world. Its parallel processing capabilities, supported by up to thousands of computing cores, are essential to running deep learning algorithms. This form of AI, in which software writes itself, enables computers to learn from data and serve as the brain of computers, robots and self-driving cars that can perceive and understand the world. GPU-powered deep learning is being rapidly adopted by thousands of enterprises to deliver services and features that would have been impossible with traditional coding.""You can see a definitive shift in tone with respect to AI from 2016 to 2017. I would wager that there was more discussion and discovery at the developer and researcher level years prior. The 10K officially documents the company's AI focus to the investment community.With all due respect, AI is very intentional and not a stumble. I would not refute a similar claim about Bitcoin mining. That was stumble, trip and fall IMHO.",0,0.549,0.994,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Yeah I think I should have been more clear that I think they stumbled into it around 12 years ago and actively developing towards it 8 years ago (and have done a great job!) and were able to do that because they had 20+ years of graphics hardware development, even then. They didn't stumbled into it last year.The only new thing that happened last year was ChatGPT capturing the imaginations of the general public.",1,0.57,0.999,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I think OP means they weren't planning on building a multi-trillion dollar ML accelerator business back in the 90s or early 2000s when they were laying the architectural foundations, and while they've always been a well run company, they got very lucky.",1,0.549,0.997,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Time to sell NVDA then,0,0.542,0.961,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I'll trade you handmade genAI art for your shares.,1,0.511,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Me too thought this was common knowledge,0,0.502,0.97,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",TLDR; buy pure storage which is replacing all HDDs and savings space/energy in data centers,0,0.537,0.99,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I agree but I also think we will start seeing specialized ASICS for specific AI/ML workloads replace general purpose gpus much like we did with crypto mining. We are also seeing companies add accelerators for some ai workloads on cpus like intel’s newer Xeon chips. It might not be necessary to buy nvidia hardware for some workloads.That’s the risk on an nvidia ai play. I’d put money on specialized LLM tuned stuff in the next two years. That said I’m still holding some nvidia shares because they have other uses.,0,0.556,0.996,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Ugh, we all knew this 5 years ago, in regards to the use of GPUs... nothing new. It's priced in at this point. I'm only interested in the stock because of the latest pull back.",0,0.587,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Dad is wrong. In the Podcast Acquired they talk about how Tesla relies on NVDA chips for their AI algorithms, even though Tesla has been thresting to develop their own for years now.",0,0.524,0.994,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",I feel like this elaborate story has been made up just to start another bullshit rumour to prop the tesla price up.,0,0.509,0.999,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",It's just a bullshit reddit story. Happens every year.  Last time there were a bunch of posts about people getting screamed at for having crypto,1,0.558,1.0,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Two people who know little about nvidia lol,0,0.547,0.75,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","It's not just the NVDIA GPU. It is the eco system. There is no alternative to CUDA.Pretty much like how Netflix has its own huge content. That Apple, HuLu etc can not match up.",0,0.511,0.584,POSITIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",How could you possibly think that which algorithms are used in AI and GPUs are ‘common knowledge’?90% people don’t even know the difference between a GPU and a CPU,0,0.524,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","TPU is not about being faster, it's about cutting costs and making big interconnected clusters. If you want fast (tokens per second) hardware, there are Groq and Cerebras chips. CUDA, while having good support, would eventually die, everybody making good XLA compilers these days.",0,0.523,0.99,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","Tesla really isn't even in the same stratosphere as NVIDIA as a technology company. Tesla is an integrator, for the most part; they integrate other people's silicon, drives, batteries, etc. into a package that just happens to be a car (and from what I saw in a teardown of a Tesla motor driver, they're not that great at that). NVIDIA, on the other hand, creates parts more advanced than any of their competitors and the ecosystem to go along with it, and it seems like they have a plan to maintain their pipeline.",0,0.539,0.99,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Dad say Nvidia make laws for Turkey Day!,0,0.531,0.898,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)","I just bought a computer with a Nvidia GPU, I think ill buy some more of their stock.",0,0.607,0.998,NEGATIVE
,"My dad is an active investor, has been for decades and we talk about stocks and companies around the holidays. Yesterday we were talking about Nvidia and in our conversation there was something about AI and Nvidia and that he didn't know that I thought was common knowledge, but I'm curious how common it actually is or if it's a generational gap. The knowledge that I thought was common was that the biggest reason Nvidia is well positioned to benefit from spending on AI is that graphics algorithms (e.g. video games, CGI rendering, etc.) and machine learning algorithms rely on the same kind of computation, large matrix multiplication. They have spent decades developing GPUs for graphics processing + other stuff, but through no effort on their own, the ""other stuff"" is now more valuable than the graphics processing. Their ""AI hardware"" is really just the most recent version of their GPUs that they sold to consumers, to Pixar, etc. not similar, they are *literally* the same thing. They didn't somehow develop AI hardware in the past 2 years that's better than everybody else's, they've been developing the product for decades for another purpose and stumbled into the AI use case. This is also why it's so hard for a competitor to overtake them, they have a lot of ground to catch up on. Hardware development is really slow, really expensive and really difficult. This came up in the context of my dad telling me some rumor he had heard about an ex-Nvidia current-Tesla engineer saying something about how Tesla had hardware that was years ahead of Nvidia, which is not really even remotely likely IMHO. Google has spent almost 10 years developing its TPUs for machine learning specifically and even then they're only faster in some narrow scenarios and is way more restrictive computationally. As to why Nvidia and not AMD, it's largely because of their software stack and community support. They have put a lot of effort into developing CUDA, cuDNN, etc. and putting it in the hands of researchers starting 10+ years ago, so now it's what everybody builds on. Before all the arm-chair experts come out of the woodwork, yes I know that GPUs are general purpose parallel processing units, but the point is that they have long been designed with graphics in mind first and foremost. And I know that they've been tailoring recent generations of GPUs more towards ML use-cases (e.g. tensor-cores) and they're not exactly like GPUs of old. And yes I know that their data-center business has long been a big portion of their revenue and they make more than just consumer graphics cards. I don't mean to discount their good work, they've done a great job in positioning themselves in the best way possible, there is a lot to be said about being able to capitalize on good luck. tl,dr; the energy it takes to train ChatGPT could fry 100,000 turkeys this year (I didn't check the math)",Don’t underestimate Tesla,0,0.5,0.998,POSITIVE
