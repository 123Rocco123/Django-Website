date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","That's the problem—they got it completely backward. The technology should be just a means to solve problems. They should solve specific problems, not the other way around, trying to shoehorn AI into everything just because it's a new technology.The implementation of AI should come from the bottom up, from small startups that have found ways to SOLVE PROBLEMS using AI. The problem should come first, followed by the customer experience and then the solution.There is no way (I mean, in theory, there is) that big, hierarchical companies could generate a solution and useful implementation of AI.",0,0.552,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I feel that they did shoehorn WIFI into everything. I.e. making everything “smart.” Maybe that’s the model they’re going with, because that has been very successful.",1,0.511,POSITIVE,0.868
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","has it, though, i can think of a few instances i prefer the ""dumb"" versions",2,0.503,NEGATIVE,0.989
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",My buddies nest goes into energy save move when they leave even though they disabled it.,3,0.507,POSITIVE,0.996
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",My buddies nest goes into energy save move when they leave even though they disabled it.,4,0.507,POSITIVE,0.996
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","So relatable. Right now I couldn't adjust my room temp bc my ""smart"" thermostat couldn't connect to the wifi, and the ""dumb"" remote doesn't have the settings I need.",4,0.501,NEGATIVE,0.567
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Yes. Just because you aren't handing over your dollars doesn't mean other idiots aren't racing to give them away.I think the comment above yours was mostly implying that ""smart"" devices have been profitable more so than actually useful.My girlfriend has connected everything, though the most useful things are onlyturning off the lights with alexaability to unlock the door remotelysee text messages from her watch.All of the tools to do that probably cost $1500. To me, that's totally not worth it, but...",3,0.51,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I'm also thinking of the instances in which more advanced technology does not equate to improved end user experience. in that sense it's not consistently providing actual value on the ground.for example, my father in law just never adjusts his car's AC because it means having to tap through a dashboard ipad menu tree vs tactile knobs. or an appliance that loses basic functionality because bluetooth doesn't work or something like that.but yes, profitable on whole as we average out en masse. just taking opportunity to also complain about the practical effects of shoe-horning new tech.",4,0.537,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I laugh every time I see a $3-4k refrigerator with a wifi touchscreen on it.  ,2,0.512,POSITIVE,0.995
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","""Honey, did you forget to get the milk?""""I got it, dear. It's in the fridge.""""I'm looking at the screen right now. It says we're out.""""Uh... did you open the door and look on the shelf?""",3,0.537,NEGATIVE,0.847
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","My electric toothbrush has Bluetooth! 🤷‍♀️ I mean, it’s stupid and unnecessary, but there it is!",3,0.502,POSITIVE,0.971
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Bought a house when the appliance supply chain was at its worst. Our builder had access to those wifi touchscreen fridges at a discount that brought them in line with the ""dumb"" fridges, and they could get it in a reasonable time so we said ""what the hell, why not?"". The fridge itself works fine... but the smart features are virtually useless. Can't even get the damn thing to connect to the Internet reliably enough for the weather app to work.",3,0.533,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Dude my ""smart"" oven is the crappiest appliance I've ever owned. Give me a 1960s Oven any day.",2,0.509,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Personally I'm glad for WiFi ""everywhere."" Otherwise we wouldn't have r/internetofshit to laugh at.",2,0.545,POSITIVE,0.953
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",You sound like a guy who isn't enjoying reddit from his fridge at this very second ,2,0.51,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","There is no way for small startups to train AI models, unless we go back to the days of VC’s throwing huge gobs of money away.",1,0.523,NEGATIVE,0.997
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","For some sorts of things--like training image recognition models--you can get to a 'minimum viable product' by using an existing toolkit. For something that has a pretty narrow and specific use case (like training an image recognizer on components passing through an assembly line), that's the only way you can do things: you're not using a general image recognizer trained on animals and buildings and trees and people to recognize misaligned widgets on a white assembly line background.",2,0.512,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","""What we don't quite understand is AI, what we do understand is just algorithms.""Nobody thinks of OCR or very specific classifiers as AI anymore in common parlance. Just like in the 2000s people stopped calling A* pathfinding AI.",3,0.537,NEGATIVE,0.997
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Well, yeah; for more than six decades now there has always been ""those things that only humans can do"", ""those things we can get a computer to do"" and ""those things we thought only a human can do, but how the computer can do it""--and that third category has always been called ""Artificial Intelligence.""Recall that decades ago accounting was something thought so hard to do only humans can do it. Then accounting software came out--and suddenly ""oh, well, I guess keeping a set of books isn't that hard.""Or recall that early works in ""AI"" more or less boiled down to clever parsing techniques or recursive rule parsing. Stuff that we now consider relatively standard ""CS 101"" classroom stuff.Image classification (and not ""optical character recognition"" which often uses different techniques than image classification) does use the same techniques used by modern LLMs--and now that we've had them in industrial use for about a decade, we no longer think of using an image classifier and machine learning on a food conveyer belt to kick out bad oranges as ""artificial intelligence.""Because it's now ""those things we can get a computer to do.""",4,0.516,NEGATIVE,0.987
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","What you're describing is a fallacy that has propagated rapidly in the last 5 years in regard to the usage of ""AI"". The term's common meaning used to have at least some reference to a cognitive science sort of reasoning, about a machine that could think in a self-referential way - something like ""consciousness"". I don't know when, but in the last few years the term AI has come to mean absolutely nothing about Artificial Intelligence, and has strictly come to mean a model that is more competent than humans at something, and ""General AI"" has come to mean a model that is more competent than humans at most things. That's it.I think you're astute to observe this has all happened before. And Generative AI is no different - it's just becoming ""thing computer do"". The whole ""AI"" thing is just how business types sell products that do nothing useful.",5,0.538,NEGATIVE,0.986
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","What you're describing is a fallacy that has propagated rapidly in the last 5 years in regard to the usage of ""AI"".Blame John McCarthy who coined the term in 1956.I think you're astute to observe this has all happened before.Well, it helps that I was there.Or, more specifically, my interest in ""Artificial Intelligence"" came about in the early 1980's when I was in college and read the works of John McCarthy, Marvin Minsky and Claude Shannon. The second programming language I ever learned (after BASIC on a TRS-80) was LISP, and my high school science fair project was building a primitive theorem prover which could solve basic problems in propositional calculus. Which, at the time, was considered ""Artificial Intelligence"", but now is just a fascinating parlor trick done by nerds with too much time on our hands.(Edit to add: for those interested, the works I used to help me with my science fair projects were Patric Winston's Artificial Intelligence (first edition) and J.A. Robinson's Logic: Form and Function.)",6,0.521,NEGATIVE,0.53
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Does this conclusion not seem to be at odds with history?Nasa has almost 18k employees, a 25 billion dollar budget, plus - it's literally the government. Not a small start up by any means. But at the same time they have:Invented the tech that all modern phone cameras use. Which in turn makes modern social media possible.Developed the food safety standards that the nation now used, and which has been credited with doing things like removing glass shards from our food.Was critical in the construction of the gps system, which has obviously transformed the world in countless ways.This list could keep going... for a while.I'm not saying a small start up can't lead the way on AI implementation. There's plenty of examples of start ups changing the world too and solving problems. But it doesn't seem. To me. To be a start up exclusive businesses. Hierarchy and red tape isn't lacking in the wins department on solving problems with the creation and implementation of new technology.",1,0.528,NEGATIVE,0.747
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","NASA came up with all that technology as a means to solve problems they actually had... Some things did turn out to have useful applications elsewhere as well, but the current AI craze seems a lot like saying, ""Wow, the Saturn V is awesome! Let's find a way to integrate it into our product too!""",2,0.53,POSITIVE,0.978
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I wish my cell phone had a Saturn V. Then it could take me to the moon while I browse Reddit.,3,0.513,NEGATIVE,0.975
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It would likely require a new model architecture of some kind that has key features no one has even thought of yet - some kind of genius breakthrough. Training foundational models as a small startup right now is fundamentally too expensive, and this is very unlikely to change given the nature of those models. And sure, there are some like 6B models that ""pErFOrM aS wELL as lArgER moDElS"", but those larger models still can't count, still hallucinate - the comparison isn't actually favorable.",2,0.517,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","That's the problem—they got it completely backward. The technology should be just a means to solve problems.+1AI is a solution looking for problems to solve.  Tesla's full self driving(FSD) is the best example of machine learning/AI not meeting expectations.Musk has talked about FSD since 2009 and just last month they released FSD beta (FSD Supervised mode) after 15 years of working on the problem. They haven't even gotten good at the medium-difficult problem of driving relatively smoothly in city streets. It'll probably be another 15 years maybe longer before it reaches level 4 or level 5 self driving which is where the really hard problems are. Selling an AI story is easy, executing that vision is hard (eg. FSD will drive from LA to NY by itself. 15 years later Tesla vehicle still can't drive reliably on the highway).  Every other problem I've seen trying to be solved with ML/AI is difficult. Yet every single tech company mentions AI in their quarterly earnings.Too much AI, too little results. I don't even think this is an unpopular opinion; just talk to people working with machine learning/ML and ask them how complicated/costly/difficult it is to do something useful.  As usual, people's excitement for AI, has extrapolated the idea of AI much, much further than the reality of where AI is today and this is how bubbles form. Also, these speculators (they are not investors) do not understand AI will take decades to mature (eg FSD). The speculator's behavior today is very similar to the dot com bubble speculation except the internet was much less complicated than AI and the internet had returns that could be realized much sooner and were more concrete than AI.  AI is a bubble that will absolutely burst. I just don't know when.",1,0.537,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","AI already solves specific problems. No different than Internet, Electricity which were all general solutions but were solving specific problems to start with.Just because you and OP lack the vision to see AI/LLMs 5 years down the line doesn't mean other visionaries won't.You can't have Millions of DAU without solving specific problems. Enterprise adoption takes years. Computers, Email, Internet took a long time for adoption. But AI already has 10x faster adoption than those technologies.",1,0.536,POSITIVE,0.979
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","You can always invest in the sectors you're only interested in. Want an ETF that has SP500 excluding tech? There's an ETF for that. Want to only invest in healthcare, or Utilities, or any other sector that you think is not affected by the AI mania? There are ETFs for each one of these things. You just have to get the timing right. If you exclude whatever theme you think is extended, and that theme is still in flavor for the next year or two, that's a lot of gains lost.",0,0.538,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It's not just tech companies. Every CEO is pushing for AI investment right now. Car companies, accounting, insurance -- I've talked to engineers being asked to ""add AI by any means necessary, basically."" The people doing the work know that this is stupid and know the limitations. We do our best to push back enough to ensure that privacy laws are followed. Large sums of money are being wasted and additional large sums will be lost in upcoming lawsuits when customers figure out how their data has been mishandled.In the end it will be very difficult for an outside investor to predict which companies will take a hit from this based on sector.",1,0.537,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","LLMs are just one type of AI. There are also a lot of Machine Learning (ML models) that are doing very well and rely on Nvidia chips.Image processing, for example has done very well in the past 10-15 years. You know that picture you took with the phone? A lot of the processing is done through ML/AI models. A lot of medical devices also use this technology to improve images and remove noise.Natural language processing (NLP) is another; you know how some systems progressively get better search results the more you use it? It uses some type of ML algorithms to learn.Sensor processing is another different one. A lot of our smart watches uses ML technology for algorithm development.ML and AI has been doing very well for a very long time, and a lot of it is hidden to the general public. LLMs are just the one that has the most visibility right now because most people don’t really understand how fundamentally ingrained it is to our day-to-day lives. Almost all high-end technology we use these days has some data science team working on some type of algorithm that uses Machine Learning.",0,0.542,POSITIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",This. People are way too fixated on LLMs and poor implementation of them.,1,0.499,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","But isn’t this the exclusive reason AI has made these companies stock explode?The market is pricing in LLMs revolutionizing the world, and that is not going to happen. Nobody gave 2 shits about ML before and they would not give them now either.",2,0.559,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I was in grad school studying AI at the beginning of the eighties. In the early/mid eighties I was employed doing AI/expert systems. I've been carefully watching, participating and investing in AI and tech in general for over forty years. I just now retired.Keep plowing money into info tech and enjoy the ride. Occasionally it will crash and you'll lose half your money, but it will scream back and you'll be doubling your money before you know it. In the long term it's the best investment you can make. Are we in an AI bubble, sure. Might it get bigger, sure it might double before we get a correction. I've tried to time the tech market, but overall I've lost more money trying to do so than just riding it out. Just keep investing. Only now do I have to start seriously thinking about crash recovery times, not still not too much. In ten years, it'll be much more important.Consider that almost all major business growth, in all sectors, had been due to info tech infusion into the business. Biotech even had it in the name.",0,0.538,POSITIVE,0.976
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Shouldn’t your username be “LongUSA” with that thesis?,1,0.504,NEGATIVE,0.974
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It's vague. The US government policy is set by corporations. Well by politicians beholden to corporations. Which is why it makes sense to be long in the global corporations listed on US based stock markets. On the other hand, those global corporations have no commitment to the country. The citizens do, but they're getting screwed. So the citizens, and country itself are screwed. But the global corporations, well they're in charge and making a killing.Citizens quality is life relative to other countries citizens is falling, middle class is shrinking, etcThe Country is going bankrupt with huge debt that just keeps growing (mostly because of corporate welfare - which I can go on about).Corporations generally have record revenue, record profits, and at the save time are contributing a smaller percentage of revenue to Federal coffers than anyone can remember, maybe less than ever.This is an about forty year trend.ShortUSA. LongGlobalCorporations.",2,0.537,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I think LLMs are being misapplied. It’s a language model, it’s a chat bot. It’s going to be used to replace chat functions like call centers, help lines, secretaries, admins, etc.The visuals stuff is way overstated. ",0,0.505,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I didn’t hear a single person talking about LLMs two years ago and suddenly they’re overrated lol,1,0.518,POSITIVE,0.627
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I didn’t hear a single person talking about LLMs two years ago and suddenly they’re overrated lol,2,0.518,POSITIVE,0.627
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","The examples the OP give are oversimplified and show the lack of understanding in applying AI. A vast majority of the comments and OP are essentially the executives they claim to be over hyping AI, except they are over simplifying.LLM are a learning tool that is progressing towards removing engine searches for general knowledge. The ability to prompt a LLM for specific questions is the equivalent to a detailed Wikipedia. With the bonus of continued investigation on how the human brain functions.Their example of visual learning already exist for certain sectors. But it won’t be generative AI, why does a metal sheet maker need to generate a new image of a metal sheet? AI will be used for fixing the small actions within those fields that can be automated, fixing 1% of your total business will be the true value.Anybody on the outside looking to invest should be asking themselves: “What system 2 action in a sector is being moved to system 1?” and “who has access to private data?”Nobody working on AI is using open source data to potentially influence their business. That is asking for trouble.",1,0.533,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I actually prefer search engines,2,0.505,NEGATIVE,0.837
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","What do you mean by lack of understanding in applying AI? It's not the good samples that you want to generate, you need to generate bad samples that look realistic enough to train an object detection or segmentation model on. The problem is that you need the data to generate the data hence the chicken and egg problem and also why gen AI is overhyped since its just trained on stuff that's commonly found via a search on google images and/or public repositories on GitHub.To add value and solve a complex problem you need to mine data over several months or even years depending on the occurrence rate, and this is exactly the point I'm trying to prove: people who have no clue and overestimate the power of todays AI think that you can give any model an image of a metal sheet and use it to check if there are any production defects, when in reality that does not work at all and requires a substantial investment and also every time the item changes.""Nobody working on AI is using open source data to potentially influence their business. That is asking for trouble."" Tell that to all the people using transfer learning lol, seems like you are the one lacking understanding?",2,0.532,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",How about inversing the proces? Train it on photos of good samples and let the AI discard anything that does not resemble a good sample.,3,0.532,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",How does your company train new human inspectors? Don't they have a database of past defects for them to train on?,3,0.628,NEGATIVE,0.988
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Besides the fact that Customer A shouldn't really be using an LLM for their solution, any model would probably need at least 10,000 examples to get a baseline model worth anything.Human QA Training probably needs less than 100 to be fully able to do the job effectively.",4,0.514,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Generative AI is playing match the input with a output. In which the weights and the process of input/output is a two way street. So you aren’t using generative AI to create new images strictly for the purpose of finding possible defects.The math for these type of networks is already done, there is enough of a base to understand that the problem you are suggesting is broader than the application. Your examples are essentially trying to solve a multi step problem in a single solution.",3,0.533,NEGATIVE,0.995
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I think what you are missing is what's already deployed in the field but no one talks about it because it literally just works.Take retail logistics for example.Walmart, Target and Amazon already have been using AI to optimize their ordering and logistics for years. It is not perfect but takes a lot of tasks off of the human who used to do it manually.Self driving cars isn't what you would expect as Amazon had done it with their warehouses. They use a self-driving robot to move the entire shelf to a human for picking, instead of having humans run around the warehouse for picking.There isn't going to be a big overnight change as people would have imagined but rather slowly replacing humans starting from tasks until it could replace the job entirely.Likely the Internet didn't change how we lived during the dot com rush but fast forward to today, the Internet fundamentally changed how we live. AI would unfold similarly, but faster and greater than the Internet IMO.We just can't predict who is going to win from the AI revolution, just like no one thought a website that sells books in the early Internet age would become one of the largest companies on earth today.",0,0.522,POSITIVE,0.941
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Because of the hype cycle people are trying to apply AI to everything, even in places where it's not ready yet. Just because some applications are not a good fit does not mean all are bad. Claims are that AI basically saved Meta from ATT [1]. I've seen AI improving a lot of boring data improvements that you're probably using and don't even realize.With that said, I'm not an AI will take over the world person (yet). The current trajectory feels more like an assistant akin to the computer in Star Trek. Full disclosure, I closed a portion my NVDA position recently when I was up 600%, mainly because of customer concentration.[1] https://www.adexchanger.com/data-privacy-roundup/meta-may-be-rising-from-the-post-att-ashes-but-regulators-still-have-it-in-their-crosshairs/",0,0.539,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I do wonder about tech valuations, though I have no firm belief. That’s why I simply invest in the global equities market at market-cap weight. It cuts my exposure or the S&P500 by almost half. (I don’t want to be entirely out of tech.)But I think AI has a lot more potential than you seem to believe. It’s not yet at the level of generating a precise, technically sufficient work product, as you say. But there are all kinds of tasks that don’t need that kind of precision, and it’s better to think of it as a tool the employee uses rathe than a replacement for the employee. What I don’t know is who will profit the most from this: tech companies, physical-economy companies, consumers? That’s why I invest in a little bit of everything. ",0,0.527,NEGATIVE,0.883
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Having worked on LLMs for the better part of two decades, I share that frustration. I can tell you what happened at my former employer - the folks who had experience weren’t consulted, in fact our groups were deresourced, and the trustworthy folks who had been great friends to the director level and rewritten certain UX components every cycle got put in charge due to their track record of being able to deliver quickly by cutting every corner. Didn’t end well.",0,0.522,NEGATIVE,0.997
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","As someone who works in the application of AI, there are still plenty of issues for sure, but the tech you’re disappointed by is the worst it’ll ever be, and it’s evolving really fast. The massive influx of cash is driving R&D everywhere, and stuff that will be available in a year will be vastly different than what we’re playing with now. Frankly we can do things now that seemed would take much longer, just a year ago. There are so many aspects where AI makes improvements, while the applications are virtually limitless, it’s hard to imagine a scenario where demand for the technology crashes. ",0,0.544,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Yes, the possible pathways to explore in development are numerous. There is also huge room for growth in many applications. Many people think AI is bad, but we're at the beginning of many different new technologies.",1,0.557,POSITIVE,0.995
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",What are your thoughts on Palantir's ontology and AIP product?,0,0.519,POSITIVE,0.791
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It’s funny, there’s such an AI-hating mob in investing that an anecdote of shit implementation is a sign the biggest, richest tech companies are somehow doing the same.There’s this hurray, we were right to doubt and leave millions on the table. We knew it!You could post this exact thread every month and get the same results. Even the ppl cheering it don’t understand or fully read and will keep cheering cuz they think it’s new or somehow indicative of technology failure.I bought spark plugs once and fit them wrong. This resulted in my car having issues. Clearly automobiles are not ready for mass production and still have hundreds or thousands of serious flaws. Short all stocks trying to leverage this internal combustion engine as a cure all. They got it for tractors and airplanes and busses and it’s not ready",0,0.52,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","My biggest doubt is that the requested investment for most mid to large size companies outweighs the potential ROI within a realistic time frame (as of today). E.g. you can most likely pay a worker to perform the same task for 15$ per hour (and the fact that the worker can adapt to changes faster than it takes to acquire data and retrain your AI model), at least this has been true for the many use cases / PoCs I've encountered during my career.Google/meta can afford to pump out massive LLMs that takes weeks and millions of dollars to train, but if they are not good enough to solve complex problems that are common across multiple verticals for many mid/large sized companies then where is the value? All I see are golden shovels right now and a chat bot that is fun to play around with, please do convince me otherwise.",1,0.52,NEGATIVE,0.991
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I’m not here to convince you and I don’t, and likely you don’t, have insight into the implementations of these things.I’m a tech investor primarily. I was a computer programmer for about 15 or so years. I went through the.com bubble and was born before the Internet.AI has been pushed and trumpeted for decades. And it’s never been there. It’s just been a lot of hot air. They finally have hardware that could implement this stuff and that was always the bottleneck. They’ve had the AI languages forever. You could’ve done this stuff 30 years ago , it’s just that you’d be running on Windows 95.AI is simply the next step in programming. It’s such a no-brainer. It’s everything we’ve ever been doing in coding a step further. Like going to object-oriented programming. That was a no-brainer. From linear top-down programming. The savings in coding and efficiency was massive and no one could predict all the things we would be able to do with reuseable code. Code with hundreds of developers working on their isolated parts.The gaming industry, which is ginormous, would be a speck of a shadow of what it is now if we stayed with old programming languages and models.I am by no means an AI expert. But I know when technology is ripe. And it is imperative we go this direction.But you’re correct in that monetizing it is a whole other matter. And that’s not up to technology that’s up to business people.It’s like we’ve invented the wheel. If people just use them to make frisbees, it’s not going to be any big change. The me existence of the wheel does not buy itself create a engineering revolution. you need people who come up with ideas that can be leveraged and implemented using the wheel.And that’s one of the disconnects I keep seeing people ignore. The technology is no joke. It is a no-brainer. We will go this way and 10 years from now you’re going to be laughing at yourself for ever questioning it. It’s just gonna be everywhere. But making a widget that sells a billion copies is a whole other set of skills. It’s a vastly different set of skills. And we need those people to come forward and start making their widgets.",2,0.523,POSITIVE,0.852
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I like the examples and analogies you made there man, well said.",3,0.629,POSITIVE,0.996
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","There is no hedge, they are too big. If they go down, they are taking the whole market down with them. Even bonds will probably tank. The only thing I can think of would be an option income strategy with a bearish tilt, but that can only do so much.",0,0.526,NEGATIVE,0.996
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","This is what I am scared of, feels like my only safe bet is to cash in on my gains and pay off a substantial chunk of my mortgage.",1,0.504,NEGATIVE,0.87
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","The fact that this commentor said there is no hedge because it wouldn't akr the market down is precisely why you could hedge by buying SPY or QQQ puts. If the AI theme crumbles and takes the broad market with it, being short indexes via puts will offer substantial hedging cover. It'll obviously decrease returns vs a naked portfolio though.",2,0.551,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",My shitty house assessment went up 40k in three years on a shitty house. No investments are safe.,2,0.516,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Cash out anything that is overvalued in your opinion. Use your specialist knowledge in your field to find companies that you feel have a good future AND are undervalued. If there is a great software company which makes money and isn't affected by Ai boom then work out if it's at a fair value or not. This is very boring advice but you are not alone in feeling there isn't much worth investing at current values. Berkshire Hathaway have a massive ever growing cash pile and are taking similar position and are selling out from this Ai boom. Some money in that might do better than a tracker if the next bust is focused on Ai, although they still have a massive, if reduced, Apple holding.",2,0.547,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I agree with this take. If you are concerned about the value of AI long term AND short term, buy BRK.B.If the tech stocks plunge then you can sell some BRK.B and buy in at a price that you're comfortable with.",3,0.563,POSITIVE,0.574
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Do typical retail investors buy BRK.B?,4,0.55,NEGATIVE,0.975
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Yes, fractional shares these days. There is a small army of retail who go to the AGM every year.",5,0.564,POSITIVE,0.996
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Case A is visual processing. We are decades ahead on text processing vs visual. Visual is catching up but you may be asking for what AIs can do in 2054.Case B I know a lot about. Copilot is not the right tool. You need much better context window management and a full fledged RAG (not CoPilot’s barely a RAG). Langchain or Monty for example.,0,0.514,NEGATIVE,0.995
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","What about paying artists to draw/photoshop defects over images of the target products for model training.Shocking I know, paying to create the training data. Imagine it.",0,0.521,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I was going to suggest something similar, simulate the defects in some way.",1,0.511,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","NVDA metaverse is exactly what you're looking for. Model the object, simulate defects, then fine tune on defects. Or better yet, once context windows are long enough, feed the defects in the prompt.",2,0.511,POSITIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","For the hedge part of your question:If inflation comes back - EnergyIf economy slows and deflation pops the equity bubble - BondsIf AI applications are unprofitable but infrastructure arms race continues anyway - Utilities and Bitcoin Miners (because they've hoarded lots of cheap energy)If AI is unprofitable but animal spirits equity rally continues (Fed runs loose) - small caps, equal weight S&P, biotech, clean energy, BitcoinThose are my main market scenarios and the levers I would overweight for each.Of course cash is always a good all purpose hedge.",0,0.523,NEGATIVE,0.989
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",the more posts like this the further we are from the top,0,0.523,POSITIVE,0.992
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Wait until this guy finds out it’s not A.I. at all.,0,0.5,NEGATIVE,0.998
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine.Not an investor, but a tech guy who works with different technologies.To be blunt, ""AI"" has replaced ""blockchain"" for 'buzzword compliance.'That is, while blockchain has its uses, too many technology leaders were reaching for 'blockchain' the buzzword, and trying to figure out how to incorporate 'blockchain' into their products, not knowing if it was the right tool for the job or even if it made any sense whatsoever.And now we're seeing this happen all over again with 'AI'.I'm sure there are valid uses for AI, and I'm sure most companies trying for buzzword AI compliance aren't using AI for anything useful or meaningful. Worse, having been through various exercises in making stuff 'buzzword compliant', often what happens is that the mandate comes down from the upper brass that we have to incorporate some random technology into our product--and that leaves people like me scratching my head and going 'da fuk?'",0,0.536,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I’m stealing “buzzword compliance” from you. Perfect phrase.,1,0.501,POSITIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I think your examples are what we might ask of AI in 10-20 years. AI currently helps developers and people in the weeds. It won't help a C-suite with surface level knowledge do anything.,0,0.541,POSITIVE,0.573
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It can help a lot with big data, which can help the c suite make decisions.",1,0.523,POSITIVE,0.978
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I went to an investing conference a few weeks ago and there were AI hype presentations there as well. One of the claims was even that AI was going to start replacing entire companies! So the hype is real and pervasive.I also work with AI on a daily basis (software engineer) and I feel the same as OP -- it simply cannot do what all the hype is saying.Now as it affects investing - there's a lot of money changing hands based on AI promises, so yes -- there is a bit of a bubble. But I believe there won't be a sudden burst of that bubble....so many companies have made significant investments into their AI infrastructures that they'll be able to pivot into more useful things when the truth about AI capabilities (or lack thereof) sets in. It won't just be a bunch of money lost, but they will have to redirect their resources.Yes, there are some acquisitions happening today (including where I work) that won't pan out in the end, so that's money lost -- but only for the acquirer, the equity owners of the acquired company have converted their equity into cash, which flows back into more investments, more companies, purchases in the economy, etc.So while I do believe there's a bit of overvaluation in the market based on AI hype, I don't think there will be a sudden ""bubble burst"". It will be a slow correction, IMO, and might be marginalized by all of the other factors driving market movements, and therefore not even noticeable.IMO investing to bet against AI won't make you rich. You could make some money, yes, but probably won't be a huge win.This post is intended for educational purposes and is only expressed as opinion, and as such should not be regarded as financial or investment advice of any kind.",0,0.545,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I agree that AI is going to disappoint. It doesn't live up to the hype and there isn't any real evidence it's going to anytime soon. But mega cap will keep on mega capping, they'll find the next big thing",0,0.526,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Even just looking at chatgpt where it was on release vs today I'd say it's made huge strides in a very short period of time.Heck, how annoying was it that at first it was already like 2 years out of date? If I open it right now and ask about an event that happened an hour ago, it can usually answer it for me. The ability to instantly search multiple sources on the web, read them, and parse that data quickly for me is amazing when we're talking actual usability.Look at image and video generation progress in the same time period. Leaps and bounds ahead. Original videos were completely alien, you could barely tell what it was supposed to be. Modern videos at first glance look real.This is a tech that's still basically in its infancy. Is it possible we hit a cap soon that never really lets it leave its infancy without tons more energy and investment? Sure, maybe. But is it also possible we go the other direction and manage to make equivalents to modern gpt/Claude/Gemini that use half the computing power? A tenth? Yeah, that's also possible and we're already seeing much smaller models pop up capable of performing similarly, at least in specific categories of responses.I use it for work and the breadth of knowledge is the most important part. I can google stuff myself and not even find anything relevant. Ask the same thing in gpt and get a specific step by step on how to solve something, or details on an answer I need. I genuinely don't even know where these answers come from, some random conversations deep on the Internet away from googles search results?",1,0.519,POSITIVE,0.991
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",[This user has left Reddit because Reddit moderators do not want this user on Reddit],2,0.504,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",As far as we know it’s just repeating what some guy in his basement was writing,3,0.51,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",[This user has left Reddit because Reddit moderators do not want this user on Reddit],4,0.504,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Buy a liquid alt with either negative or zero correlation to equities. A managed futures fund or equity market neutral fund come to mind.Or buy a buffered ETF with a deep buffer range.Use Portfolio Visualizer to check historical return correlation and volatility.,0,0.527,NEGATIVE,0.998
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",[This user has left Reddit because Reddit moderators do not want this user on Reddit],0,0.504,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","It’s a hundred percent over hyped. I heard some big shot in AWS say that in another two years you won’t need software developers any more and I laughed out loud. I use GitHub copilot at work and the code is spits on is wrong most of the time. I was really hopeful, but finally gave up and shut it off because it was hurting my productivity, having to go through all the code it generated and discarding most of it.I deal with a lot of these big cloud vendors (AWS, Azure, Google Cloud, MongoDB). The first thing they say when you get on a call with them for anything is “have you tried our AI product”. Like you get on a call with your account manager to talk about something completely unrelated you need help for and they start pitching AI until you shut them down just so you can get to the actual reason for the call. My answer is that none of our customers are clamoring for AI feature in our product or even asking for that matter, so it’s not a priority.I think the big tech companies will keep investing though.Microsoft started a war when they invested so heavily in Open AI. This is as much about big tech like Google, AWS, etc… protecting their turf as anything else. It’s worth it for them to spend billions on Nvidia chips as insurance to protect their businesses just in case AI takes over. And the hype machine is about generating ROI for the massive investments in a technology that clearly isn’t ready yet.Based on what I’m seeing, we’re still a long way from achieving what the hypesters are promising. Doesn’t mean we won’t get there eventually, but it’s hard to predict stuff that’s maybe 5 years out. Maybe it doesn’t ever pan out. Seems like AI has made some leaps recently, but it’s been around for like 25 years and always seems to be 5 years out.",0,0.528,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",The true power of interrupting tech is when a new tech combines with another new tech that’s not even invented yet.,0,0.574,POSITIVE,0.556
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Whatever MBA a few years ago was brave enough to start rebranding what AI means is the culprit. The term AI was already meant to mean something much more than what a LLM is capable of, but try explaining that nuance to the general public not versed in tech. Now they have unrealistic expectations of what is coming down the pipe and sooner than later the chickens are going to come home to roost.And there are usually much easier methods of solving a problem than training a neural net. I can't imagine why an LLM would be needed to find memory leaks.",0,0.528,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Go ahead and short nvda if you truly believe ai doesn't have staying power. Don't be surprised when you end up behind the dumpster at Wendy's though.,0,0.525,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","BRK-B is a good way to hedge against tech. Warren Buffett invests in what he understands and he doesn't understand tech, so they don't really invest in tech much; despite this, they have outperformed vs the S&P 500.",0,0.524,NEGATIVE,0.869
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",I read the build-out has just begun. Sounds like you are expecting immediate results.,0,0.509,POSITIVE,0.836
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Software engineer checking in. I stopped paying for Copilot and just started running any questions or code recommendations against any of the million free code LLMs out there. Copilot started recommending dumb shit to me and making me forget basic shit.I do think there's a case for AI when it comes to asking those 101 questions or for those rubber duck moments... But overall, kind of meh use case. I'd rather have a coworker to chat with and, you know, socialize again, with people. 🙄",0,0.526,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I feel the same way, also a Fortune50 DS here. The hype of AI is comically overblown and self-evident. I am not sure how you can hedge against it though, it's just part of the tech stock bubble at this point - even companies that have nothing to do with Not-AI have probably seen their valuation increase because of it for various reasons. The best you could do would probably be to follow some kind of Boglehead strategy, and if you really want to get away from Not-AI, lean into a total international index.",0,0.556,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Developer here as well.The word ""microservices"" kinda made my left eye twitch, and reminded me of this essay about The Churn.",0,0.514,POSITIVE,0.684
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Developer here as well.The word ""microservices"" kinda made my left eye twitch, and reminded me of this essay about The Churn.",1,0.514,POSITIVE,0.684
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","Here is an example:I asked chatGPT a specific technical information regarding a product:Whats the electronic viewfinder refresh rate of a sony a6400 camera?The answerThe Sony a6400 camera has an electronic viewfinder (EVF) with a refresh rate of 60 frames per second (fps).Further questioncan you do it on a6300?the answerYes, the Sony a6300 does offer an option to increase the electronic viewfinder (EVF) refresh rate. You can choose between two refresh rates:60 fps (default)120 fps (high)**To change the EVF refresh rate on the a6300:Go to the Menu.Navigate to the Setup tab.Look for the option labeled FINDER FRAME RATE.You can select either Standard (60 fps) or High (120 fps).The higher refresh rate of 120 fps provides a smoother experience, which can be beneficial for tracking fast-moving subjects. However, it may consume more battery power.Now whats the real answer? Taken from the review website:The A6400 inherits the same 2.36 million dot OLED electronic viewfinder from the A6300, which features a 120fps high frame rate setting to help track moving subjects more smoothly with virtually no lag.Further communication:I think you made a mistake, because A6400 features the same EVF as the a6300 and it seems thier refresh rates can be adjusted both to 120fps.ChatGPT fallows up:You're correct, and I apologize for the confusion. The Sony a6400, like the a6300, does indeed allow you to adjust the electronic viewfinder (EVF) refresh rate to either 60 fps or 120 fps.To adjust the EVF refresh rate on the a6400:Go to the Menu.Navigate to the Setup tab (usually represented by a toolbox icon).Find the ""Finder Frame Rate"" option.Select either ""Standard"" (60 fps) or ""High"" (120 fps).The 120 fps setting provides a smoother viewing experience, particularly useful for tracking fast-moving subjects. However, keep in mind that it might lead to increased battery consumption.So yeah, all that hype for something that can bullshit you with the straight face....",0,0.515,NEGATIVE,0.999
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?",Anyone with a basic understanding of the concepts involved knows the AI of today is incredibly overhyped. And dont even get me started on these 'data scientists/engineers' ridiculous claims of AGI. The smart ones know its a grift but the that wont stop the rest from touting it.,0,0.522,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","This is an imaginary scenario where an LLM shouldn't be implemented, yet you used it to conclude that AI is bullshit, and that the leading tech companies are wrong, and you need to hedge the impending doom. This is wild.Are you sure you work in AI? What kind of AI work have you been doing for several years? Current systems do about 30-40% of my team's work each week, and they're rapidly getting better. Listen to Meta's earnings call. AI is responsible for significant improvements in their platform.Most of the over belief comes from senior positions that have very little insight in how the algorithms actually workWho are these seniors you've spoken with?",0,0.541,NEGATIVE,0.976
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I don't agree. zero-shot foundation models such as Florence-2 are great at captioning or queries on more common objects such as cars and people, etc, however once you put something challenging that its realistically never seen before it struggles and starts hallucinating (for obvious reasons). My point is that useful AI needs a lot of work and fine tuning to be of any value, and to do that often requires more resources than reasonably available, why pay 100k when you can have a worker solve the same problem for a fraction of the cost?I'm happy that your AI model(s) saves 30-40% of your teams weekly work, I never said that AI is completely useless, I just think that ChatGPT, GenAI and all of its derivatives are glorified search engines that output trash most of the time, I personally use it to generate boilerplate code that I need to rewrite. To answer your question: I've worked mainly with computer vision models, multi object multi camera tracking and deep reinforcement learning for automating smart agents.Senior staff / management and board members of large multi billion dollar companies across many verticals who overestimate the capabilities of AI by about 99%. They think that we're halfway to AGI when we are in fact closer to the birth date of Jesus Christ. When I explain what they actually want to do, how expensive and complex the solution is about 9/10 customers usually drop their jaws and back out or even state the magic sentence ""but this is not AI!!"".",1,0.528,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","I think this is anecdotal experience, and your reasoning is dubious at best. I'm also skeptical that you actually work in the AI industry.Nothing you've imagined is evidence that AI is bullshit, or that any company is built on a house of cards, and my companies experience directly contradicts what you're suggesting, so how do you reconcile that? The $100k figure is pulled out of thin air.Senior staff / management and board members of large multi billion dollar companies across many verticals who overestimate the capabilities of AI by about 99%Which senior tech staff do you know at billion dollar companies? Overestimate by 99%? You're just making things up.Buy puts if your this confident. I don't recommend it though.",2,0.554,NEGATIVE,1.0
,"We all read the headlines ""AI is coming to take over"", ""new LLM with massive performance gain"", ""something something AI"". Now let's try to apply AI to real world scenarios that provide actual value for companies (to justify the insane valuation of selling golden shovels - yes I am looking at NVIDIA which I miss when it was just a company building gaming GPUs when I was a kid in the early 00's), here are two examples from personal experiences: Customer A wants to build a system that replaces humans on the production line for QA. Customer A feeds images to Florence-2, visual GPT or any other pretrained generative AI model. Since the data is so sparse (it has not been trained on defects of a niche production line), the output is just complete garbage ""there's a spider crawling across a web in the middle of the image"" was seriously one of the outputs when asking one model to find dents on a metal surface (super visible obvious dent, even a monkey could be trained to detect it). Then you may ask, but why not train a custom object detection model? Well surprise surprise, the defect rate is 0.001% so it will take 10 years for the customer to collect enough data to fine tune a model and by that point the production item is obsolete and replaced by a new one, also gen AI can't generate realistic images because, like before - the data does not exist (chicken and egg problem). Customer B wants to use an LLM to sanitise code from memory leaks and overflow attacks before it gets pushed into production code. Unfortunately the LLM has been trained on global GitHub data which guess what, contains a bunch of crap code full of memory leaks written by humans (again, shit in - shit out). I've also struggled using copilot or GPT for anything more advanced than really trivial stuff, it also struggles writing test cases, it understands the ""general logic"" of the code however it messes up the details completely. So why do I post this crap here? Well, there's this general over belief in AI which scares me as I am quite exposed to these companies from an investment standpoint (I've made a shit ton of money out of it, but it just doesn't feel right at the end of the day). Most of the over belief comes from senior positions that have very little insight in how the algorithms actually work and I feel like all the top 10 companies on the S&P are just circlejerking and propping up their valuations without providing any real value rather than a biased glorified search engine. How can I hedge against this AI bullshit when I am forced to buy these companies as part of my global diversification strategy?","How about 4 out of the 10 largest oil and gas companies in the world? Same with rail, automotive manufacturing, nuclear power, defence industry, etc. The problem is not the tech staff, the problem is the non-technical executive staff above that rides the AI hype wave just because ""they need AI because everyone else has AI"" (isn't that the first sign of a bubble?). I sit with people from companies like these every month and it's the same old story. At least some use cases are viable but require large asset allocation, cash and time to implement, I have a really hard time to see how a shitty LLM prompt can solve any of the problems I've encountered, it just does not work. You need to invest time and money to fine tune the crap out of it and most of the time it's more viable to keep the current worker doing it employed due to time constraints and project budgets. AI is great but extremely overhyped in its current state.Also everyone who invests in the stock market knows that its just herd mentality, even though I probably know more about the practical implementations of AI than 90% of the people who bought Nvidia stock I still need to buy it to not miss out on the run, so no - I won't short it, I will just diversify more.",3,0.543,NEGATIVE,1.0
