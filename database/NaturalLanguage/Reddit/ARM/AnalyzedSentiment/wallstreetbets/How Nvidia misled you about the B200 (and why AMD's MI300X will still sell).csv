date of comment,main comment,comment,depth,PTR Sentiment,Flair Sentiment,Flair Outlook
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Once you go Blackwell you never go Backwell,0,0.502,0.862,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Again you guys never consider the technical support and software suite that you get with Nvidia. AMD is no where close in that aspect ,0,0.571,0.885,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",CUDA rules AI. Not the hardware,1,0.57,0.999,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Straight up, AMD cards excel at benchmarks at best.What, you wanna train a robot in-silica to near-zero-shot your specialized task? Well it's open source so if you can't it's your fault. ðŸ¤—What do you mean you're gonna go omniverse insteadðŸ˜’ oh, you want to actually compete with industry and not play software-dev before getting progress? ðŸ™ƒ",1,0.529,0.87,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Yep exactly. Itâ€™s the equivalent of saying a muscle car with 1000 horsepower is definitively better than a Camry while conveniently ignoring every other aspect of the car ,2,0.504,1.0,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","ZLUDA already proved this wrong... some applications already run 1-1 performance with CUDA on AMD cards through thatn (its a CUDA libary interceptor that runs on top of HIP's runtime natively). And it uses the HIP runtime, natively that means if you optimize your HIP application for AMD it will run just as fast as Nvidia.AMD's software is slower is 100% a myth.I mean what is the explanation for that?Probably that whoever wrote the HIP versions of stuff is a just didn't optimize it or potentially intentionally didn't optimize it.",2,0.539,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Listen: if they were so good, it would be reflected in their earnings.Wake me up when market share correlates to this cope.",3,0.578,0.573,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Listen: if they were so good, it would be reflected in their earnings.That idiocy is exactly why it is not reflected in their earnings.",4,0.561,0.998,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","You call it idiocy, yet they call it CapEx and productivity gains.I've got a feeling the guys moving billions to secure compute have thought about anything you can think of and more.Perhaps, maybe, perchance, there could be the slightest possibility that you understand things incorrectly? Perhaps, maybe, perchance, the ecosystem, compatibility, expandability, reliability... Perhaps there is something superior in NVDAs products that a simple benchmark or $/flop measurement doesn't show?Y'know - maybe... you're wrong? It's not even speculation at this point, we're about to have 4 quarters straight of NVDA leaving AMD completely in the dust.You just have to reconcile your feelings with reality.",5,0.515,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",billions to secure computeYou mean the guy that lost the entire supercomputer market?All the billions he spent and still no next generation super computer will touch him with a 10 foot pole...,6,0.524,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Yip. Ppl have been talking about it for years amd drops the ball with software and support,1,0.577,0.995,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","as tinygrad showed the problem with AMD is at the instruction level right now (might be different between the consumer chips and the DC chips, but I somewhat doubt this).on top of that, add that there isn't this fast interconnect available - you thought 400GbE is fast? try external NVlink switches. they are a killer feature.as much as I love AMD, AMD can't hold a candle yet to B200.",0,0.545,0.528,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",why would anyone buy mi300x if you doubt there is zero difference between software support between amd consumer and server?,1,0.576,0.863,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",I didn't say that. I said that the core instruction set firmware is pretty close in my opinion.,2,0.577,0.944,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","ZLUDA says that is bullshit.... we already have CUDA applications running at native expected performance. So... who are you going to believe, the ZLUDA guy that spent two years writing a CUDA library on top of HIP ... or George Hotz ... LOL. What is his actual last personal claim to fame PS3 hacking???",1,0.522,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",I bought calls because of you,0,0.525,0.972,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Iâ€™m in,0,0.5,0.959,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",so NVDA 1000$ ?,0,0.505,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Can someone more versed in this sector elaborate or point on me the direction to learn more about cuda vs amds proprietary software packages?Also where can I geek out even more and read up on the R&D on both of the flagship chips?Lastly, with regards to Amd vs Nvda, isnâ€™t the Nvda forward earnings much lower trading or undervalued comparatively to AMD? Meaning even at its current price NVDA could still be undervalued when compared to AMD?",0,0.552,0.581,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Where do you read itâ€™s undervalued,1,0.503,0.985,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",amd cant actually compete in the data center space due to how NVIDIA infrastructure is set up. free is not cheap enough.,0,0.56,0.999,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",AMD! ðŸš€ðŸŒ•,0,0.738,0.702,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",appreciate you insight on this.,0,0.502,0.99,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Trust in Lisa,0,0.515,0.998,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Bruh, you put a lot of work into that pump. AMD is fucked.",0,0.567,0.999,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Appreciate your comments and thoughts u/1ncehost.I have two questions for you:Is Moore's law changing? It seems there has been an acceleration in the speed of change and improvement in chip performance due to the AI phenomena?Have you heard of a company called Cerebras (currently privately held, IPO supposedly in the second half of 2024) and what are your thoughts about their approach and strategy?Thanks in advance!",0,0.539,0.875,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","imoMoore's law (i hate calling it a law... cant we call it a theorem or effect?) has been slowing down. The improvements in the AI boom are from parallel scaling and related efficiency improvements. Basically moore's law is 'better' and we are doing 'bigger'Yeah, but they dont provide enough real info for me to understand if they have meaningful breakthroughs. Just remember an IPO of an early stage startup means no educated investors want to back them. I'm doubtful of them because if they were any good we wouldn't have heard of them yet because they'd be too busy getting big contracts.",1,0.525,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","Agreed, law not the best word (it's just the popular phrase to describe the math), good with theorem, effect, whatever. It just seems it will hit a point of diminishing returns at some point but is currently in early stage hockey stick uptrend.I just heard about Cerebras in an article from Barron's earlier this week. Interesting concept if I understand it correctly, kind of a cloudshare/timeshare approach to LLM access, i.e. a SaaS approach to LLM development. Not needed by the big guys but could be a game changer for small and medium sized AI companies.",2,0.521,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",What about that PE ratio though?,0,0.502,0.905,POSITIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.","didn't read, amd is meme in AI space",0,0.627,1.0,NEGATIVE
,"Hi, me again. I gave you the AMD DD yesterday (https://www.reddit.com/r/wallstreetbets/comments/1biqt1k/dd_i_ddd_the_nvidia_run_up_last_year_250700_and/). I liked reading all the insight from people smarter than me and the regardation in the comments, thank you for that. I'm here today to talk about the B200 because it came up a lot. What I'm going to show is that AMD will still sell a lot of MI300Xs even when it comes out. First let me say the B200 is great and will be the leader, and will sell a gazillions. However, for whatever reason Nvidia decided to make their stats and metrics highly misleading in their announcement, especially for laymen who don't know what the fuck they are looking at and just see big numbers and monkey brain instincts kick in. The B200 and GB200 do have a sizeable lead on the MI300X, but its not 4x or 1000x or whatever I was seeing in the comments. The raw performance difference from MI300X to B200 is 90% for FLOPs and 51% for memory bandwidth. The B200 has 36% more transistors than the MI300X, so it is simply a larger chip (by transistor) and that accounts for a lot of the difference. AMD can scale their next chip up if they want more FLOPs and memory bandwidth too. Basically the issues with Nvidia's announcement boil down to 1) not revealing their methodology, 2) doing apples to oranges comparisons, and 3) giving metrics that are intentionally misleading so people watching do apples to oranges comparisons. I'm not going to cover everything, but I will just give a quick rundown of how I got to my numbers. We are going to use these two pages: https://www.nvidia.com/en-us/data-center/gb200-nvl72/ https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html First lets define what we are talking about. MI300X The MI300X is basically like a giant GPU. Up to 8 of them fit in a big computer that goes in a rack in a datacenter. They are connected to the server using PCIE like your graphics card, but are also interconnected with each other using Infinity Fabric cables. Infinity Fabric is important because it allows the cards to bypass the CPU when transferring data from one card to another. AMD's metrics are for one card. GB200 The GB200 is a giant two-rack computer. It is made up of 72 individual graphics cards and 36 CPUs. Nvidia's version of Infinity Fabric is called NVLink, and all of the 72 GPUs and 36 CPUs are all connected to one another using NVLink. This is the largest interconnection of its kind, which is the impressive part of the GB200. Nvlink also can connect multiple GB200s together so all of their GPUs can talk with one another. This scale of Nvlink is really the killer app of the GB200, not the chips. In Nvidia's marketing material, it shows metrics for one whole GB200 system and also for one GB200 ""superchip"" which includes 2 GPUs and a CPU. Superchip is intentionally misleading though, because its 3 separate chips on a board (and one is a CPU), not one big chip. 2x GB200 ""superchips"" each with 2 GPUs and 1 CPU B200 The B200 is a single giant GPU. Its probably something like what each of the GPUs on the ""superchips"" are, but we don't know for sure. Either way its a single chip like the MI300X. I'm going to call the GPUs on the superchips B200s for simplicity sake. Nvidia's Long Green Dick Bar Charts I've looked through all of these, and they are all almost completely meaningless other than as trend indicators. They don't tell you even what metrics are being shown other than a two word description. Forget testing methodologies and all the details needed to understand them. I can tell you that the massive NVLink setup in the GB200 is going to lead to huge efficiency and clustering improvements, but on an individual chip basis, there is not as big of a difference between the B200 and the H200 or MI300X other than size. Comparing apples-to-apples The MI300X's specs are plainly available, but nvidia hasn't released all of them for the B200 yet. Nvidia does give us 10 PFLOPs per ""superchip"" of 16 bit floating point with sparsity (this is a common type of number for AI) and AMD gives us 2.61 PFLOPs per card. Remember, the ""superchip"" is actually 3 chips on a board: a CPU and two GPUs. The CPU on the GB200 superchip is probably providing a minuscule amount of FP16 compared to the GPUs, so we can just divide the number by 2 to get 5. So that's the best metric we can get right now: 2.61 AMD vs 5 Nvidia (FP16 PFLOPs w/ sparsity) or 90% more. Memory Bandwidth For LLM inference, or what happens when you type ""tldr this wsb post:"" into chatgpt, memory bandwidth is the most important bottleneck. Nvidia gives us a comparable figure for this: 16 TB/s for 2 GPUs or 8 TB/s per B200. The MI300X has 5.3 TB/s of memory bandwidth, so the B200 is running with 51% more memory bandwidth. Yes its good, but not out of this world The new nvidia chips coming out later this year are a solid generational move and will be great for customers. However there is no magic, and most of their per-chip performance increase from previous nvidia chips is because they are a lot larger. The H100 and H200 are 80B transistors, the MI300X is 153B transistors, and this new B200 is 208B transistors. There is no magic with the new B200 other than it is a lot bigger in terms of transistor count and has slightly smaller more efficient transistors. The real killer app of the B200 is the way it can be combined with hundreds or thousands of other B200s using NVLink. AMD does have Infinity Fabric which connects 8 GPUs, but they haven't done this scale of integration yet. However its a reasonable assumption AMD can scale Infinity Link in the same way, so this isn't putting AMD out of the question for its next generation. Where does the MI300X fit in? Read my other post for more details about the AMD value proposition, but I'll do a quick summary here. Remember that the MI300X is probably currently the leader in performance and will be until the B200 comes out. That gives it 9 months give or take at the top. However, it does not need to remain the leader to sell in a volume that will be a huge increase in revenue and profit for AMD. There are plenty of niches that the MI300X does fill that the B200 doesn't. For instance the MI300X does have over double the 64 bit floating point performance of the B200. 64 bit floats are used in scientific modelling but not in AI. Presumably Nvidia will release another card that competes for general compute performance, but the MI300X does both AI and large scale compute well. Also for smaller deployments that can't leverage NVLink's large scaling efficiencies, the MI300X can easily make sense. The MI300X does have a similar interconnect at 8-card scale with Infinity Fabric. Also its likely companies will pick up the MI300X simply to diversify so Nvidia doesn't have negotiating leverage on them or because they want to reduce supply chain risk. Lastly AMD can always drop the price and still have a huge profit margin. Its easy for me to imagine a 15% AMD AI accelerator market share with the B200 existing for these reasons. Furthermore, none of the additions from the GB200 or B200 are serious moats for Nvidia that AMD can't cross. AMD can scale up its chip transistor counts, add more stacks of HBM, and scale Infinity Link in the same way. The GB200 is great, but it does not represent a hurdle AMD can't cross.",Company still has literally the fastest AI GPU on the market at half the cost per TF ... and they are meme LOL.Maybe stability.ai wouldnt' be in finanical trouble if they had spent thier money on AMD GPUs instead of double price Nvida...,1,0.582,0.814,POSITIVE
