date of comment,main comment,comment,depth,PTR Sentiment,Flair Sentiment,Flair Outlook
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Apple started working on the arch in 2008 when they hired Jim Keller. Jim has designed AMD K8 and K12 (Zen), Teslas chip for autonomous drive and put down the foundation that is M1 today. Fucking legend",0,0.538,0.761,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Jim Keller is indeed a very capable engineer and a favorite of Reddit. However, there are many other competent engineers in both Apple and AMD and they should also be given credit for the success of Zen and M1. Jim himself said he's more like one of the uncle of Zen and not the father.There are lots of great information in his AnandTech interview and I highly recommend you to check it out:https://www.anandtech.com/show/16762/an-anandtech-interview-with-jim-keller-laziest-person-at-tesla",1,0.538,0.764,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I mean, I'd not undersell a good chip design head. Half the battle of dealing with really complicated architectures is having someone at the helm that can efficiently direct engineering traffic.A part of being a good head is being humble and listening to your subordinates. A worse head would have taken all the credit.It's true that Jim couldn't have done everything on his own. It's equally true that with the same set of engineers a lesser leader would have created an inferior product.",2,0.517,0.938,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",what you are forgetting is innovation changes everything - material sciences change things etc etc - what is relevant today in 20 years will be ancient legacy tech - thats the race and challenge all tech companies have to deal with,2,0.53,0.997,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Didn't someone say amds design is based loosely on a 1960s ibm mainframe in it's processing design and Intel on a 1970s one? Not that newer was better just wildly different ways of doing everything. Amds is harder to wrap your head around and maybe optimize but Intel is brute force standard way of thinking about things,2,0.522,0.97,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Came here to say that. It's also, with the new ultra cpu basically arm taken as far as it's profitable right now in terms of technology. Doing that takes a 1 trillion dollar company",1,0.596,0.986,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","There are still uses for x86. x86 processors still kick the M1 chip from Apple in the butt in a lot of tasks.M1 and ARM have though, opened up a way for more portable and efficient machines for everyday use. A lot of tasks and programs doesnt need the power hungry chips from Intel and AMD. That is an undeniable fact.",0,0.53,1.0,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I do agree that ARM has the efficiency advantage in ultra-low power design. However, I expect the competing ARM processors to be in the ballpark of current Snapdragon and Exynos, instead of the M1 series.On top of that, x86 can be quite efficient in Laptops and handheld gaming devices as displayed by AMD in the Steam Deck. I don't think ARM have a huge efficiency advantage above 10W.",1,0.533,0.718,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",I agree with both of ya. Intel is hitting like 35w and idk about their new gpu cpu family but amd is ballparking like 15-20w and people still complain it's too slow and the gpu too weak and by the way the battery life is awful where apple shines and arm shines with supposedly 8+ hours.The steam deck is crazy impressive but also apple looks like they designed a bigger chip that focuses on doing what it is used for better.,2,0.535,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","It takes even more time and money to re-write software for a different architectureThis isn’t super true especially for higher levels of the stack. Device drivers and low level OS code, yeah, but for everything else it’s not hard to port to a different architecture. It’s way way harder to port to a different OS or even graphics library. The real cost is testing cycles and infrastructure, not the code.Linux already runs well on ARM and Microsoft has windows ARM. In the medium term, I wouldn’t be surprised if more server side stuff moves to Linux on ARM. Performance per watt especially considering cooling costs is far superior on ARM and single thread performance isn’t that important. I’m surprised it isn’t the standard already.",0,0.519,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Linux stuff will move first. Enterprise can't even get off Intel because no one was fired for chosing Intel.,1,0.534,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Windows on ARM still need x86 emulation for lots of the program and both the performance and efficiency is terrible when running emulation currently.Lots of business infrastructure is built on legacy code and migrating those might be more of a headache than codes written in high-level languages. Lots of newer software are designed to be ISA-agnostic, but it's not the case for older software. Lots of banks are still paying big bucks for capable COBOL engineers.On top of that, AFAIK the energy efficiency of ARM isn't that big when going for higher-power design. Jim Keller have said the design is more important than is ISA, and x86 can be very efficient when designed for lower power envelope. Modern x86 is actually RISC-like on some level and high-power ARM designs have quite a bit of similarity to x86 as well.",1,0.53,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","every big tech companies will be designing their own ARBut they are ... I've worked at Amazon, Microsoft and Google and was involved in all their ARM64 efforts. Ultimately compression, encryption and serialization libraries dominated the effort of porting from a performance point of view.volatile and thread semantics were the next, but ultimately an army of engineers, solved that too.I don't think you realize how close both Microsoft and Google are on running ARM64 for large swaths of their workload (hint: Storage Servers on Azure and GCP)",2,0.513,1.0,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","ARM is more cost-effective for some work loads but x86 is still better for general purpose computing in the current state.As processor design and manufacturing become more reliant on economic of scale in the future due to increased complexity, I expect the advantage provided by centralization of dedicated processor vendors to outweigh the architecture advantage and cost saving of custom ARM processor.",3,0.528,0.833,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I am surprised to hear that compression had hand written assembly that needed to be rewritten. LZ4 and zstd do not use handwritten assembly. That is only used in some places for gzip, which… it was gzip, wasn’t it?",3,0.505,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Linux device drivers work across more than a dozen architectures. Only things using hand written assembly need to be rewritten and that is rare. Most of it is in libraries that already have assembly for both ISAs.,1,0.504,0.999,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I don't really disagree, but I think you are overstating things. So here are my disagreements.It takes a lot of money and time to built a world-class chip-design teamAgreed, but I think there are more than a few companies that could do that (or arguably already have). Samsung, Broadcom, qualcom, or nVidia to name a few.It takes even more time and money to re-write software for a different architectureLess time now-a-days than ever before. Particularly if you are already targeting an existing architecture like ARM. Apple didn't make a new ARM instruction set.Very little modern software is written in such a way to make it dependent on the instruction set.Few companies can afford to use the most advanced semiconductor manufacturing node.While true, the same companies I pointed out earlier ARE already using those advanced nodes from TSMC. Really, the only difference is they've not chosen to enter the server market.However, that's something that's been changing as of late. Because of ARM's somewhat natural energy efficiency you are starting to see cloud providers offer ARM based systems.It's really only a matter of time for some of these mobile manufacturers to decide they want a large piece of the pie.The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors.You can't really point at transistor numbers and draw much meaningful conclusions. M1 Max is both a CPU + GPU. Further, it has a rather monstrously sized L2 cache (24MB). The 5900X has a 4MB L2. That's not really being inefficient with transistors.But even so.. so what? Transistor count might push up die space requirements which decreases yield, but that's not really terribly impactful on the bottom line.",0,0.526,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Given the efficiency difference between previous iPhone SoCs and Snapdragons on the same node, I'd say the design team of Qualcomm is far less capable than the one at Apple. Nvidia do have custom ARM-based uarch in the works but there's currently no third-party efficiency or performance data available. Samsung's phone SoCs is even worse than Qualcomm, not to mention catching up with Apple.Apple did built some hardware dedicated to x86 emulation into the M1 series, which is the reason Rosetta 2 works much better than the x86 emulation on Windows on ARM. Considering Apple put precious engineering effort and die space into x86 emulation, and x86 emulation is still a big headache for WoA, migrating software to ARM should be quite hard, otherwise these things won't happen.All other chips built on TSMC N5 came to market far later than the M1 series. Even the Nvidia A100, which cost more than a maxed-out MacBook Pro with M1 Max, is built on the N7 process. Hyperscalers would only have less volume than Nvidia and it would be harder for them to justify jumping to the latest node early.Larger die increases the manufacturing cost exponentially since yield would be lower with larger die. Otherwise chiplets wouldn't be the trend. The cost advantage of AMD Ryzen and EPYC largely comes from the chiplet design utilizing multiple small die to form a single processor, so I'd say transistor count does have an sizable impact on cost.",1,0.522,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Yes i know I'm still invested in AMD,0,0.669,0.998,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Thank you. I really appreciate posts like these that teaches me things that is hard to get elsewhere.,0,0.504,0.981,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Point 2 could not be more wrong. The problem described by it was solved in the 1970s. It is around 50 years out of date.,1,0.513,0.87,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","If moving to a different uarch is that easy, why wouldn't most of the programs on Windows already have ARM version and why would Apple put dedicated x86 emulation hardware into the M1 chip?Moving to a another uarch might be easier than ever, but I think it's still a lot more work than just recompile the code, especially for latency-critical and performance-critical codes which might contains lots of architecture-specific optimization.",0,0.509,0.535,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","If moving to a different uarch is that easy, why wouldn't most of the programs on Windows already have ARM version and why would Apple put dedicated x86 emulation hardware into the M1 chip?Moving to a another uarch might be easier than ever, but I think it's still a lot more work than just recompile the code, especially for latency-critical and performance-critical codes which might contains lots of architecture-specific optimization.",1,0.509,0.535,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Here is the opinion of an actual software engineer. Most of the time, it involves no more work than recompiling code. The issues you describe were solved in the 1970s. Your understanding of software is about 50 years out of date.That being said, there is a such thing as endian-ness that could pose a problem, but ARM switched to little endian at the OS level to match Intel (which could be done because ARM is biendian), rendering endian-ness a non-issue.",2,0.536,0.994,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",If it’s just recompiling then why do lots of programs still rely on x86 emulation on Windows on ARM and MacOS?,3,0.506,0.651,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","The user base for ARM is too small to justify the time spent packaging ARM builds on both platforms. That is changing on MacOS as time passes. The same thing happened when Apple switched from PowerPC to Intel. You did not see everyone ship Intel builds right away (and some never did).Also in the case of Windows, the user base is likely to remain small for a very long time. There just is not the same push to replace Intel among Windows users that is being made in other areas of the industry.",4,0.528,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",AFAIK the Tensor chip is mostly designed by Samsung with some parts by Google. I suppose Google went this way because they do not have the experience and technical assets to design a complete phone SoC from the ground up.,0,0.522,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",AFAIK the Tensor chip is mostly designed by Samsung with some parts by Google. I suppose Google went this way because they do not have the experience and technical assets to design a complete phone SoC from the ground up.,1,0.522,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","People often underestimate the difficulty of designing processors. Even with the money and talent of Google it would still takes years to come up with a competent processor design, and Apple is improving their design at the same time with world-class talents and budget, so I think it would be a long time before Google can catch up with Apple, if ever.",2,0.525,0.932,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","People often underestimate the difficulty of designing processors. Even with the money and talent of Google it would still takes years to come up with a competent processor design, and Apple is improving their design at the same time with world-class talents and budget, so I think it would be a long time before Google can catch up with Apple, if ever.",3,0.525,0.932,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Apple should definitely pump out better hardware than google. Googles an advertising company and apple's acomputer manufacturer. As their wealth has gone into the trillions they've most certainly branched out but to expect google to go from SoO/ads to better hardware than apple would be to expect apple to have a search engine that beats or is as mature as googles.,1,0.534,0.824,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","i wouldn't be surprised if M1 and its successors are beaten soon enough if volume order and cost of development/manufacturing can be justified since microsoft and apple have shown there's market for laptops with capable mobile arm64 chips like this. on software optimization side apple has advantage of having less hardware to support, but i'm not sure that matters as much as whether a given user is looking for apple, microsoft or google software ecosystem.https://9to5mac.com/2021/07/02/qualcomm-can-beat-apple-m1/",0,0.541,0.624,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","The problem is that the cost of development is rapidly increasing with each new generation of semiconductor manufacturing process and the increase in design cost is arguably faster than the increase in market size.When we go to N2 and beyond the design cost could reach 10 billion, and a processor vendor selling to many customers are more likely to have the volume to justify the design cost.Qualcomm has the potential due to them purchasing NUVIA, which contains lots of ex-Apple engineers so they don't need that much time accumulating experience and technical legacy, which is not the case for Microsoft, Amazon and Google.",1,0.552,0.998,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","i don't know that the former Apple engineers are that crucial for beating M1 performance. it's not like apple employees are made of magic. if the economics of the product being designed makes sense for the amount of effort required Qualcomm or similar should be able to surpass M1 even without them.and i'm not sure what you mean with that last bit about MS, Amazon, and Google not catching up. they'd just use SoC from Qualcomm or something. like MS did with their Surface Pro X.",2,0.521,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","None of the Qualcomm chips are comparable to Apple chips on the same process node in terms of efficiency since the A5 in iPhone 4S AFAIK.It's not that Apple have magic, it's the amount of resources they put into the design team over a long time is simply much more than Qualcomm and ARM.I doubt Qualcomm can catch up with Apple since Apple have more budget for the design team and process node. By the time Qualcomm catch up with M1, the new generation of architecture of Apple, AMD and Intel would all be on the market.",3,0.542,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","meh. agree to disagree. if one of the other laptop manufacturers want an overpriced laptop to compete with apple's and give Qualcomm enough lead time on a well incentivized contract, i'm pretty sure they can beat whatever Apple has planned for that generation.",4,0.541,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","The current Apple lineup isn’t overpriced if you take portability and battery life into account. There is no non-Apple laptop that can match the M1-equipped MacBooks in performance, battery life and portability at the same time.",5,0.515,0.889,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",you and i must have a very different definition of overpriced.,6,0.504,0.958,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Yeah, the m1 works for Apple devices because they aren't upgradable, and they aren't customizable, and their customers don't mind at all. It gives Apple more control of cost structure and makes software easier to test (ie they only have to worry about performance on a couple of hardware targets). I think Amazon, Google and Microsoft can use this model for servers, if they want, but designing and maintaining new chips is super expensive, so if Intel or amd are making chips that are ""good enough"", there's not much cost justification to maintain the design team. Worst case for them would be if they spent billions on a home brew chip, then Intel releases something better that same year.",0,0.549,0.998,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Thank you, but no thank you. I want my RAM memory to be upgradable.",0,0.499,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","lol, so hostile",3,0.502,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Apple laptops not suiting your needs and preferences doesn't mean it's overpriced. I use ThinkPad myself and would have purchased the Framework laptop if it was available a few years earlier. However, for some people, no products satisfies their demands better than Apple's offerings, and Apple laptops are good value for those people, thus I won't consider Apple laptops overpriced.",1,0.526,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",While I agree with you he didn't say anything about price,2,0.51,0.912,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",I thought I was responding to another reply that claimed Apple laptops are overpriced. My bad.,3,0.504,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","What you'll find in the overpriced argument is people just want to take the lowest priced components that are anywhere in the same league, total them up, and say the price is crazy! It's like taking a dodge bell cat and comparing it to a Ferrari. While yes the dodge has even more horsepower you aren't paying for horsepower alone.It's unfortunately a conversation not normally worth engaging in",4,0.504,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Another argument to be made on this topic are that most people who buy apple products will never need the computing performance provided by their computer. Using your Ferrari analogy, while the Ferrari is masterpiece of engineering, most people need nothing more than a Honda civic. All of which suggests that many people buy apple products more for ""being trendy"" than the actual utilitarian need for such machine.As an aside, I get a kick out of seeing obese people wearing apple watches. Yes the apple watch is awesome for what they can do. But, essentially, people are paying for a $1200 pedometer",5,0.517,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","When did apple watches start being 1,200? Are these people all wearing Hermes?",6,0.497,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Actually I think Apple products have some merits that is valuable to the majority of users. iPhone have longer software than any Android devices and the app quality is generally better on iOS.Regarding MacBooks, good battery life is valuable for most laptop users and MacBooks has better battery life than most x86 based laptops. The ARM laptops with comparable battery life to MacBooks are usually far worse in terms of performance.On top of that, most tech reviewers agree that MacBooks have the best trackpad and speakers. The display quality is also among the best in their respective price bracket. While most people might not be able to fully utilize these advantages, they should be able to appreciate it.",6,0.527,0.988,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","You completely did not get my point, but I am done with you.",2,0.511,0.983,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","You're kinda sounding like an Apple salesman more than anything. We get it, you like Apple.",2,0.509,0.996,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",That’s just what Apple chooses to do. It’s not a requirement of the architecture.,1,0.505,0.946,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Steer clear of that shit fest unless you're a graphic designer. Hell even creative cloud runs faster and more stable on windows in my experience.,1,0.508,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","x86 is referring to the whole processor family based on x86, which includes x64.Besides consumer devices, lots of server and datacenter codes are also written for x86-64, and rewritting those codes is extreme expensive and risky since rewriting can introduce new bugs which could interrupt business operation.",0,0.539,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","x86 is referring to the whole processor family based on x86, which includes x64.Besides consumer devices, lots of server and datacenter codes are also written for x86-64, and rewritting those codes is extreme expensive and risky since rewriting can introduce new bugs which could interrupt business operation.",1,0.539,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",It's very hard to design processors as efficient as the M1 series even with the transistor budget. Otherwise everyone would be doing it.,1,0.515,0.808,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","It's very hard to design processors as efficient as the M1 series even with the transistor budget.No it isnt.Otherwise everyone would be doing it.The reason why nobody else is doing this is because its expensive, they sell chips rather rhan products with chips and they dont have a huge crowd that will buy their chips regardless of how sithe or expensive they are.Not because its extremely hard.",2,0.519,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","There's currently no evidence supporting your claim.Hyperscalers have always been willing to pay hefty price for high-performance, high-efficiency processors and the market is rapidly growing, yet the current custom-design processors are still far behind Apple in efficiency.On top of that, throwing transistor budget out of the window may result in the processor being more expensive than buying from Intel or AMD, which makes no financial sense.",3,0.539,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","There's currently no evidence supporting your claim.There's currently no evidence supporting your claim either.Hyperscalers have always been willing to pay hefty priceSource?Hyperscalers are extremely sensitive to cost of hardware and cost of running it.Lower cost is literally the biggest marketing point of Gravitron.yet the current custom-design processors are still far behind Apple in efficiency.Which processor you're talking about?throwing transistor budget out of the window may result in the processor being more expensive than buying from Intel or AMD, which makes no financial sense.Which is exactly what i saidOn similar lines,Hyperscalers have always been willing to pay hefty price for high-performance, high-efficiency processorsalso makes no financial sense.And well, if hyperscalers are not throwing transistors like Apple, then you cant really say thatyet the current custom-design processors are still far behind Apple in efficiency""""Ultimately, people wont buy intel/AMD chips even if they are sithe or extremely overpriced like they would buy Apple hardware.",4,0.528,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Energy cost is a significant factor in TCO of processors if it's running on high-load 24/7/365. That's why we see EPYC and Xeon chips clocking lower and closer to the efficiency sweetspot than their consumer counterparts. EPYC and Xeon chips are also significantly more expensive per core when compared to consumer hardware.The AWS Graviton you mentioned is far less powerful on a per-core basis compared to the M1 series. Considering the power consumption of the M1, I doubt the Graviton could be more efficient.On top of that, Apple hardware isn't shit or overpriced at all after the M1 series are introduced. The Macbook Air actually has better performance and battery life than Windows laptop in similar price range and of similar portability.",5,0.52,0.602,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I doubt the Graviton could be more efficient.So the source foryet the current custom-design processors are still far behind Apple in efficiency.isDude, trust me.Got it.AWS is not throwing transistors like apple.Point is nobody is throwing transistors like Apple for consumer stuff, mainly because its not economical, not because its hard.",6,0.513,0.999,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Anything related to stocks? Yes the x86 is still relevant and even an Intel atom with some ram still usable for the average user.,0,0.525,0.984,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",It means AMD will continue the fast growth and not going out of business due to companies turning to designing in-house ARM based processors.,1,0.589,0.992,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I do agree that AMD is not easily replaceable. But your DD is tech heavy and very light when it comes to investing .fast growth, you need to justify it beyond a single point.Tech is unpredictable especially without numbers to back it . Not too long ago Intel wanted to stop its chip manufacturing.Toshiba once a leader in laptop's, hard drives, sold it entire PC business if am not wrongGateway is long gone and forgotten. Among many others.So can you justify fast growth?",2,0.543,0.865,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","AMD has 49% revenue growth and 67% EPS growth for the last quarter and it’s trading at the P/E of 37, which shouldn’t be the case if people expect the growth to continue.The growth rate of AWS, Azure and GCP are all insane and these create lots of demand for AMD processors. Their enterprise revenue is up 75% YoY, and with more computing and business moving to the cloud, the best is yet to come.With TSM reporting 35.5% YoY revenue growth this quarter I expect to see AMD to have 40%+ rev growth since growth on advanced nodes should be higher than average for TSM and AMD is mostly on N7 and N5(products on N5 are not yet on the market).People don’t expect AMD to sustain the growth and the point of my post is that it will.",3,0.585,0.904,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Any thoughts on the potential of RISC-V?,0,0.511,0.953,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","If a capable design team is put to work on a RISC-V based uarch there could be something amazing. However, it seems most companies don't consider RISC-V mature enough for adoption and the openness and lack of license fee not outweighing the potential development and migration cost.",1,0.521,0.896,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Have you heard of Rivos?,2,0.497,0.994,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Acquiring PA Semi & Intrinsity is Apple's best decision ever.,0,0.521,0.912,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","So why would Nvidia try to buy Arm and make an Arm CPU?Why would Lisa Su of AMD say""I think AMD has a lot of experience with the ARM architecture. We have done quite a bit of design in our history with ARM as well. We actually consider ARM as a partner in many respects.""""From an AMD standpoint, we consider ourselves sort of the high-performance computing solution working with our customers, and that that is certainly the way we look at this. And if it means ARM for certain customers, we would certainly consider something in that realm as well,"" Su explained.Why would intel join RISC V?You make reasonable points, but the news seems to point in the opposite direction.The next generation of ARM essentially builds into the architecture the features that Apple has been creating from scratch and making them available to the parties that choose to license out ARM designs so they configure them to make their own chips. This means other companies will have a much easier job catching up.",0,0.542,0.852,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Both AMD and Nvidia already have a capable chip design team and it's much easier for them to build a good processor on ARM than companies that have to build a design team from scratch. What matters is having a good chip design team and not the ISA of choice. Nvidia was trying to buy arm because they want complete control of a ISA, and they can't make x86 processors anyway due to licensing.Apple would come up with a even better architecture when the processors built on next-gen ARM come to the market. You can't catch up with Apple without the budget and talents of similar ballpark.",1,0.542,0.988,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Apple’s pockets are deeper than most, I agree with that.But I don’t think the other big 3 would stick with x86 out of a sense of hopelessness, I think they’ll take the competitive advantages of ARM/RISCV.Are there benefits to x86 other than Raw Power from the massive power draw?",2,0.51,0.916,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","x86 isn't inherently energy inefficient. Efficiency depends on the objective of the design and the capability of the design team more than the ISA. The i9 12900K performs similarly to the M1 Max when both are power limited to about 30W while the i9 is built on an inferior process. While 30W is probably the efficiency sweetspot for the i9 and far above the sweetspot of the M1, it shows that x86 have the potential to be very efficient if designed for energy efficiency and built on a good process.The main advantage of x86 is compatibility with legacy codes, which is a significant part of business infrastructure.",3,0.525,0.693,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",What is a semiconductor manufacturing node?,0,0.535,0.689,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","The minimum feature size (I.e how small are the transistors) - could be 7 nano meter or 5 nano meter, etc.",1,0.507,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","The question isn’t whether other companies will switch to ARM with a M1-like offering of their own, but whether Apple will gain market share as a result of M1",0,0.543,0.994,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Apple isn't interested in getting into the server and datacenter market which is the most important and high-growth sector in the next 5 years, that's why some of their engineers left to found NUVIA.",1,0.549,1.0,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","One other thing-Apple dictates the hardware and locks their OS so it only runs on THEIR hardware. It has been this way for decades (there are workarounds) Apple is the only company that makes both the operating system AND the hardware it runs on. Microsoft might be able to pull off something like this, but it's highly unlikely given their abysmal record on hardware manufacture.",0,0.543,0.962,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Makes sense for Apple as they can use the M1 (and varients) to power all their devices as well one language to run it all. Vertical integration to keep costs down, similar to what car manufacturers do building a common platform.Love see windows go ARM given the many advantages. There is even a ARM version of Windows. However will take a lot time and money for transition, not only CPU manufacturers but more legacy software companies not wanting to spend more money to transition.Couldn't believe the initial idiot backlash for the M1. It's a huge step forward",0,0.521,0.619,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","Point 2 is wrong. You can just recompile for ARM in most cases and things will just work (unless you rely on hand written assembly, but that is rare). This is why Linux systems can be migrated to ARM without an issue. I migrated a client to ARM last year and it was easy. I just recompiled the source code for ARM and that was it.",0,0.534,0.999,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Yes,0,0.5,0.992,POSITIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",I wish apple would have tried to buy ARM instead of Nvidia,0,0.559,0.996,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.",Nobody has mentioned Global Foundries? $GFS.They used to be part of AMD. They bought a few of the former IBM foundries as well.,0,0.563,0.623,NEGATIVE
,"When the Apple M1 series came out, lots of people were saying that x86 is doomed, Intel and AMD will be out of business and every big tech companies will be designing their own ARM-based processor. In my opinion, these statements are quite detached from reality and is a display of lack of understanding of semiconductor and technology. Here's why: It takes a lot of money and time to built a world-class chip-design team: Apple started using their in-house designed ARM-based architecture with the iPhone 4S, which was released in 2011. They should be working on the architecture at least 2 years prior to that, which means it takes Apple more than 10 years to be able to build the M1 series. Almost every generation of Apple's in-house design has a huge efficiency advantage over the standard ARM core, which implies the design team of Apple is far more capable than the team in ARM and Qualcomm. Despite that, it still takes them more than 10 years to accumulate the technical legacy and experience to make the M1 series. Few other companies can afford to spend that amount of money and time do built a design team as capable as Apple's, even if they could, it could still be more expensive than just buying processors from Intel or AMD. It takes even more time and money to re-write software for a different architecture: processors are expensive, but the time of competent software engineers is even more expensive. Good software engineers are required if you want to migrate your software to a different ISA and have it performing well, and they are expensive and rare. It makes more financial sense for companies to assign these engineers to other purposes than to re-write software that runs fine on existing hardware. On top of that, most companies doesn't have the complete control over their platform like Apple does on OSX and iOS, which would makes this task even harder, more expensive and time-consuming. Few companies can afford to use the most advanced semiconductor manufacturing node: Apple started using the TSMC N5 process more than 1 year earlier than almost everybody else, which is a significant contributing factor to the efficiency advantage of the M1 series. Even datacenter products with extremely high margin such as the AMD EPYC and Nvidia A100 are one generation behind in terms of process node. It takes an insane amount of money to design a processor on the most advanced process node, hundreds of millions for N5, billions is expected for N3. If processor vendors like AMD and Nvidia can't race Apple to the most advanced node, processors for in-house usage by Google, Amazon or Microsoft definitely won't have the volume to do so. The M1 series is actually quite transistor-inefficient: the M1 Max contains 57 billion transistors, in comparison, the RTX 3080 contains about 28 billion transistors and the Ryzen 5900X contains about 19 billion transistors, even the Nvidia A100 contains ""only"" roughly 54 billion transistors. While the M1 Max is powerful, it's definitely not as powerful as the RTX 3080 and 5900X combined, even if it contains more transistors. If only fabbing cost is considered, a M1 Max is more expensive to manufacture than 2 RTX 3080 since the cost per transistor stopped dropping since 28nm. The design of M1 series sacrifices transistor efficiency for energy efficiency, which makes sense for mobile products, but for HPC applications better transistor efficiency should be more desirable. AMD and Nvidia products are still more efficient in terms of computing power to manufacturing cost ratio. In conclusion, the M1 series is the result of Apple pouring tremendous amount of resource over a long time, which most companies can't afford to do. Even if they could, it would probably make more financial sense to simply purchase processors from existing vendors.","I think TSM is a better buy than GFS at its current valuation. TSM has far better moat, solid growth record and far higher margin.The only thing going for GFS is lower P/S ratio but it's cancelled by the lower margin.",1,0.521,0.577,NEGATIVE
