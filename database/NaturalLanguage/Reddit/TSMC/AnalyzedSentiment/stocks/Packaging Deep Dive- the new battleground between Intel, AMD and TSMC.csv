date of comment,main comment,comment,depth,PTR Sentiment (Comment),Flair Outlook,Flair Score
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","That’s a great read. I am a packaging engineer and I loved reading this. As you said, Intel isn’t out of the game yet. But if they don’t get their shit together in next couple of years, they will be the next IBM.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",I’m an IBM employee who recent bought 100 shares of Intel. Your analogy makes me nervous.,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Intel won’t die overnight. So no worries. If you are 5 years or 10 years long, then the future for Intel is uncertain compared to 5 years ago.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Ibm also didn't die overnight.,3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Yep. At this point, the fate of Intel is uncertain. Because they are also tying up with TSMC for fab.",4,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Thanks! Would you mind if I asked you a couple questions and picked your brains? (We can do it over DM/here)Firstly, Intel talks a lot about bridge dies being more cost effective to manufacture than active interposers. (But then they're more expensive to validate for)Would you know roughly what sort of manufacturing cost differences you'd see between the two approaches?Secondly, there isn't that much stuff out there about how you can cool a stacked chip. The micro-channels seem to be the only idea which has been publicly released. Do you know of any alternatives to this? Or is the research more focused on alternative materials that are cooler/making each die more power efficient etc.?",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Costs not exactly my forte. But I think yield is the major difference between using an interposer vs bridge die. Interposer tech has been around for a much longer time compared to Intel’s EMIB. So, Intel has a lone hill to climb in getting the packaging yields as high as that of the interposer package. Maybe they were able to offset it by using smaller bridge dies as opposed to a large interposer.I am only aware of the solutions you already described. As far as I know, Thermal dissipation is a big bottleneck as of now, to scale 3d stacking performance-wise. Research wise, I recently came across a great seminar on this exact topic through MIT Innotherm Colloquium: https://youtu.be/DBaBNPcLNhoThey cover a lot of research in there. I don’t recall too much detail.There’s one going on right now on a different topic (thermal comfort). Check it out if interested: https://mit.zoom.us/j/95435192213",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Ah that's great. I suppose they must have been referring to the theoretical yield/cost possible with EMIB Vs having TSVs and an interposer. Given their recent history with yields though I'm not holding out too much hope for EMIB yields. The zoom seminar seems very interesting, thanks for the link!",3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","From your comments I feel like you hate microchannels or something. There is nothing wrong with microchannels. Regardless, another promising approach is nanoscale radiative heat transfer.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",What happened to IBM?,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Nothing. They didn’t capitalize on their leadership position in the computing industry and became complacent just like Intel became complacent, and in process lost the leadership crown.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Thank you! $100c 1/15/21 TSM,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",will read later but TSM is the best long in the semi space,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Im with you. Literally the only worry i have is China invading Taiwan, otherwise i think they are the best growth stock bar none.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",I highly doubt China will physically invade Taiwan. That would spell too much trouble and they don't need to physically invade Taiwan to damage them it's the 21st century.,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Skyworks or TSM both are comparable. Imo?I’m interested to be proved wrong (seriously),1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","yes they are comparable, but they aren’t really competitors. Both manufacture chips, but the manufacture them for very different products.TSM primarily focuses on computer and smartphone chips(think AMD, Nvidia.) Basically they make the CPU’s, GPU’s, etc. I honestly don’t know much about Skyworks, but it seems like they do a lot of things with radio frequency. So they make chips that allow phones or other devices to connect to wireless signals.So you could certainly buy both and not have to worry about them stealing customers. They actually have some customer overlap (Tsm makes ARM bases chips for Apple and skyworks is making 5g chips for Apple.)I just like chip manufacturing companies in general. It is very difficult for companies to both manufacture and design chips. Just look at Intel. These chip manufactures do it better than the rest and they will always have customers as long as they keep improving their tech.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Thank you for the clarity. Regarding the two, they are both huge suppliers for Apple and their phones. Not to mentions “millions” of other useful applications. I find they both have a strong infrastructure to meet demand but I’ve always been confused on what exactly makes them different.I’m a big semi conductor fan. I have a significant portion of my taxable portfolio in SOXX which covers the gamut. I’m also like Enphase and a few others in the ETFI’m with you on Intel, especially for the price that the stock currently is sitting. Hard to pass up.",3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Curious on your thoughts about nvidia acquiring ARM, and how that affects intel.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","It's a difficult call. If this was Intel from just one year ago I'd say ARM would become a big player in laptops, while x86 (Intel) holds its grip on datacentres. However, in laptops with the resurgence of AMD and subsequent Tiger Lake launch from Intel (assuming Intel gets decent yields even on these low power smaller die chips) means that the performance differential between ARM and x86 has been reopened. It then comes down to whether users can actually that much performance in laptops or whether battery life is the predominant concern. Personally I lean towards the former but I'm very much biased as a tech enthusiast. If it's the former, x86 wins, if it's the latter ARM wins.Either way this is wholly dependent on ARM CPUs making a big jump in performance, to Apple ARM levels, which would require a lot of focus from Nvidia on this area. It's worth noting that the predominant focus of the acquisition appears from Nvidias comments + rumours to be datacentre expansion, so they might not even place that much time on laptops where they have the highest chance of success.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",intel = screwed. laptops will start using ARM chips from Nvidia instead of qualcomm once acquisition is fully through in a year so QCOM is screwed too,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",No laptops will not start using Nvidia ARM chips in a year or so. Do you know how long it had been taking ARM to get a foothold? You think Nvidia is going to have a superior arm core producing in volume to takeover the laptop market in 1 year+. They have practically zero experience designing high performance arm cores.,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","The personal PC market is not very important to Intel. Losing it to Nvidia isn't high on their priority list, and they certainly aren't ""screwed"" if it happens.",3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","macbook and surface both use ARM, I think it’ll just be a matter of time before other laptop makers follow",3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Yes but how long is that matter of time when amd is rapidly increasing the perofrmance gap with their mobile apu's.,4,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",I guess you don't know that Nvidia has been designing ARM processors for some time already. In fact the Nintendo Switch processor is a Tegra X1 (Nvidia ARM).,3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",I was aware of that which is why I said practically zero experience. They have a few other desgins which haven't been successful. My main problem though was that in the comment they thought this could happen in a year.,4,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",The merger will take 1.5 years they said so Intel still have time and more importantly they still have market share,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Hello. I'm a bit confused with ASML. Are they a competitor to TSMC or something? Do they supply all of them with lithography machines?Edit: Thank you all for the responses!,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","No knowledge in the field but as far as I know ASML has a monopoly on EUV lithography machines, so TSMC is not a competitor.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Yeah ASML are the only ones producing EUV machinery so ultimately all the foundries premium (7nm+) wafer capacity is constrained by the supply of EUV machines from ASML.They are not a competitor to TSMC, Samsung or Intel but those 3 are heavily reliant on them.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Euv is only one part of an extremely complex process. There are reasons tsmc intel and samsung are at different playing fields rn.,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",ASML provides foundries like TSMC with EUV lithography machines. They sell practically everything they manufacture and there's a long line to receive the machines. They are not at all a competitor.,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Same question here,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",ASML sells EUV and DUV lasers to them.,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","""Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion""Those R&D are not comparable. A big percentage of Intel R&D comes from design team.Also lots of acquisition like Mobileyes which shouldn't be comparable to TSMC.For a $100K TSMC engineer, Intel have to pay $200K. But that doesn't mean Intel's individual engineer is more productive than TSMC engineer.For example, a special TSMC engineering team implement 3 *8 hr shift for 24 hr non stop R&D which is not possible in Intel.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Thank you for this post. I always appreciate a deeper understanding in the companies I am invested in. This is one of the best posts I have seen a while.,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Around the time that 10nm started looking... less than ideal their R&D spend ramped up massively, so circa 2014. https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expensesIt's worth noting part of the growth will have been to do with acquisitions, like Mobileye and Altera. They've made plenty of advanced in technology, just not that much in areas that are hugely lucrative to the top line yet - e.g. Optane memory, self driving etc.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Around the time that 10nm started looking... less than ideal their R&D spend ramped up massively, so circa 2014. https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expensesIt's worth noting part of the growth will have been to do with acquisitions, like Mobileye and Altera. They've made plenty of advanced in technology, just not that much in areas that are hugely lucrative to the top line yet - e.g. Optane memory, self driving etc.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",It looks like their R&D expenses were around $12 billion in 2015. They seem to have increased YoY with a decrease in 2018-2019.2015 - $12.12 billion2016 - $12.68 billion2017 - $13.03 billion2018 - $13.53 billion2019 - $13.36 billion,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Maybe a bit off topic, but does this amazing write up not pertain to Micron Technology as well?I know they have 3D products as well and wondering if any or what the cross over is",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Yes, this applies to Micron. 3D stacking has been used in NAND and DRAM for a few years as you mentioned, but the competition there isn't as fierce. RAM and NANDD both have standards that companies have to work within, so the competition there is more focused on costs and margin expansion. These expensive advanced packaging techniques are only going to be used when they basically have no choice and existing technology can't reach the minimum spec.In a similar vein, NAND and DRAM can use EUV just as much as CPUs and GPUs can - but since there's no need for it yet MU are unwilling to pay for the machines.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Ahh thanks for the insightWhile I know they are quite different products I have been a bit confused why most of the semi sector has done so well while MU has been left behind,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","MU is a very cyclical stock. While the areas they compete on are oligopolies, that's still worse than most of the semi industry which is in monopolies. This means that the stock price (and short term financial performance) are hugely dependent on DRAM and NAND prices, which are basically commodities.Over a long period of time MU are going to do very well, but you need balls of steel to go through their cycles. Personally I can predict the cycles quite well so I buy them for a few months in a trough before selling them. I find them to be one of the best stocks to do this more speculative type of investing/trading since the fundamentals are strong and when I buy them on the dip I'd be happy holding them for years anyway.",3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","I mean. As far as I understand AMD and Intel both buy wafers from TSMC and Intel keeps building fabs but AMD rents their fabs. This leads to AMD having limited manufacturing power while Intel just keeps building fabs and scalinh their manufacturing to infinity. And all the while they can buy more and more of the wafers TSMC makes through demand, which will eventually short AMD a supply of wafers due to the fact that Intel could just outbid them for the same wafers because they can afford to. That would put AMD out of supply for quality wafers and they would sink.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Intel owns its own fabs; while AMD is fabless and buys wafers from TSMC. Due to their lithographic issues outlined here Intel have been forced to turn to TSMC for chips.The second part of your statement would be true in a completely free market, but TSMC's management have stated that they don't view Intel as a long term customer (while they do view AMD as one). This is pretty evident given the fact that once Intel sort out their process they'll abandon TSMC and go back to in house manufacturing only, meaning TSMC's long term interests are aligned with ensuring AMD sees long term growth over Intel. From the comments TSMC management has made over the past few years and the execution of their business plans, I believe that TSMC management is focused more on long term value generation than short term profit, meaning your scenario wouldn't be likely to occur.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","You've really done your homework. As an AMD investor, I appreciate your insight a lot.",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",There is a subreddit r/AMD_Stock that has detailed this for the past 4 or so years.,3,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Thanks for the heads up. Subbed now. I still appreciate op's summary a lot,4,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Yeah it's solid,5,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Thank you for the write up and sharing the info.,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Hey man great write up, gonna read it with some tea later, appreciate the content!",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Great read. Just read both 1 and 2. So Swann is Littlefinger?,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Exactly.,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Why is Swann Littlefinger? Seems like he didn't even want to positions but said YOLO. I see Murthy Renduchintala as Littlefinger. Seems like he was the cause of a lot of internal tensions and he got kicked out.,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","And then you realize that GPUs are the future since CPU performance gains YoY are pathetic in comparison, 3D stacking notwithstanding.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","This is also applicable to GPUs as I mentioned. I mainly dived into CPUs as it's more mature there, but there's no reason it couldn't be used in GPUs.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",I just bought all semiconductor and chip makers just to be sure,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Would it be possible to get a TL DR for the lazy?,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Thank you for both posts! A lot of it went over my head, but you did a great job of explaining it nonetheless. As an Intel investor myself, I have to admit I was somewhat blindsided by the recent earnings call which caused a 20% single-day decrease in share price. Made me a little upset that in went from being +25% after 2.5 years of holding strong to only about about +7% as of today. Like you said, this was not terribly surprising to those who were actually paying attention.I imagine Bob Swan’s remaining time at Intel is limited. I know with people who worked with Bob at Radial and eBay, and he later oversaw the spinoff of PayPal from eBay, which I would imagine is what opened up my job back then.I do have a question for you u/InfiniteValueptr Where does Micron Technology fall within this arm’s race of semiconductor companies? I see plenty of positive news and soaring share prices for companies like NVIDIA and AMD, but the memory chips for Micron are quite disappointing as a long-time investor.",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Copied from a different comment on MUMU is a very cyclical stock. While the areas they compete on are oligopolies, that's still worse than most of the semi industry which is in monopolies.This means that the stock price (and short term financial performance) are hugely dependent on DRAM and NAND prices, which are basically commodities and MU doesn't have much pricing power.Over a long period of time MU are going to do very well, but you need balls of steel to go through their cycles. Personally I can predict the cycles quite well so I buy them for a few months in a trough before selling them. I find them to be one of the best stocks to do this more speculative type of investing/trading since the fundamentals are strong and when I buy them on the dip I'd be happy holding them for years anyway.Lmk if your questions is more on how MU fits into the value chain of semiconductors rather than their outlook, but I hope that answers your question.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","This submission has been randomly featured in r/serendipity, a bot-driven subreddit discovery engine. More here: r/Serendipity/comments/itstzq/packaging_deep_dive_the_new_battleground_between/",0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",Thank you very much for this.,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",,0,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",TL;DR It's complicated.,1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",The entitlement of some people is mind blowing sometimes.,2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Would you actually invest your money based not just on a redditor's claim, but a TL;DR version of a redditors claim?",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","TL;DRPackaging is in its infancy, but 3D packaging(stacked chips) is the future. Problem is they haven't figured out a solid way to cool stacked chips. With Intel's budget on research, seems likely they'll be able to crack the code first.",1,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves","Oh and forgot the most important part. Intel's internal struggle has been Game of Throne and is currently being run by a balancing act between LittleFinger and the Iron Bank.Hopefully that's an accurate-ish TLDR... It was an interesting read, but I have minimal knowledge of semiconductors",2,nan,nan,nan
,"This is part 2 of a series on semiconductor companies. If you read part 1,available both on my blog and reddit, you’d be forgiven for thinking that the future is all doom and gloom for Intel. However, the reality is that’s not necessarily the case. Their longer-term future bears some promise. Although it’s incredibly important, lithography is not the be-all and end-all of integrated circuit design manufacturing. Another critical part which has often been overlooked in the past is packaging – essentially how you make the calculations done on the transistors be transported to the outside world. The process of making a semiconductor device consists of fabricating on wafers with lithography, slicing the wafer into dies, which are then packaged. Packaging protects the die from damage, and lets the chip communicate with the outside world – as well as within the package itself. One of the few things Intel management have said over the past few years that I whole-heartedly agree with is that packaging, not lithographic process, is the future. The main reason for this is the falling benefits of moving to smaller processes [1], coupled with rapidly increasing costs – a EUV machine to make advanced processes can cost up to $125million [2], while the R&D costs to make smaller processes are also increasing [3]. What is packaging? The best analogy for understanding packaging design is that of architectural history. Imagine each transistor is a human worker. Traditional lithographic improvements have been focused around making the ‘workers’ more productive in smaller spaces – first hunter-gathering, then farming, then offices, etc. However, we’re now at the point where workers have so little room to move in their workspaces (cubicles) it’s extremely difficult to do any more cost-savings the traditional way. Instead, what we need to do is break out of the traditional paradigm we have of placing everyone in one massive building and move to more radical designs. There are two main ways we could go about doing this: chiplets and 3D stacking[4]. In the terminology of our analogy, chiplets are like if you split up the old massive office building into a campus with multiple smaller offices, where each team can work on a job that they’re specialised in, or where you can cut costs by having inexperienced interns working in one particular department that’s not too important instead of being forced to use identical workers of the same quality across all departments under a ‘monolithic’ , single-office design. The challenge we have is to ensure that communication flows easily and quickly between each different office building. 3D stacking is like going from a society where we only have bungalows (single-layer chips, the traditionally dominant choice in high-performance computing) to high-rise buildings (3D stacked chips, where you have multiple dies on top of each other). The challenge we have is constructing life-support systems like elevators, that can enable data to transfer rapidly out of the CPU, or sewage pipes, that help let waste (heat) escape. Chiplets Chiplets are sometimes called 2.5D stacking, referring to the fact that they’re commonly viewed as a stepping stone between 2D and 3D packaging. The most common method, the one that’s widely used right now (notably in AMD’s Zen 2 architecture), has an ‘interposer’ sitting underneath the chips you want to connect.[5] This interposer acts as a conduit between the two (or more) chips, enabling rapid communication. Think of it as having a concrete floor between the office buildings, where before you had knee-height grass making communication between offices inefficient (PCB traces). AMD famously uses Infinity Fabric, which we’ll discuss later, but on a hardware level it’s likely that they’re using TSMC’s CoWoS – Chip on Wafer on Substrate technology. In this implementation, the Chip is put on a Wafer that helps communication between the chips (the concrete floor), which is put on a substrate (the ground). [6]The second approach, which Intel is pursuing with EMIB (and TSMC are exploring in very early stages), is basically forgoing the concrete floor and choosing to use tunnels between the office buildings instead. [7]While this approach is faster – each tunnel is dedicated to only connecting two dies, so you won’t have issues where data is trying to get to lots of different places at once and getting ‘stuck’, as you would with AMD’s approach, it’s harder to design products for – you have to design and place a tunnel, rather than simply placing a concrete foundation and plonking whatever you want on the foundation. Really, this is a simple continuation of the two companies’ philosophies. AMD’s HyperTransport, the predecessor of Infinity Fabric, is an open source interconnect that can easily be implemented across processors. Meanwhile, Intel’s EMIB predecessor, FSB, was proprietary and needs to be specially modified for each processor design. While Intel has made it’s successor to FSB, AIB (advanced interface bus) open-source in an attempt to alleviate flaws, much like its 10nm process, the differences inherent in initial design choices are difficult to overcome. Which approach is going to be superior remains to be seen. In my opinion, it’s evident that each company has chosen the approach that’s best for it – Intel with its larger budget/manpower can afford have teams of engineers working on specialised implementations of packaging for each product, to get slightly better performance; while AMD’s smaller size drives them to seek a scalable, easily replicable design that can be used across many designs with minimal adaptation. Infinity Fabric, which is AMD’s own protocol implementing extremely high speed connections between different dies (essentially, the secret sauce in how the ‘concrete foundation’ is made), can in theory be used by any company to connect their custom chiplets to Zen chiplets with minimal input from AMD, which is impossible with Intel. [8] At the same time, it could also be used by AMD in GPUs to make chiplet-based designs, which could have a massive impact on scalability of performance, manufacturing costs etc. – just like what happened in the CPU area. One only needs to look at the massive Nvidia A100 chip, with its correspondingly massive manufacturing costs, to see how much of an impact this could have on the market. Chiplets are here right now from AMD/TSMC, they’re sort of here from Intel, expected to come online at a much larger scale from Intel in the next few years, and will be here to stay. The two ‘teams’ have differing philosophies in how their chiplets are constructed, and it’ll be fascinating to see which is more successful. In my opinion, if Intel can improve the morale of its engineers and kick things into high gear, EMIB will be more successful in high-performance computing workloads, while the simpler approach from AMD/TSMC have secured them dominance for the next two years at the very least. 3D stacking In theory, going 3D is just another form of chiplets (you put each chip on top of each other, instead of next to each other), but the challenges are sufficiently different that it’s easier to discuss them separately. 3D stacking is probably a lot closer than what most people think. Despite the fact that Intel announced its’ ‘Foveros’ stacking tech in 2019 [9], Samsung announced their ‘X-cube’ packaging a few weeks ago [10], and TSMC announcing their ‘3DFabric’ family just a few days ago [11], the reality is that 3D stacking has been around for years. As explained earlier, the problem with 3D stacking is that you need to dissipate the heat, as well as transfer data. High-performance chips like those made by Intel, AMD and Nvidia produce large amounts of heat and require large amounts of data throughput, making it difficult for them to use 3D stacking. However, other types of chips with lower heat and data, like mobile phones, have been using PoP (Package on Package) stacking forever, which basically folds a 2-chip processor in half like a book, to reduce to horizontal footprint while increasing the vertical footprint.[12] Even ‘true’ 3D stacking – where you build layers one on top of another, is viable in certain use cases. DRAM manufacturers have been using 3D stacking since 2011, and NAND manufacturing wasn’t far behind.[13] SK Hynix recently released a 128-layer NAND SSD! For the high-performance chips we are most concerned with, we have mostly solved the issues with transferring data, and simply need to solve the cooling issues and manufacturing challenges. The way that data is transferred through different layers is using TSV (through-silicon vias). [14] These are essentially elevators going through the layers that transfer data. Compared to PoP solutions, the chip is higher density, and the connection is shorter -elevators are direct and faster. While we can technically stack any chips we want together right now – TSMC’s SoIC is slated for mass production in 2021 (likely for mobile chips), [11] the issue is that cutting edge chips – especially x86 chips used in datacentres which go for thousands of dollars, throw off a lot of heat. AMD Epyc’s 7742 has a TDP (thermal design power – an approximation of how much heat a processor outputs under load) of 225W, [15]while Intel’s Xeon 9282 has a TDP of 400W. [16] When you consider the fact that all this heat is coming off a piece of silicon <500mm2, it’s apparent that it’s extremely difficult to disperse all that energy. Current techniques, where we essentially have a lump of metal on top of the chip conducting heat up to a fan that shifts huge volumes of cool air past the chip, cooling it down, are about as optimised as they can be. Before we can stack CPU logic chips on each other, we most likely need a paradigm shift in design philosophies – either drastic improvements in efficiency, or more likely some way to conduct heat out of the chip better than we have now. The most advanced suggestion so far seems to be tiny channels between layers that circulate coolant inside the chip, making it so each layer is cooled well rather than just the top layer as is the case with current designs. [17] Obviously, this is completely uneconomical to mass produce currently. However, it is possible with current technology to place a less hot component on top of a logic die and reap the speed benefits between those two chips – for instance Samsung’s newly announced X-cube technology, which places SRAM on top of a logic die. [10] We’re likely to see a few years of this sort of in-between state, where RAM; IO; FPGA etc. are stacked on top of logic dies, before we reach the ‘Holy Grail’ of high-power logic on logic – and perhaps even every component in a system, from CPU, GPU, RAM, to various accelerators being all packaged into one single chip horizontally, laterally and vertically. True 3D stacking is so far away that it’s impossible to see the winner, however when judging the intermediary efforts that various vendors have announced, I am of the opinion that Intel has the greatest potential with Foveros for the reasons below; while Samsung is extremely competitive – even leading, in the areas where it is offering solutions, and TSMC is the overall leader with the widest array of offerings which are all highly effective, both in cost and performance. After reading all that, you probably feel cheated. Wasn’t I going to tell you why Intel’s future is promising? It feels like I just explained the opposite doesn’t it? Well, the reasoning for why I said Intel’s future is promising is threefold. Packaging is in its’ infancy compared to process. Intel might appear to be behind now, but the distance is definitely not as much as they are behind in process, and given where we are in packaging maturity (we’re at the very start of the diminishing returns graph, where it’s easy to make big strides), Intel could easily climb ahead given their engineering prowess. In addition, opposite of what has become evident in lithography, where outsourcing to a specialised foundry has ‘won’ the battle against vertical integration, it could very well be that integration will be more successful in packaging, with a holistic, unified approach to designing a chip proving useful. Intel spends far more money on research than anyone else - $13 billion in 2019, while TSMC spent $3 billion. (or 18.6% of revenue vs 8.5%). [18][19] Although Intel’s operations are broader, with investments in anything from process/packaging to self-driving and memory, it’s likely that Intel’s spending on packaging and process exceeds that of competitors. In addition, Intel’s engineers are colloquially known to be some of the best in the business. It’s current woes are largely a function of management problems, not monetary or talent-based. Intel’s (mis)management deserves a post, if not a book of its own. Suffice it to say the corporate equivalent of Game of Thrones has been going down for the past decade, and now Littlefinger has finally managed to slay his enemies and gain complete control. Littlefinger may not be the best person to lead an semiconductor company, and his position may be constantly under threat from the Iron Bank (the board), but at least the internal politics and strife should be largely out of the way, opening up the chance for a focused Intel putting their full momentum behind a comeback – a terrifying thought for any competitors. If these possibilities do come to pass, the future of AMD, and to a lesser extent TSMC/Nvidia looks bleak. Semiconductors are an incredibly complex industry. The very things that create such huge barriers to entry for competitors, and impressive returns for shareholders, make it difficult for us to truly understand the technological aspect of the companies are we invested in. My investing style relies heavily on an understanding of the things I’m investing in, and I hope that this series looking at some of the more technical aspects of semiconductors can help all of you gain an understanding as well, and to cut through the marketing spiel that management regularly throws at us. Feel free to contact me if there’s anything you’d like me to look at, and thanks for taking the time to read! [1] https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/ [2] https://www.eetimes.com/euv-tool-costs-hit-120-million/ [3] http://euvlsymposium.lbl.gov/pdf/2012/pres/G.%20Yeric.pdf [4] https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/ [5] https://www.researchgate.net/publication/340843129_Chiplet_Heterogeneous_Integration_Technology-Status_and_Challenges [6] https://www.tsmc.com/english/dedicatedFoundry/services/cowos.htm [7] https://www.intel.com/content/www/us/en/foundry/emib.html [8] https://www.amd.com/en/campaigns/hpc-solutions-webinar [9] https://www.eejournal.com/article/intel-foveros-3d-packaging/ [10] https://news.samsung.com/global/tag/samsung-x-cube [11]TSMC Technology Symposium 2020 [12] https://sst.semiconductor-digest.com/chipworks_real_chips_blog/2019/01/16/the-packaging-of-apples-a12x-is-weird/ [13] https://www.nvmdurance.com/history-of-3d-nand-flash-memory/ [14] http://www.appliedmaterials.com/files/Applied_TSV_Primer.pdf [15] https://www.amd.com/en/products/cpu/amd-epyc-7742 [16] https://ark.intel.com/content/www/us/en/ark/products/194146/intel-xeon-platinum-9282-processor-77m-cache-2-60-ghz.html [17] https://www.nature.com/articles/d41586-020-02503-1 [18] https://www.intc.com/filings-reports/all-sec-filings/content/0000050863-20-000011/0000050863-20-000011.pdf [19] https://www.tsmc.com/download/ir/annualReports/2019/english/index.html [20] https://www.tomshardware.com/news/intel-leadership-tech-team-changes-not-delayed-murthy-renduchintala-leaves",For real? Read it and make your own. How about props for the in-depth blog?,1,nan,nan,nan
