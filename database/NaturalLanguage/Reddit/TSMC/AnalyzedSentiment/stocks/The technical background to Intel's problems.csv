date of comment,main comment,comment,depth,PTR Sentiment (Comment),Flair Outlook,Flair Score
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This is probably all true, but it's also all superficial to the core issues at Intel. I'm pretty sure TSMC could have made quad-patterning work if they've gone down that route. The real problems at Intel I think is that they've lost their mojo, it's a company run by bean counters and they lack innovation, it's a company that kills talent.Jim Keller joined Intel 2 years ago and just recently left without accomplishing anything. Pushing things through all the bureaucracy and getting it approved by the bean counters just kills any innovation. There's in fighting and civil wars going on inside Intel, Jim Keller couldn't take it anymore and left. Raja Koduri is apparently gunning for CEO with his massive ego, most of the executives hates each other and want each other's job. They lack leadership.Any hope you have when joining Intel quickly gets killed, and then your days are spent just finishing your work and going home. Nobody really gives a fuck, that's why there was just a huge leak of internal files and documents. Screw this job, grab some random files, password is Intel123, upload them online, who cares.10 nm is a complete shitshow? ""Well, I did exactly what you asked me to, don't blame me. I said this was stupid idea from the start.""Clearly they have the money to build great products, I don't think that's really the problem. It's people having been content with shoveling shit out the door since 2011. Now that they need to change, they are starting to realize they can't.It's happened to the Russian Space Agency,https://www.wired.co.uk/article/soyuz-rocket-launch-failure-emergency-landingIt's happened to the Boeing,https://www.theatlantic.com/ideas/archive/2019/11/how-boeing-lost-its-bearings/602188/It's happened to the legacy auto manufacturers,https://www.handelsblatt.com/english/companies/sibling-rivalry-audis-problems-through-technology/23829266.html?ticket=ST-3649131-Dav4cw1TPPOLjkUvXM4X-ap5You reach the top, you start stagnating, several years later when you actually need to kick back into gear, you realize that you can't.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","great comment, and im (was) an intel bull at these lows",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","My dad is somewhat important at Intel and he thought it might not be the worst idea to invest in the firm now. This is saying a lot for him, he never recommended the stock to me before.He thinks Intel will be fine in the long run and will continue to innovate whether or not they still produce semiconductors. Says Bob Swan is maybe the second-best CEO they've had since Andy Grove. Forgot the rest of what he mentioned but I can ask if you guys like. He doesn't have any crazy insider trading knowledge, I've tried...",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Intel financially is very sound and undervalued compared to the industry. Plus they have all the capital in the world to make a change into any direction. But you hit the nail on the head, they need innovation and better people in their executive positions. That will happen imo but who knows how fast? I personally think intel is a great value right now and have been continually cost averaging into a bigger position while it’s this low.Time will tell but just go look at companies like AMD and NVDAs p/e and p/b ratios, margins, etc and you’ll see intel is wildly undervalued as a stock compared to their peers. Let’s just hope INTC can get things back on track technologically and we could see a nice comeback.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This is the death of big tech giants and it happens over and over again.HP is another example where bean counters took control and destroyed innovation for short term gains. As is typically with bean counter companies, they tried to acquire their way to success (Has that ever fucking worked?). The end result has been the slow bleeding, breakup, and sell off of HP. Same thing happened to AMD when they spun off Global Foundries.There's also a big issue with executives thinking they can hire their way to innovation and fire their way to profitability. It's an open secret in the tech industry that the more senior you become, the more likely you are to be fired due to the salary you command. The way the bean counters see it is ""Hey, these senior folks are costing us a lot of money, lets bring on a bunch of cheap interns, train them with the seniors, and then fire the seniors.""The problem? It takes YEARS to gain the experience needed to approach what (some) senior tech people are providing in value. Further, people aren't fungible like bean counters think they are. The fact is that a junior tech person is going to deliver a far lower quality product than a senior person. Bean counters don't give a shit about that. They think they can compensate by hiring 10x the juniors. (because, obviously, when you have a bunch of cooks in the kitchen food starts tasting better... right?)Probably the worst part about it is that junior people usually produce more features than senior people. Why? simply because they are less careful in what they are producing. A senior tech person will test the shit out of the stuff they write to make sure it is bullet proof. A junior person will through out shit, get burned, do it again, get burn again. Yet, bean counters usually measure by how much shit is going out, not how good it is.Then, they are stuck with a company that has a metric ton of shit to clean up throwing their hands in the air saying ""How did we get here! What can we do!""... and, unfortunately, the result is usually ""Well, lets fire more senior people to free up more budget to hire a bunch of junior people. Then, we'll have a big push to fix everything!""Finally, when the whole company is on fire, that's when they start saying ""Fuck it, let's break up the company to get rid of the parts that aren't turning a profit""You can imagine how well that all works.In the mean time, guess who is getting fired and who gets bonuses? You guessed it, board members get big payouts and stay on staff right up to the brink. The employees get to experience mass layoffs and ""fuck you""s from a company they may have been at for the last 20 years.Intel looks to be headed towards that death spiral. Fuck those high powered CEOs and their non-tech advisors.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This, this is so true!",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","My thesis is on this, I'll avoid the technical details:Iterative improvement require specialisation. You want to get better and better at a very niche thing. You want to outpace all of your competitors are realising the next marginally improved version of that thing.The only way to out compete the market leader in an iterative race is to look very far into the future and leap frog them, or to disrupt and break new ground. Sometimes the market will present an opportunity for disruption because of new tech in other areas.A company which has hyper specialised has often lost its disruptive innovators, the people who rapidly and readily adapt to change. Their processes aren't built for change. Change is high risk, they don't want it.So, you get these situations. Companies that are absolutely fragile in the face of new, disruptive opportunities.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","That's cool writing a thesis on that, in what field are you doing it?Although, I still think you're missing a part with just iterative improvementMcDonnell, later merged with Douglas Aircraft, later bought by Boeing, are the ones who built the both original NASA space capsules, Gemini and Mercury which carried the first American into orbit.They should know how to build a crewed spacecraft, it's what they do, or at least what they used to do.Yet, they deliver a botched flight that fails it's primary mission, has ""Fundamental Problem"", can't synchronize a clock and would have killed people unless for it having failed less catastrophically earlier.The Boeing 707 was the first widely successful commercial jet in the world. They should know how to build planes, it's their main source of revenue. New airplanes do not fail. Newer planes are always safer than previous planes. The Airbus A380 had it's first flight in 2005 and thus far had zero fatalities. Airbus A320neo first flight in 2014 and 1,306 planes delivered, fastest selling commercial aircraft ever, 5x more than A380s, zero fatal crashes in it's entire history.Boeing 737 MAX, first commercial flight in 2017. On October 29, 2018, Lion Air Flight 610, 737 MAX 8 plunged into the Java Sea 13 minutes after takeoff, All 189 people on board died. Same thing again in early 2019 with only 387 total planes in service. The plane has been grounded ever since.Like, How do you fuck this up? This is what you do. World's largest plane manufacturer, and their most popular plane isn't even allowed to fly in the air anymore.Maybe if you say iterative improvement towards making money, Boeing has definitely specialized in squeezing every dollar out of it's government contracts, they're really good at that. It's almost like a self-destructive cycle, iterative improve your profits to the point where you actually forget wtf you're supposed to be doing, and the whole thing comes crashing down into the ocean.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",That's what happens when you have executives that only care about profit.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Iterative processes are good at improving what you design it to improve every step up to some limit. In other words, it might not improve what you desire. And the result can be worse than what you started with",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Reading all those articles again was pretty depressing actually, so on opposite end, the companies in the process of killing them,TSMC CEO, Dr. C.C. Wei, PhD in electrical engineering from Yale Universityhttps://www.reuters.com/article/us-taiwan-tsmc/tsmcs-chang-known-as-father-of-taiwans-chip-industry-to-retire-idUSKCN1C70NHAMD CEO, Dr. Lisa Su PhD in electrical engineering from MIT,https://www.forbes.com/sites/patrickmoorhead/2016/11/01/amd-ceo-lisa-su-and-the-art-of-a-turnaround/#397db9f149fc (This was written in 2016 when AMD was at $10)SpaceX COO, Gwynne Shotwell MSc in Mechanical Engineering and Applied Mathematics.https://edition.cnn.com/2019/03/10/tech/spacex-coo-gwynne-shotwell-profile/index.htmlTesla CEO, Elon Musk BA & BSc in physics from University of Pennsylvaniahttps://interestingengineering.com/elon-musk-innovator-and-engineer",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",One of those people is not like the others,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","That's why Musk has Gwynne Shotwell as a handler. She's basically as much of a cult god as Elon is in SpaceX fan circles. Elon time is a meme, Gwynne time might happen.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I've learned never, ever, trust anyone who calls themselves an engineer (or Doctor) but doesn't have the credentials.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","IMO, for a tech giant to be successful, it HAS to be ran by someone with a tech background.Tech companies die all the time because the tech founder leaves the company only to be replaced by assholes with business degrees who think every company can be ran like Toyota.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",core issuesHow many cores tho?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",My graduate supervisor is an Intel alumni and has said exactly the same thing unfortunately.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Can confirm. I have a two friends currently at Intel who are sr process engineers and they literally work less than 2 hours a day and then bounce. They are both very much in the “I hope they fire me” mindset. They may or may not already be working second jobs at FAANG companies,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","As someone doing research in the nanodevices space I appreciate your accuracy in summarizing the scaling techniques that have been involved in moving from each technology node to the next so far!Just some things I think are worth pointing out: Intel has actually switched to a tri-gate architecture, which is very similar to FinFET except that it lacks a spacer on the top side of the Si channel, this means tri-gate controls electrons in the device channel from three sides while FinFET only really has control from two. Now this might all sound a bit pedantic, but the reason I bring it up is that it means that moving from tri-gate to GaaFET is actually much less of a technological leap than going from FinFET to GaaFET, which is already much less of a leap than going from a classical MOSFET to FinFET. Moving forward it's really anyone's guess if any nodes further than 7nm will actually be possible, we're approaching some serious fundamental physical limits with Si and so it will be exciting to see where the industry goes. I'm not sure where the transistor industry is with the adoption of wide bandgap semiconductors but if scaling picks up with those we could eventually see considerably smaller devices in the future. The other billion dollar engineering problem is heat management, processors have been stuck at or around the 4 GHz mark for years now due to the inability of Si to dissipate heat at higher frequencies. Whether this is achieved through new materials (once again wide bandgap SCs like SiC do this much better) or exotic materials processing (thermal bandgap materials are an interesting new area of research) I think building transistors that can operate faster may be the path the tech development takes in the future.Anyways I'm rambling, I don't get to talk this stuff too often and thought you might appreciate these thoughts. Cheers!",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Please discuss more on thermal bandgap materials, please.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I'm not an expert but the basic principles is that heat within materials is transmitted by excitations called phonons with a characteristic wavelength. By fabricating superlattices (alternating nanometers thin films of different materials) with thicknesses equal to these phononic wavelengths, you can encourage reflection of phonons within a given layer and achieve destructive interference, meaning that phonons with that particular wavelength are ""forbidden"" similar to how semiconductors/insulators have forbidden energy levels for electrons which form the material's electronic bandgap. The current challenge with thermal bandgap materials is that materials often have several different phonon wavelengths that can exist in the material, so if you make a film with thickness equal to one phonon wavelength it won't likely block phonons with a different wavelength. Therefore current research focuses mainly on blocking the most common phonon modes in the materials.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Gonna save this post until I have time to read it. Thanks in advance looks like a lot of work,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",What's the link to his blog though,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",www.infinitevalueptr.com/blog,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I found links in your comment that were not hyperlinked:www.infinitevalueptr.com/blogI did the honors for you.delete | information | <3,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Some 20 years ago, Lucent Technologies (now Nokia) had an x-ray lithography process... How would that compare to EUV? (Wavelength advantage?)I haven't a clue what they've done with it (x-ray) since, but I'd think UV has about reached its limits.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yep, wavelength advantage. EUV is 13.5nm, X-ray can theoretically go sub 1nm. (For reference, silicon is 0.2nm).I haven't seen much discussion about commercialising X-ray lithography; the current consensus seems to be that it's more cost-effective to go down the 3D route with chip stacking, 3D transistors etc. X-ray and Deep XRL(basically the X-ray version of EUV to UV) could very well be what we turn to after those, when we get to single-atom transistors.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Thanks.. I'm wondering if x-ray will ever see commercialization. (I learned about it thru an old lecture series Lucent had),2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",You can never say never with the chip industry... It wasn't that long ago I was wondering if we would ever move off 4 cores and now I'm rocking 12 cores!,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Oh, true.My dad worked for RCA... Broadcast equipment. They had a flat screen, and a (1970) $1 billion CRT production line. Guess which they kept?There's more to development and sales than logic!",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Single atom transistors are impossible from a quantum mechanical point of view.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Not impossible, they've been demonstrated, it's just the temperature issue that's the biggest problem.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","That isn't the issue, unless you are going to cool the transistor to milli-kelvin temperatures... The electron wave function will have a substantial tunnelling effect through a 1 atom transistor. I've taken a few courses on semiconductor physics. If I'm wrong, please link me the source and I would be glad to learn from it.",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",https://www.nature.com/articles/nnano.2012.21Not as experienced as you from the sounds of it but just reading the paper looks possible.,5,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Thanks for sharing. I'll definitely be reading that later haha.Quick glance though -- yes this single-atom transistor operates at milli-kelvin temperatures:The transistor operates at liquid helium temperatures, and millikelvin electron transport measurements confirm the presence of discrete quantum levels in the energy spectrum of the phosphorus atom.Very cool research nonetheless.",6,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I don't know anything about x-ray litho technologies, but any attempt to go sub-10nm with it will suffer from even worse issues than EUV sees from ionizing interactions. Namely, the generation of secondary elections and widescale proximity effects in the resist. I don't see x-rays being at all viable in overcoming contrast and aspect ratio issues. In fact, I doubt the industry will use wavelengths shorter than current EUV ever, but instead switch to multi-column electron beam lithography or something else entirely if they even want to go lower. Which they may not, considering the other computing hardware advances to be had beyond just transistor densities, like quantum computing and dedicated architectures.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",So...buy AMD. Got it. Thanks!,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Looked at the post, didn’t understand a damn word, saw your comment and said “ah fuck it let’s go in on AMD.” I’ve always liked underdogs and since Apple is splitting with intel in the future, this is a chance hereLet’s go AMD TO INFINITY AND BEYOND!",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This is really great OP thanks.Can anyone explain AMDs stock price to me? P/E of 160, so 10 fold growth to become as big as Intel is already priced in? Where's the upside?""While it initially manufactured its own processors, the company later outsourced its manufacturing, a practice known as going fabless, after GlobalFoundries was spun off in 2009.""So what are you actually buying? Any bulls here willing to give a summary? Genuinely curious as I must be missing something.Maybe something like the overall market has huge room for growth and AMD could benefit from that?I see cutthroat competition up against intense technical barriers based on fundamental physics, like quantum tunnelling, as the OP points out.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This market trades on news, story, and headlines. AMD is going to have a lot of good ones over the next 2-3 years. Intel won't.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Nah the whole industry is stand still. Anything industry that’s stops innovating will move to other country such as Taiwan for commodity production.Issue underlying all these technical errors is that all the talent goes into software development and not hardware development since pay is 2-3x.Most of the EE that graduated in my class last year from grad school go to software since they also code decently well.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I don't think a single analyst on Earth would say AMD is stalling. And they're just about to drop new and improved hardware...,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Lack of American talent is flowing into this field. The current work force of super talent Asic and Microarchitecture design is from the 80s and 90s and will be retiring in the next 10 years.It’s harder for a chemical engineer PhD or skilled person to switch to some sort of software dev. Most of my EE graduating class from UT is going into software.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Idk why you're getting downvoted, there's absolutely a glut of software engineering jobs out there. There's still up and coming industries in hardware though (such as wide bandgap companies like Cree and Qorvo) so I don't think hardware jobs are dead in the water in North America.",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","When I was in grad school for EECS from 2009 to 2014 there were zero other Americans in my research group or the adjacent research groups. Meanwhile there was no shortage of smart, motivated, engineering students from Asia. The US is going to face serious brain drain issues if we continue down our current path.",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Well he is kinda right. Silicon is reaching its physical limits,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I dunno, I think I’m gonna ignore you. I’m having too much fun watching AMD go to the moon with my money in it.I just bought another stake today actually, with the convenient 5% discount.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","The whole indsutry? AMD is pushing forward accesible micro chips to be featured in next gen gaming, including Sony and Microsoft. They will continue that relationship going forward.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",The incoming talent is important. You will see the effects in 5-10 years.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Seems like one takeaway missing here is that companies like $ASML are good investments, since EUV is superior to patterning and ASML is the only company who can manufacture these lithography systems.Doesn't matter if Intel jumps on board, if AMD continues to use EUV with great success, etc....ASML is going to profit because they're supplying them all",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yep, there's basically no limit to demand for EUV machines. I'm yet to see ASML be able to ramp up supply correspondingly though and translate that demand into bottom line profit to justify their p/e.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Are you saying ASML are the ones selling the shovels?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Shovel?,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Metaphor. The old saying to invest in the guy selling the shovels in the gold rush.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Oh. Never heard that before.Yeah, this is the shovel guy",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",can you post a link to your blog in the comments?,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Sure, here it is:https://www.infinitevalueptr.com/blogMods, if linking in comments isn't allowed, please delete this comment.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Completely offtopic, but as a rebuttal to some of your points on railways in africa in An opportunity that many fail to see.No, because of- Theft of infrastructure.- Lack of infrastructure / maintenance.- Competition from alternate providers - eg in South Africa the Taxi industry is actively destroying trains - setting them on fire, stealing wiring.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Great post thank you, I'll continue to hold AMD",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade.In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues.Intel bulls should pay close attention to this part. Bulls talk all the time about Intel's superior margins, but those margins are not some Intel magic. They're just a function of(a) A large market for 14nm chips where they have lots of capacity that's probably pretty depreciated and the per unit cost is super low.(b) Intel maintains its strength in datacenter where the margins are super juicy and have been the biggest driver of Intel's earnings growth for the last few years.If the highest margin products get chased off 14nm because of heat, yield, performance, etc. issues vs AMD and Intel's next process isn't ready to take up the banner, its margins will fall quickly. Operational leverage in industries with high capital expenditures works both ways.Bulls will also point to Intel's large growth in datacenter/enterprise, but that's mainly because of the emergency expansion of computing capacity of its existing client setups because of covid-19 and Intel's security patches. At some level of performance disparity, organizations will commit to Epyc for newer setups as Microsoft and Google have already done for their cloud solutions.14nm is now under strong assault in every product segment from AMD. Even Intel's OEM cabal is starting to lose faith because of inventory and performance issues. 10nm yield has questionable economic viability in terms of yield and performance; it's already a burning platform. 7nm is the last shot for Intel's former glory, and I just see bad news.Intel has other initiatives, but I doubt that they'll grow fast enough to offset the x86 pain that is coming in the next 2 years.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Another thing that gives Intel's datacentre an advantage is that all workloads are written with Xeon in mind, and the software optimisation offputs a considerable chunk of the difference in raw compute power.The thing is that hyperscalers have the ability to optimise software for Epyc, something smaller data centre operators can't afford to do.The stuff Google/Microsoft etc are writing for Epyc and releasing as open source software should reduce this last bastion of competitive advantage too.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","AMD has done really well against Intel when they could bypass OEMs and go more straight to the end user (DIY, hyperscalers, HPC). I suppose that's been the strategy from day 1: use this group as a foundation plus Tier 2+ OEMs to attack the rest of the x86 market and leave the tier 1 OEMs beholden to Intel as phase 2.AMD's SVP of DC, Forrest Norrod, said somewhere that Rome was the clear winner in about ~70% of use cases. But Milan will raise that number to 100%. Sure, there's some AMD puffery there, but it does look like that Zen was the proof of concept, Zen 2 was getting the foot in the door, but Zen3 will be busting it down and where I think the margin will come from.You never know if Intel will pull a rabbit from its hat for 7nm. AMD was at death's door 4-5 years ago. But unlike AMD's resurgence against an Intel just milking everything for margin, Intel is going up against an AMD (now powered by TSMC) whose strategy, execution, and luck have just been amazing.I don't think AMD can afford to give Intel any breathing room. Rather than try to optimize for margin, AMD should run as fast as they can to move the margin battlefield away from the 14nm ocean into the 10nm pond. Cannibalize your own top end if you have to and make Intel stroke out trying to get 7nm to work while preserving its markets.But some skin in the game: INTC 210618P40 @ ~$2.75. See y'all in May 2021.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Couple of completely anecdotal things here: When creating wafers intel has always gone with the ""tick-tock"" idea where first you shrink the processor(tick), and then later, you change the design (tock).With 10nm Intel got cocky and for the first time said fuck it, we're doing both of these changes at the same time. Obviously that didn't work out and gave them all sorts of delays. Like....they were getting real bad yield results on the wafers.This wouldn't be as big of a problem if the Krzanich hadn't bungled it up more. The guy would not hear bad news. You were risking your job if you told him shit wasn't working. So no one really told him and then shit hit the fan.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market shareThis doesn’t bode well for AMD’s future stock performance. In 2013 Intel stock had traded flat for a decade and was down almost 70% from its 2000 high. If Intel’s peak as a company does not align with the 1990s growth period when their stock price rose 6900% in 10 years (comparable with what AMD’s has just done) but with this later period when they were dominant but their stock price was stagnant, then that should imply that once AMD overtakes Intel as the market leader and moved into its own peak period, the stock growth period will have long since ended.Or, it just means company performance has next to nothing to do with stock performance, in general.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",TLDR - AMD all the way.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Link the blog, I’m an engineer but a civil engineer, I’m going to dip my toe into computer engineering a bit so I can better understand these processes",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I have some basic undergrad level notes on photolithography I could send your way if you're interested.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yeah definitely, I can PM my email?",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Can i get it too?,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","FYI, I'm a computer engineer. You probably won't get too much about computer engineering from a CE degree/research. Computer engineering is far more about digital logic than it is about manufacturing and lithography. For that, you want to look at material sciences.Computer Engineering is like Electrical Engineering Lite :D",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Good thing Intel has ""Optane"". That's gotta keep them afloat /s",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Ok made me sell my intc position for a small profit when I bought the dip. I am easily swayed.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I agree with AxeLond's comment that it's more likely management issues and that the details you mentioned are mere symptoms of the problem.I would also add that you're a little too detail focused and forgetting about big picture of cpus. The perfect cpu is one that can handle workload quickly, uses very little power, takes up very little space, and costs very little. The technology that's used to get there will be a combination of techniques. There is no one solution that will solve this problem and customers don't care how the problem was solved.The technical issues you're discussing places too much emphasis on the ""tick"" to solve the problem of handling workload quickly with lower power. A fixation on only one key aspect of cpu design occurred in the early 2000s when everyone promoted the idea that the cpu with the highest clock rate would be the fastest cpu. People were predicting cpus would be 10-20 gigahertz in the future. Unfortunately they hit a wall around 4gigahertz (i forget exactly where) and the increase in clock no longer correlated with decrease in processing time. What changed the mentality of the whole industry was a new cpu architecture that was significantly faster and used less power. We are on like the 10th or something generation of that initial iteration.Anyway, I'm just pointing out a brief example that you're a little too focused on lithography. There are a ton of many other factors like the fact that cpus are so fast now that people can use 10 year old pcs and still get their basic computing needs accomplished so there is a diminishing rate of return for producing the the fastest cpu on the market. There is also the decline of the consumer PC market as people start to utilize mobile devices. The upside is that it draws in new demand for server CPUs with higher profit margins but they lose out on a lot of volume. The list goes on and on and each of these things play a factor in Intel's stock price.Ultimately, I think it's a management issue that many former great giants in industry are suffering through.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Considering that INTC is still hugely profitable with strong cash flow, the company shouldn't lack resources in competing with the ARM camp. Its delay in process technology is clearly more of a problem of management than technology.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",So... when is nvidia gonna buy intel? :),0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I don't think anything like that would happen realistically.If any other chip company that tries to buy intel would just crumble apart trying to integrate with such an engrained monolithic company.It'd be just like every time somebody tries to aquire Cray.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Good stuff, sold out before earning when it was around 63/64. Didn't see where they would go in the mid term. Been trying to decide between putting cash into ASML or LRCX.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Thank you for the very insightful and informative post!,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Thank you for a very well-written post.I’m currently holding TSMC, INTC, and AMD. I’m looking to add ASML and AMAT.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","ASML will be huge, their US competitors are getting screwed over right now because of the White Houses decision to implement a ban on use of American built semiconductor Fab technology outside of America.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",ASML is virtually a monopoly now - it has no competition in EUV and little competition in DUV.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",So how does ASML picture into this?,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Great explanation, thank you",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Thank you so much! I love you.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Wonderfull thread, thanks to all the contributors!",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","At the start you coin die shrink as ""tick"" and microarchitecture as ""tock"". But then you seem to abandon it, calling 10 nm a ""tock"" and saying the incomplete Skylake architecture is a ""tick"".",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","In terms of stock price, I heard this same doom and gloom before, but patiently rode INTC from $40 nearly up to my target of $70. (It hit $68+ before the current slump)Stock price and making money aren't always about technical mastery or having the hot hand of the day. Sometimes it's about being big and bad and lumbering and having huge margins, even if volume slips a bit. Supposedly Intel still has dominance in their most lucrative area like data center plus they have good roadmap for mobile and AI. That said I'm not blind so that's why I had about 4:1 AMD:INTC (before taking profit on AMD in this run up)",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",aaaand this is why i dont invest in tech lol. great write up!,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","This is an industry I actually understand and this still has my head spinning. I figured, like most, Intel was due for a dive when AMD started catching up, but this whole thread has me realizing I probably underestimated how bad it might get.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I work in the industry and honestly couldnt get through your whole analysis but I will say this(sorry if you mentioned it). Intel is still way way ahead making their own chips....why? Because AMD though a superior product right now will not be able to get the fab capacity to put much of a dent in intel's marketshare, as the same fabs that make AMD's chips, make Apple's and Nvidia's, 2 arguably larger companies....intel has no such problem making their own ...for the next year with everyone staying home(plugged in to an outlet) who ever can make chips in volume wins. AMDs advantage in power and efficiency doesn't matter much when folks are plugged in....And in a year they will be able to use all that profit to get to 5-7nm working. This cycle has been typical for intel, as why spend more money if you dont have to. Let others spend the R&D getting 5-7nm working, buy the tech from the equipment manufacturers when it's ironed out and cheaper....then dominate the market without the overhead AMD has to pay TSMC....i've been slowly swapping my AMD stocks i bought several years ago for INTC since the drop, ez money in a couple years...",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","BTW, TSMC is building 3 more 7nm/5nm fabs right now(2021 online), and 2 more under planning(2022/23). Once those in 2021 completed, they will start converting fabs for 3nm or below.Also, same problem within Intel applied. They won’t be able to covert all their fabs to do 10nm and 7nm, because they got other BUs to run (AI, logics, older 14nm, 20nm products).Capacity will be an issue for Intel if they are not expending their fabs.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Holy long,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Nah, Holy short intel",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Holy long post*,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Intel had the money to pursue both strategies in parallel, so why did they bet it all on one?",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",My head exploded reading this. I feel so dumb.,0,nan,nan,nan
