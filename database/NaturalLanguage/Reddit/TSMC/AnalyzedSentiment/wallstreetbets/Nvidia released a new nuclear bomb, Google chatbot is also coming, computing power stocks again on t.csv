date of comment,main comment,comment,depth,PTR Sentiment,Flair Outlook,Flair Sentiment
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",GPT-REEEE,0,0.5,NEGATIVE,0.999
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.","Yep, Nvidia is getting close to all time high, looks like good time to buyClasic WSB",0,0.537,POSITIVE,0.987
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",Ath PE right now lol,1,0.505,POSITIVE,0.992
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",just copy and pasted this from somewhere. no one will read it anyway,0,0.512,NEGATIVE,0.999
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.","It's an endless series of proxy battles, fought by AI and machines. War, and its consumption of life, has become a well-oiled meme.",1,0.536,POSITIVE,0.947
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",Nice to see an MGS4 reference.,2,0.501,POSITIVE,0.998
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.","AI...yadda, yadda....BS.",0,0.566,POSITIVE,0.788
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",The extra GPU demand created by Chat GPT is negligible. Estimates are only 30K card over a number of years. That is a trivial $300M if they buy the top of the range H100.,0,0.524,NEGATIVE,0.999
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",So how does this help me make money. That’s the tldr I’m missing,0,0.51,POSITIVE,0.966
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.","I've gotta hand it to Nvidia, they seem willing to pivot and ride the wave of whatever fad/movement will sell their products...",0,0.526,POSITIVE,0.933
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.","Hey, do you have a source? Please add a link in the post or as a reply to this comment!I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.",0,0.504,POSITIVE,0.793
,"On March 21, arithmetic stocks (data centers, AI chips, CPO, etc.) were again sharply stronger. In news, overnight Nvidia released a dedicated GPU for ChatGPT, claiming to increase inference speed by 10x. Also in recent days, ChatGPT users around the world have been finding error warnings popping up on the site due to a large increase in access, and even privileged Plus accounts have not been spared. The latest research report of Anxin Securities has made an inference on GPT-4 arithmetic power demand: GPT-4 has increased its inferred arithmetic power demand to more than ten times due to the increase in complexity and the addition of image recognition function. NVIDIA Releases ChatGPT Dedicated GPU, Increases Inference Speed by 10 Times On March 21, Beijing time, Nvidia CEO Jen-Hsun Huang announced the inference GPU (graphics processor) H100 NVL designed for ChatGPT at the 2023 GTC developer conference, which stitches together two of Nvidia's H100 GPUs to deploy large language models (LLMs) like ChatGPT, according to Punch News. The only current GPU that can actually handle ChatGPT is the NVIDIA HGX A100, and compared to the former A100, a standard server with four pairs of H100s and dual NVLINKs can now be 10 times faster, reducing the cost of processing large language models by an order of magnitude, said Jen-Hsun Huang. In addition to custom chips, NVIDIA also released cuLitho, a breakthrough lithography computational library. NVIDIA announced that after years of collaboration with TSMC, ASML, Synopsys and other companies, it has launched a new computational lithography application that greatly reduces the time and energy consumed by chip foundries in this process, ready for the arrival of 2nm and more advanced processes. In addition, NVIDIA has launched DGX Cloud, an AI supercomputing service, NVIDIA AI Foundations, a cloud service that accelerates enterprises to create big models and generative AI, and the world's first GPU-accelerated quantum computing system in partnership with Quantum Machines. Jen-Hsun Huang said that the AI industry is in an ""iPhone moment"" - startups are racing to build disruptive business models, and industry giants are looking for ways to respond, and are making bold statements - ""NVIDIA is about AI. -Nvidia is to be the TSMC of AI! On the domestic front, Jen-Hsun Huang said that in China, it has special customized Ampere and Hopper chips, these will be provided through Chinese cloud providers, such as Alibaba, Tencent, Baidu, these companies to provide the ability to land, fully believe that they have the ability to provide top-notch system services, for Chinese startups will certainly have the opportunity to develop their own large language models. Google chatbots are also coming, AI big models lead the arithmetic layer for long-term benefits On Tuesday, March 21, US time, US tech giant Google Inc. launched a beta version of its AI chatbot Bard in a bid to compete with OpenAI's ChatGPT. Bard, which will run separately from its Google search engine, currently generates answers in English only and currently offers access to users on a waitlist on a first-come, first-served basis. Google says over time, Bard will be adapted to more languages as well as rolled out to more regions. Axiom Securities pointed out that ChatGPT, GPT4.0, Microsoft 365 Copilot, Wenxin Yiyin and Google Bird were released one after another, and the AI big model represented by ChatGPT and its initial application ""stirred up a thousand layers of waves"". It said that with the release of multimodal large model GPT-4, the pace of applications based on text, pictures and other vertical scenarios is expected to accelerate from 1 to 10, similar to the blossoming of various types of APPs in the mobile Internet era, and the competition pattern will gradually intensify. And analogous to the late 19th century ""gold rush"" in the western United States, a large demand for shovels, jeans, it believes that the GPGPU as the representative of the arithmetic infrastructure as a large model of AI base will benefit long-term stability. GPT4 arithmetic power demand increased to more than ten times The latest research report of Anxin Securities made an inference on GPT-4 arithmetic demand and future trends: GPT-4's speculative arithmetic demand increased to more than ten times due to the increase of complexity and the addition of picture recognition function. It said that OpenAI CEO Sam Altman pointed out in a public interview that GTP-4 has 20 times more parameters than GTP-3 and requires 10 times more computation than GTP-3; GTP-5 is released at the end of 2024 to 2025, which has 100 times more parameters than GTP-3 and requires 200-400 times more computation than GTP-3. The bottleneck of arithmetic power is not in the absolute size of the arithmetic power, but in the cost of achieving that arithmetic power. From the perspective of cost, Guoxin Securities has also made a calculation on this, which according to the public data of GPT-4, the cost of asking a question per 1000 tokens is $0.03 and the cost of completing an answer per 1000 tokens is $0.06 under the context length of 8K; under the context length of 32K context length, the question cost per 1000 tokens is $0.06, and the answer completion cost per 1000 tokens is $0.12. It says that this arithmetic cost rises higher (50%-200% increase in input cost and 200%-500% increase in output cost) compared to the cost of GPT-3 (about $0.02 per 1000 tokens), and is even more significant compared to the cost of GPT-3.5-turbo, with a 14-29 times increase in input cost and a 29-59 times increase in output cost. The increase is 29-59 times. In addition, the expansion of the AI server market has simultaneously driven up the demand for high-speed NICs, HBM, DRAM, NAND, PCBs, etc. At the same time, the technology around the solution of GPU ""power wall, memory wall"" in the large computing power scenario continues to upgrade, such as storage and computing integration, silicon light / CPO industrialization process is expected to speed up; advanced process chip evolution in the existing Chiplet and other technical paths will also benefit; Risk-V due to open source and free, developer freedom, high degree of autonomy, high control, and the risk of the development of the chip. The advantages of Risk-V, such as free open source, high developer freedom, high degree of independent control, and more adaptable to AIoT processor architecture requirements, will drive the number of participating companies around AI scenarios.",NVIDIA are going to drop PR bombs like this every week or so until they've destroyed every short on the planet.,0,0.523,NEGATIVE,0.996
