date of comment,main comment,comment,depth,PTR Sentiment (Comment),Flair Outlook,Flair Score
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Putting technical issues aside. Lets look at the geopolitical scene for a while.Intel is ""too big to fail"" by that i mean it is actively important to US National Security. It owns the only fabs in the US currently able to produce ""cutting edge"" microprocessors.Both China and Russia are actively working to develop their own in country silicon supply chain.The next generations of hardware the military needs to stay competitive will rely on micro architectures and process nodes being developed today.Import restrictions on outside silicon could make the simple fact that intel controls their own domestic fabs very, very valuable in the not too distant future.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I'm not sure i buy the part about TSMC Taiwan isn't a big fan of china.Currently Taiwan actually views its semiconductor industry as a shield against Chinese ""incorporation"" due to supplying a significant portion of the western world with silicon. Much like Hong Kong Taiwan has no love for the mainland and is not eager to cede it's sovereignty to china.This is actually becoming a very big point of tension in US-China Relations, and probably a big part of why Russia is also interested in homegrown semiconductors",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I agree with you, I'm not trying to throw shade at Taiwan at all, but the sad fact is that China has an active spying ring specifically focused on industrial espionage of Taiwanese semiconductor secrets.Taiwan can try as much as it wants to ward off Chinese spying, but it's a losing game for them.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Okay but being spied on by China and spying for China are vastly different. You sound like you’re talking out your ass.,4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Global Foundriesits was GF that refused to move beyond 12nm because it was to expensive,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yea, they are honestly languishing.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Global Foundries is never going to compete against tsmc at this point for large fabless companies like apple and nvidia because its technology is far too behind. It takes years and years of planning for a new fab and they're like 2-3 nodes behind tsmc.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",But TSM is also going to manufacture within US borders? Not quite sure you’re factoring this in about TSM.https://www.google.com/amp/s/www.fool.com/amp/investing/2020/05/17/taiwan-semiconductor-is-expanding-manufacturing-in.aspx,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yes that's true, but that plant isn't supposed to be running until 2024 with full completion by 2029, and It will still be owned by a Taiwanese company. I just wouldn't count the chickens before they hatch.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Most of the big investors into TSMC are western. TSMC was literally founded by American investors.,4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I simplified somewhat in the post to keep it from being too long.Everyone does indeed have self-aligned multi-patterning. Some even have SAQP (e.g. TSMC N7). The difference is that their SAQP is limited to FEOL; while Intel is using SAQP for FEOL and BEOL, where competitors are using SADP or EUV. Their pitch width targets were far too ambitious and they couldn't achieve it.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I simplified somewhat in the post to keep it from being too long.Everyone does indeed have self-aligned multi-patterning. Some even have SAQP (e.g. TSMC N7). The difference is that their SAQP is limited to FEOL; while Intel is using SAQP for FEOL and BEOL, where competitors are using SADP or EUV. Their pitch width targets were far too ambitious and they couldn't achieve it.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Actual in-depth analysis. Good job and keep up the hard work.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yes, but he shouldn't have posted it here to punish r/investing mods. They let you write posts for hours only to tell you afterwards: ""No. Deleted."" - This nazi style moderation needs to be punished by not supplying high quality content.Note: If I won't be able to reply anymore, this might be due to the mods censoring/punishing me for speaking out here.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","While the mods are far from perfect, they aren’t doing too bad either. They aren’t getting paid, if you don’t like it go somewhere else. r/stocks is more lightly moderated and it shows in the average lack of quality of content.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Not sure if you've noticed, but this sub is virtually dead. It used to be quite active and full of life, but the poor quality mods here have moderated this place to death. 100 individual threads in 3 days? This sub used to have way more activity than that. Some balance is needed.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","He said it's to punish the mods, not to improve the sub. He doesn't talk about improving the sub anywhere.I'm just a visitor here, I know nothing about the mods and have a neutral opinion of this sub, just pointing out that you've jumped to some conclusions here and are arguing a straw man.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","He said it's to punish the mods, not to improve the sub. He doesn't talk about improving the sub anywhere.I'm just a visitor here, I know nothing about the mods and have a neutral opinion of this sub, just pointing out that you've jumped to some conclusions here and are arguing a straw man.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Did this happen to you???,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I read the whole thing and didn't find the promised ""reasons"" just a bunch of technically misguided assertions.First EUV is a form of patterning. Pitting it against ""patterning"" doesn't make any sense. That may seem like semantics but it shows how much you don't know what you are talking about.nProbably copy pasted from some other blog.Second, Intel didn't choose EUV for their 10nm because... EUV technology wasn't ready for the planned ramp of their process. Granted, if they had known how much delay there would eventually be, they might have felt differently about that decision in he 1st place. But they didn't have a cristal ball and it was a sensible choice at the time.Third, the SAQP process is very stable and performs very well. Better than EUV by all accounts. As far as we know their 10nm yield issues have nothing to do with patterning.The benefit that EUV has over SAQP is cost. Not defectivity. Not performance by and large. It's cost (and manufacturing throughput).Your claim that using multiple patterning multiplies the defectivity is just wrong. Contemplate that EUV multi-patterning is already being used to reduce defectivityMoving on to GAAfet. It's not actually a ""different beast"" it's fairly evolutionary. All foundries, and I mean all, are contemplating using GAAfet in the future and they all have a finfet as plan B. Intel is no different than Samsung or tsmc in that respect.Did intel not successfully roll out the 1st finfet process in the industry ? If history is any guide here, what we should expect is that Intel might be the 1st to roll out GAA.Something people forget, is that Intel has optionality here. If they are successful with their 7nm, they will have an edge over the competition. It they aren't, they can fall back to tsmc and use the same technology as competitors. They are doing the right thing by opening this door.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",That explains why every instance of its' is used incorrectly. It's like he did a Replace All instead of Replace One.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",That explains why every instance of its' is used incorrectly. It's like he did a Replace All instead of Replace One.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Thanks for the comment, it was very insightful but I have a few quibbles...Perhaps I wasn't clear enough when I talked about patterning (woes of trying to make a complicated topic accessible). When I talk about patterning, I'm referring to multiple exposures of a wafer to lithography, while EUV is the actual machine doing the lithography. They get to the same result (or are supposed to), but one gets there through improving the actual equipment while the other gets there through improving the technique of using the equipment.it was a sensible choice at the time.Absolutely, and that's something I didn't put together for some reason. I mainly focused on the fact that they were expecting to be able to use 4 cores indefinitely, making high yields a luxury rather than a necessity. Thanks for pointing it out!As far as we know their 10nm yield issues have nothing to do with patterning.This is incorrect; Intel themselves stated that patterning was the reason to for the delay in the Q1'18 earnings call. I simplified again here - I believe that it is specifically the SAQP process used on BEOL that is creating these problems for Intel's yield. SAQP used on FEOL is a mature technology - indeed TSMC N7 uses SAQP for FEOL. If you have a source for BEOL SAQP performing better than EUV in terms of yield I'd be very interested.The benefit that EUV has over SAQP is cost. Not defectivity. Not performance by and large. It's cost (and manufacturing throughput)I'm curious as to why you think this. It's pretty obvious from your comment that you understand SAQP has a higher number of steps in the lithographic process to actually finish the wafer. Generally, the more steps you have, and the more you mess with the wafer the more you get defects. That's a pretty fundamental law of semiconductor design.Intel is no different than Samsung or tsmc in that respect.Agreed, though I'm not so convinced on the evolutionary aspect.If history is any guide here, what we should expect is that Intel might be the 1st to roll out GAA.If history was any guide, Intel should also have been leading the process race by a generation as it always did, instead of struggling.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Perhaps I wasn't clear enough when I talked about patterning (woes of trying to make a complicated topic accessible). When I talk about patterning, I'm referring to multiple exposures of a wafer to lithography, while EUV is the actual machine doing the lithography. They get to the same result (or are supposed to), but one gets there through improving the actual equipment while the other gets there through improving the technique of using the equipment.Gotcha - this is not quite so and I'd like to clarify. Patterning is the process of defining a shape on the target wafer. EUV is one litho process - the new kid on the block - which uses a wavelength of 13.5nm and has a max resolution of ~13nm. As such it can pattern 26nm pitch features in a single exposure (in pratice, it is limited to ~34-36nm for complicated reasons I won't go into)The incumbent is 193nm immersion lithography, which uses ArF laser @ 193nm and has a resolution of ~40nm. As such, it can pattern 80nm pitch features in a single exposure.Multiple patterning is a set of techniques to improve the resolution by using, for example, sidewall image transfer which is what SAQP refers to. SAQP use 2 pitch-halving steps to reduce the minimum pitch of 193nm litho from 80nm to 20nm theoretically.With this out of the way:This is incorrect; Intel themselves stated that patterning was the reason to for the delay in the Q1'18 earnings call. I simplified again here - I believe that it is specifically the SAQP process used on BEOL that is creating these problems for Intel's yield. SAQP used on FEOL is a mature technology - indeed TSMC N7 uses SAQP for FEOL. If you have a source for BEOL SAQP performing better than EUV in terms of yield I'd be very interested.You've got somewhat of a point there - in hindsight many analysts did interpret it this way. What intel did say was more along the lines of ""limits of lithography, yada-yada"" without explicitly pointing out the problems and analysts assumed SAQP was the problem because it is the smallest pitch in the process (36nm pitch for M1) - and because SAQP is viewed as somewhat a unique choice of intel where other foundries are using triple exposure or SADP with pitch limited to 40nm.However - which is pushing the limits of lithography hardest? SADP at 44nm pitch (theoretical limit is 40nm) or SAQP at 36nm pitch (theoretical limit is 20nm). The first. Furthermore, as you pointed out SAxP is used for several generations in the FEOL. The litho process is very, very well known. It seems unlikely that lithographers would have overlooked an issue so big they couldn't fix it for years. The type of issue we are looking for is something new - something that couldn't have been predicted (should have been de-risked but that's another story). People have pointed to Cobalt metallization, or ruthenium vias - these seems more likely candidates, as being new materials it's quite likely to have unexpected issues. Anyhow... lots of speculations and I for one do think intel is just throwing a bone and not being honest about their real issues (why would they). It's really anyone's guess.Why SAQP does not have higher defectivity ? Yes it has more process steps - and each one contributes defectivity, that is true. But each of these steps has very low defectivity, resulting in the combined defectivity being lower.Have a look at Figure 28 in this paper comparing the roughness (LWR) of EUV single print vs SAQP. There is one order of magnitude difference.Here is a recent blog discussing EUV challenges.Don't get me wrong - EUV is a fantastic tech. But it has serious challenges.As to why you don't get performance improvement out of EUV - it's just another way to print the same thing on the wafer (you can print it faster / cheaper - although as pointed out by another redditor currently the cost is not that great) but ultimately you get the same shape on the wafer.Agreed, though I'm not so convinced on the evolutionary aspect.Many challenges, for sure. But it's essentially a better finFET.If history was any guide, Intel should also have been leading the process race by a generation as it always did, instead of struggling.At 10nm Intel was the first to introduce Cobalt and Ruthenium in the manufacturing process. First to introduce contact-over-active-gate (COAG) a revolutionary feature. Intel has the smallest metal pitch, gate pitch and fin aspect ratio in the industry. And all this was scheduled to be released with a comfortable time lead.Granted - they screwed up. Big time. Fucked up the integration, the risk management, the PR - a lot of things. But many of their process modules are way ahead of the rest.Now I don't know if they will succeed at 7nm or repeat the 10nm blunder. Who knows. But do they have a chance ? Absolutely.One last thought : the whole process technology lead is overrated anyway. Moore's law is already dead. You don't get cost improvement from node to node anymore. You barely get any performance and you get what - a tiny bit of power ?Where is the future? 3D packaging. And then 3DIC.Cheers",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",EUV is more expensive and lower throughput than SAQP. However it has faster turnaround times. Bandwidth vs latency.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Multiple patterning does have defect related issues, it can make the process integration more challenging, and it also takes more steps to do the same patterning which by itself adds defects to the process. Of course it self aligns which makes it simpler in other ways.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I read the whole thing and didn't find the promised ""reasons"" just a bunch of technically misguided assertions.Yeah zero evidence. Just a bunch of dots connected by luck and hope and guessing.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Material scientist PhD here. Lithography is a top down approach which is highly susceptible to defects at 10s of nm lengthscales. A completely new approach is required to build features of this length with high accuracy and precision. In science, the most cutting edge techniques that achieve this, is by bottom up self assembly which is very very difficult to scale up to production volumes.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I'd be interested to learn about who has deep R&D money in EBL. Its very likely the best option for going smaller but getting the process manufacturing ready is surely gonna be a bitch,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Precisely. Defects in fabrication via EBL are still quite common and the technology has a long way to go before it can be suitable for manufacturing scale.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",It's safe to say that some point we will hit some physical limit right? From my (very basic) understanding we are already running into quantum tunneling issues. How likely is it that we will hit a hard physical limitation in the next few generations of microprocessors that clever engineering cant overcome? Will we see more and more of these process hangups from the other players in the coming decade?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","There's also a great deal to be said for their anti-consumer practices as of late. Just one example: Intel has decided to limit memory speed on mainstream motherboard chipsets. But the memory controller is on the CPU, not the motherboard. And there's no technical reason to do this other than to require those people who are looking to overclock being forced to purchase a more expensive motherboard.AMD is not engaging in such practices, and this is another area where Intel is quite literally chasing away the customers who have the potential to be their most vocal advocates (or opponents) in non-commercial applications.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","These ""deep dives"" never mention lam or applied. It's pretty telling when they think asml is the only company involved in chip manufacturing",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yep, all EUV machines are bought from ASML, a great company in their own right (though a bit expensive for my taste; they haven't demonstrated an ability to increase supply suitably to respond to the incredible demand present for their machines)",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yep, all EUV machines are bought from ASML, a great company in their own right (though a bit expensive for my taste; they haven't demonstrated an ability to increase supply suitably to respond to the incredible demand present for their machines)",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","For what it’s worth Motley Fool stock advisor is super high on ASML. It’s basically a Monopoly that most people haven’t heard of. EUV is the future, buy buy buy.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",EUV won't have shit on EBL once someone figures out how to get it going on a manufacturing scale.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Maybe but it took more than a decade for ASML to develop EUV and have it ready for market. They have at least a decade head start, plus they may already be working on EBL for all I know.",4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Can someone give a second opinion on this comment please?,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Can someone give a second opinion on this comment please?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",ARM processors has thus far been unable to emulate x86. look at the number of desktop-based applications that Surface pro X has been able to run relative to AMD/intel.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I didn't touch on this in the article, but my view is that their edge computing is largely subsidised by x86. The future might be bright for those segments but if Intel's profits reduce due to their process problems, they'll have to cut back on spending for those segments.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I didn't touch on this in the article, but my view is that their edge computing is largely subsidised by x86. The future might be bright for those segments but if Intel's profits reduce due to their process problems, they'll have to cut back on spending for those segments.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",They had couple of embedded launches over the last decade - both failed. They cannot make their mind up and come up with the strategy they are willing to push for some time without receiving profits.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",They had couple of embedded launches over the last decade - both failed. They cannot make their mind up and come up with the strategy they are willing to push for some time without receiving profits.,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",They had couple of embedded launches over the last decade - both failed. They cannot make their mind up and come up with the strategy they are willing to push for some time without receiving profits.,3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Please describe what inroads Intel has made into IOT compared to companies like Espressif, Nordic, Mediatek, or even traditional MCU companies like STM or NXP? Hell, even the x86 ISA is less common in IOT than risc-v.I have not seen any inroads from Intel into IOT that are worth much. IOT is an absurdly cost sensitive area, something Intel is very not experienced and seemingly comfortable with.I have not seen a single Intel based device in IOT over the past few years. As very beefy gateways then maybe a handful at most. All smart plugs, sensors, etc, have been MCU's, something for which Intel has no reach and products worth a damn (as far as I know).",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","All their products are tied to their manufacturing process technology. IoT, FPGA's, SoC's,.. everything. They fail in it, they fail in everything.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Your submission has been automatically removed because the URL matches one on the r/Investing banlist due to low quality content. If you believe the article you are trying to link is high quality content please message the moderators with a short message so that we may approve your submission. Please be aware that if your post can be sourced from a less sensationalist publication we will likely require you to do that. Thank you.I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Your submission has been automatically removed because the URL matches one on the r/Investing banlist due to low quality content. If you believe the article you are trying to link is high quality content please message the moderators with a short message so that we may approve your submission. Please be aware that if your post can be sourced from a less sensationalist publication we will likely require you to do that. Thank you.I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","In simple terms, by a computer geek, they only have a slight performance advantage, besides that, they get hotter faster, louder, shit coolers, can't keep up in livestreaming and got soft, my advice, get into them before the roles reverse in ~3 years when amd goes under, they go in cycles and now it's AMD's turn",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",What are EUV and SAQP,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Intel is moving in multiple directions to address these issues. Chiplets are a strategy to reduce the size of the physical cpu chip by focusing only the critical parts of the cpu being the highest fabrication. Other parts can be fab'd at lower density as its cheaper and there's not a need for speed. This will increase yield at higher fabrication as the size of each cpu entity will be smaller. Those entities are the cpu cores and the smaller and simpler those are the easier they are to fab at highest density. The chiplet concept stitches all these things together to make the physical cpu chip cheaper to produce. This strategy is complemented by what's called hybrid cpus where you have high speed, high watt consumption cores, and lower speed, lower watt consuming cores for efficiency. These, again, allow you to design more complicated cpus following the chiplet model. This chiplet model is what Apple will be utilizing.Intel may appear down and out but internally they see AMD has a target on their back and know what they need to do next to focus their energies. It's harder to stay in the lead than it is to play catch up. Intel is the champion of the latter.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",You realize that AMD has been doing the chiplet design since ryzen?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","The arrival of gaming on Linux isn't helping Intel either, AMD works out of the box.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Wonderful read. Not my area of expertise so I don't really invest into semiconductor stuff but this was still fascinating to read,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","For some context, the Covid-19 virus is 100nm in size, so those transistors are about 10% the size of the virus, or the size of one of the sugar spike receptors the virus has to avoid antibodies.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","I think Intel can recover really well thanks to one word: volume. Intel literally dumps CPUs like no one else with all their fabs, hence their record high profits. It doesn't matter if they're slightly worse than AMD for cost efficiency or they're technologically behind. Enterprise needs CPUs and they will buy as many as they can. The shortage is so bad that they're looking at ARM, making their own CPUs, etc. As long as they can make the node leap ""soon"" enough, their volume moat is pretty strong to prop them up for a few more years. Then, Intel can finally upgrade all their fabs like they did with all their past nodes.Don't matter if there's none of the good toilet paper, you're using single-ply, tissue paper, or your hand to wipe during the Great Toilet Paper Shortage of 2020.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years agoThis just isn't true. Dennard scaling ending in like 2004 and Intel has been introducing new tricks since then to keep it moving. Replacement source drains, Hi K metal gate, gate last, these are all other major changes.I used to work at Intels TD fab and I can tell you the challenges are very hard and technical, but not the reason Intel is lagging competition now. There is a leadership problem there",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Stupid question but is there a reason we need to keep the CPU the same size? Is it possible to make a CPU the size of an iPhone and reach 8-9 GHZ? I'd take that. It's in a case, I don't really care if it's the size of a tic tac box or a phone, as long as we're not getting over Full ATX tower sizing.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Theoretically, it would be possible (search up Nvidia A100). On a mass-market scale though you'd need chiplet architecture for sure, or it'd be impossible to produce economically due to yields.However, in reality the consensus so far seems to be to expand vertically rather than horizontally, I suspect in large part due to the inherent latency of having such a big chip - if you want to send a signal from one end to another, it'd take a lot longer than if you sent it 'upstairs'.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",wrong sub... thought i was on wsb. sorry about that,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",wrong sub... thought i was on wsb. sorry about that,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",wrong sub... thought i was on wsb. sorry about that,2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Thanks for your DD! I love this kind of content.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I really enjoyed this read - thanks for putting the work into this,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Hi, welcome to r/investing. Please note that as a topic focused subreddit we have higher posting standards than much of Reddit:1) Please direct all advice requests and beginner questions to the stickied daily threads. This includes beginner questions and portfolio help.2) Important: We have strict political posting guidelines (described here and here). Violations will result in a minimum 30 and likely 60 day ban upon first instance.3) This is an open forum but we expect you to conduct yourself like an adult. Disagree, argue, criticize, but no personal attacks.--I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Good work, tks!",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I would be interested in any analysis of any deeper issues involved here. Why are they committing to strategies that don't work?I got out of Intel when they started 'woke' virtue signaling a few years back. https://www.reddit.com/r/ThisIsNotASafeSpace/comments/3zwmsm/intel_went_full_sjw_in_the_end_of_ces_2016/,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",More here https://www.gamespot.com/articles/intel-spending-300-million-to-bolster-women-and-mi/1100-6424509/,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","It's nice to see AMD doing better in recent years, but Intel is still the premium chip that provides a far more supported and stable end user experience.That's why Intel has the vast majority of chip business. AMD is picking up some extra these days, but pretending Intel is anything but the lead horse in the race is misguided.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","> It's nice to see AMD doing better in recent years, but Intel is still the premium chip that provides a far more supported and stable end user experience.this is not true in either the enterprise or consumer world.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Intel only provides better fps in gaming at the cost of half a dozen if not more security issues.Intels TCO is also significantly higher.Intels ecological implications (Massive Dies on 14nm which use a lot of power and produce a lot of heat) are also nothing to be proud about.,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Thank you!!,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",And then there is me who just bought the dip.,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Wow this is amazing, I was currently putting in some work to determine whether or not to place an investment and this has added some interesting information regarding the 10nm, in other words thanks for the assist in saving some dough",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","you hit all the point here, great DD. I expect intel to slowly decline till 2023 or late 2022, untill then AMD is king.",0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","My AMD shares with a $6 cost basis are doing quite well, thank you.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Not to mention OP's account is 2 minutes old and spammed every sub with his ""dd"". His only other contribution is discussing Hertz post-bankruptcy.What a joke.",1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","2 months, not 2 minutes",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",,0,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",Wut... I... do you say things to yourself before you type them out?,1,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","Yes. That is actually how Intel became a leader. The US sanctioned Japan and threatened them into handing over their technology to US firms. This was in the 1980's, when the Japan was light years ahead of the US.",2,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET","So Trump, in the 1980s, did all this?If I remember correctly he got elected in 2016.",3,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I am going to assume that you are just joking.,4,nan,nan,nan
,"Since its’ Q2 earnings call a few weeks ago, Intel Corporation (INTC) shares have plummeted 20% upon announcement of problems with its’ next-generation 10nm and 7nm manufacturing processes. The massive collapse has led to widespread attention among investors, but in reality the situation has been years in the making for those who’ve been paying attention. Today I’d like to look at some of the technical decisions Intel made, why they’ve caused problems and the implications of that on their future. Lithography techniques Lithography is an incredibly complicated process that forms an incredible competitive advantage for those who master it. In simple terms, you put a template of circuit designs (photomask) on a silicon base (wafer) and shine a powerful laser on it [1]. Over time, people tried to fit more transistors in the same area – this would lead to increased performance capability, lower power consumption and various other benefits outlined in Dennard Scaling[2]. This becomes progressively more difficult over time, as you’re trying to cram transistors into areas thousands of times smaller than the width of a hair. The industry ran into a particularly tricky wall around the 20nm mark, since the size of the laser you used to ‘print’ the circuit design became so relatively big that it couldn’t reliably follow the complicated patterns needed for all the transistors. Two schools of thought developed to address this problem – patterning (using more than one photomask, each with simpler diagrams, and lasering the wafer with each of these templates separately), and EUV (extreme ultra-violet, using radiation with much smaller wavelengths than traditional). Intel saw success with dual-patterning (two templates) on its’ 22 and 14nm process, and chose to go one step further and pursue quad-patterning on its’ 10nm process.[3] Meanwhile, its’ competitors TSMC and Samsung chose EUV. [4] For reference, Intel themselves have also chosen to pursue EUV for their 7nm process. That might give you a hint as to which was the right choice… Other terminology I’ll be referring to in this piece are yield (how much of a wafer is actually useable) and monolithic (the whole CPU is cut out of the wafer as a single piece of silicon) vs chiplets (the CPU is formed from several pieces of silicon stuck together) The problems with 10nm Back in 2013, Intel was in it’s prime. It dominated the CPU market with >90% market share, and was pursuing a tick-tock strategy with its’ chips – every two years you would have a die shrink ‘tick’, then the alternating years you would have a microarchitecture change ‘tock’. In the roadmaps released by Intel, they planned to have their next ‘tock’ of 10nm in 2016. The ‘tick’ – Skylake architecture came, but the ‘tock’ never did. Even today, 4 years after it was supposed to be released, 10nm still isn’t really here. On paper, it was launched with Cannon Lake in 2018 – but the total number of those are in the thousands, if not hundreds. On paper, the ‘mass-market’ generation Ice Lake launched in 2020 but they have incredibly limited supply and offer inferior performance to Intel’s own 14nm offerings. [5] The latest update is that desktop and datacentre chips will come in the second half of 2021 – but for reasons we shall soon see it is my opinion that these will yet again be flops. In fact, it is my opinion that 10nm is a total writeoff, and that the design decisions taken at a very early stage have doomed it to failure. When you use lithographic techniques, you are bound to have some defects in your wafer. After all, creating billions of devices tens of atoms in size isn’t going to be perfect. Patterning as a lithographic technique inherently has a higher defect rate than not using it – you’re basically going through the same process multiple times, thus increasing the chance of defect dramatically. As I mentioned earlier, Intel is using quad patterning in 10nm – this means their defect rates are going to be sky high. At the same time, their usage of a monolithic die compounds this problem for high-performance, high core count CPU models. As you can see from the blue wafer below, it’s difficult to draw large squares (high-core count models) that are without defect. In comparison, the red wafer is AMD’s chiplet approach, built on TSMC’s less defect-prone EUV process. (Sorry, I copied this post from my blog to not self-promote but I can't insert the relevant pictures here) Since you can paste together multiple small CPUs into one bigger one, you use a far greater percentage of the wafer, cutting costs and letting you freely choose however many high-performance chips you want to build. Of course, it’s impossible for anyone outside Intel to know the exact numbers for the defect rates, yields and unit costs for 10nm. No doubt they are improving as time goes on,as they always do with a maturing architecture. However, I can say with certainty that they are currently not yielding at rates that could let them release high core-count server chips in any volume, EVEN AT A LOSS The margins on 10nm will NEVER reach the heights that Intel has traditionally seen. Intel has enjoyed gross margins of above 60% for the last decade. In my opinion, if Intel were to replace their whole product stack with 10nm, their gross margin will never rise above 30%. The maximum price they can release their products at is capped not only by AMD’s offerings, but more importantly their own legacy performance. If Intel attempted to price at a level that would give them healthy margins, their entire product lineup would be outcompeted by their 5 year old 14nm chips on a price/performance basis, and their customers would have no reason to upgrade, decimating their revenues. These are bold statements but I believe Intel’s actions over the past few years, and their planned actions over the next few, support this view. When you release a new generation of processors, you always want to have it be ‘better’ than the previous generation. This may seem incredibly obvious, but the only exception is when the design has such big inherent flaws that you can’t physically do so. For instance, the Bulldozer architecture AMD released in 2011 performed worse than their own previous-generation Phenom II architecture [6], leading to near-bankruptcy of the company, due to the flawed design of maximising core counts from a belief that multi-threaded performance was the future; while having the processor cores shares caches and FPUs, massively reducing the multi-threaded performance of the architecture. Intel finds themselves in a similar situation today. Their design choices made back in 2013 mean that it is impossible to mass produce 10nm high core count chips. This would’ve been fine if their monopoly continued and the mainstream continued to have 4 core, 8 threaded CPUs. Indeed, they are producing Ice Lake laptop CPUs today that have 4 cores. However, the resurgence of AMD with their high core count capable Zen architecture meant that Intel were forced into raising their own core counts to compete – there has been a doubling of core counts across their entire product stack, which is fine on 14nm with its’ double patterning, but not so much on 10nm. The limitations of 10nm mean that current generation chips at the same price point from Intel have 14nm massively outperforming 10nm, with the higher core counts outweighing any density improvements that 10nm brings. Similarly, leaks for the upcoming 10nm Alder Lake desktop and Ice Lake Xeon chips suggest that the maximum number of cores on 10nm,28, will be 33-50% lower than those from 14nm [7] – not to mention AMD’s offerings which top out at 2.3x the core count at half the price.[8] The persistent lack of chips on 10nm that can outperform their predecessors, despite us now technically being on ‘10nm+++’, suggests that there is a fundamental barrier in the technology that no amount of delays and extra engineering can get past. 10nm is rotten from the very first steps taken. 7nm and beyond So now we’ve established just how much of a disaster Intel’s 10nm process is, what about 7nm? It should be better right? After all, its’ built on the superior EUV, rather than SAQP. The market obviously expects it to be Intel’s saviour, given the massive drop in Intel share price was widely attributed to the ‘6 month delay’ in 7nm rollout. While I don’t have nearly as much solid information to go on compared to 10nm, I just want to note a few things. The exact words Bob Swan used in the Q2 call were ‘we are seeing a 6 month shift in 7nm… 12 months behind our internal target… we have identified a defect mode that resulted in yield degradation’. There’s quite a lot to break down here. Many people, including analysts on the call, were confused by how 7nm could be both 6 and 12 months behind target at the same time. Have Intel achieved quantum tunnelling of time? The truth is that Bob’s claim of a ‘buffer in planning process’ as the reason, while technically true, is incredibly misleading. In any typical launch of a new process node, you spend a few months getting up to speed – running the foundry through the whole process, troubleshooting, using the produced chips as prototypes to send to OEM partners for them to design products around, etc. You don’t sell the chips produced to anyone. Industry standard is to call this period a tape-out, not a launch of a new process – that’s when you actually produce chips that you sell to people. Bob’s comment translated is that the process is delayed by 12 months, but they’re going to breach industry standard and ‘launch’ 7nm when the first fabs start spinning up 6 months before they have chips in any volume. Sound ridiculous? Well, Intel did the exact same thing with 10nm. Faced with mounting pressure over the constant delays, Intel ‘launched’ Cannon Lake in May 2018. There was 1 CPU in the whole generation, a dual core processor with a clock speed of 2.2Ghz that was slower than the i3-3250 released in 2013 for $20 less than the 10nm part. Not to mention it was nigh on impossible to actually buy one.[9] Cannon Lake was an incredibly obvious paper launch, released to appease investors at a time where Intel had just started up its fabs. Ice Lake, the first 10nm architecture you could actually buy (in limited quantities) shipped in September 2019, more than a year after Cannon Lake ‘launched’. This ‘6-month’ delay is nothing more than an attempt to sweetcoat a 12 month delay (assuming no further delays). The second part of the comment, relating to a ‘defect mode’, is just as interesting as the first. Intel are attempting to use GaaFeT technology for their 7nm process, though there's conflicting information suggesting they might move away from this if it proves to be too difficult. [10] GaaFet, or Gates-all-around-Field-effect-Transistor, is a new and unproven transistor technology that should overcome the technical difficulties current transistor technologies face at increasingly smaller sizes. Unlike normal process shrinks, this is going to a completely new type of transistor and we only have one other comparable in history – the transistor to a 3D FinFeT technology a few years ago. With FinFet, the research process from having a ‘working prototype’ demonstrating commercialisation potential took 8 years. [11] Meanwhile, the equivalent demonstration with GaaFeT took place 3 years ago. [12] While FinFeT and GaaFeT are different beasts, it is undeniable that the plans from Intel, and indeed all other foundries, are incredibly ambitious. The latest leaks suggest that the ‘defect mode’ Intel have ran into has to do with their GaaFeT implementation. If this is true, you could easily see 7nm being just as much of a disaster as 10nm is. Beyond 7nm, there are some positives to be found. As we get even smaller transistors, it will be necessary for both EUV and patterning to occur. It's likely that Intel will have an advantage in this area compared to competitors due to their experience with 10nm. At the same time, they are actively exploring chipletbased designs. They might have been late in realising the benefits, but they've finally come around with their EMIB, Foveros and big.Little technologies, all of which I'll explore in a future blog post. Conclusion I’ll leave it to you to decide what the financial implications of these deductions are for Intel, but suffice it to say the baseline scenario is far worse than what many people envision. There is no doubt that Intel will recover from this fiasco, but at what cost? Will it require yet another management reshuffle? Following in the footsteps of AMD, outsourcing production fully and writing off its’ own fabs? Acknowledgement that they will no longer be able to extract incredible margins from their monopolistic position? References [1] http://www.lithoguru.com/scientist/lithobasics.html [2]Dennard, R., Gaensslen, F., Hwa-Nien Yu, Rideout, V., Bassous, E. and Leblanc, A., 1999. Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions. IEEE Journal of Solid-State Circuits., 87(4), pp.668-678. [3]2019 Intel Investor Meeting Presentation, slide 9 [4]TSMC PR release, 10/2019 [5]https://www.anandtech.com/show/15385/intels-confusing-messaging-is-comet-lake-better-than-ice-lake [6]https://www.techspot.com/review/452-amd-bulldozer-fx-cpus/page13.html [7]https://wccftech.com/intel-10nm-ice-lake-sp-xeon-cpu-28-core-56-thread-cpu-benchmarks-leak/ [8]https://www.amd.com/en/products/cpu/amd-epyc-7742 [9]https://www.anandtech.com/show/13405/intel-10nm-cannon-lake-and-core-i3-8121u-deep-dive-review [10]https://twitter.com/chiakokhua/status/1288402693770231809 [11]https://en.wikipedia.org/wiki/FinFET [12]https://www.researchgate.net/publication/319035460_Stacked_nanosheet_gate-all-around_transistor_to_enable_scaling_beyond_FinFET",I am not. Your first comment generally has me wondering what you meant.,5,nan,nan,nan
